dataset:
  name: speech
  sessions: # each session is all the training blocks from a given subject/day
  - t18.2025.04.01_CERWER
  datasetProbabilityVal: [1.0]
  dataDir:  # path to all sessions
  - ../../Data/t18/t18.2025.04.01_CERWER/Handwriting/tfdata_20ms
  whiteNoiseSD: 1.5     # SD of whtie noise that is added to neural data for training  1.5 ####1.2
  constantOffsetSD: 0.22   # SD of constant offset noise for each channel  0.5
  randomWalkSD: 0       # SD of random walk noise for each channel 0.0
  nInputFeatures: 768     # number of neural features. Usually twice the number of channels (spikes + bandpower for each) #512 if ignoring hand-knob arrays
  sampledElectrodes: null
  staticGainSD: 0     # SD of static gain noise 0.0
  randomCut: 0          
  nClasses: 30            # number of characters. 30 for open english.
  maxSeqElements: 200     # maximum number of phonemes per sentence.
  bufferSize: 1024
model:
  nUnits: 512             # number of units per GRU layer
  inputLayerSize: 768     # input layer size is typically same as number of input features #512 if ignoring hand-knob arrays
  subsampleFactor: 1    
  weightReg: 1.0e-05      # weight regularization
  actReg: 0.0             # activity regularization
  dropout: 0.6          # dropout for GRU layers  0.7
  trainable: true         # is the model trainable?
  nLayers: 5              # number of GRU layers  5
  patch_size: 20       # how many patches 16 15  #### 20
  patch_stride: 16 #12      # take every nth bin 12 9 ##### 16
  inputNetwork:
    nInputLayers: 1       # one input layer per network
    inputLayerSizes:      # input layer size should be the same as the number of input features #512 if ignoring hand-knob arrays
    - 768
    trainable: true       # trainable input network?
    activation: softsign  
    dropout: 0.25         # dropout for input networks  0.2
gpuNumber: '0'            # which GPU to use for training. 0 uses the first one.
mode: train
outputDir: T18_LM_comparison_models_HW/0.1CER #/home/justin/Projects/NGEC/Data/t17/t17.2024.08.29/Typing/brainToText_model
loadDir: null   # TODO: let the model start from a pretrained state
loadCheckpointIdx: null
smoothInputs: 1           # temporal gaussian smooth of input features
smoothKernelSD: 2
learnRateStart: 0.02      # learning rate starts here and decays over X batches
learnRateEnd: 0.0
learnRateDecaySteps: 10000
learnRatePower: 1.0       # power for learning rate degredation. 1 is linear.
trainableInput: true
trainableBackend: true
seed: -1
batchesPerSave: 0
batchesPerVal: 50         # how often to do validation (lower # = slower training)
nBatchesToTrain: 10000    # how many batches to train for
batchSize: 64             # how many trials per batch
inputScale: 1.0
inferenceOutputFileName: null
inputLayerForInference: null
gradClipValue: 10         # clip gradients to prevent explosion
lossType: ctc
normLayer: false
randomWalkAxis: -1
warmUpSteps: 0
early_stopping_num_batches: 1000
EndCER: 0.35