{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training datasets for decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils.mat_to_tfrecord\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from T18_SessionArgsArraybyArray import get_session_info\n",
    "\n",
    "sessions = [\n",
    "   't18.2024.12.04',\n",
    "   't18.2024.12.05',\n",
    "   't18.2025.01.14',\n",
    "   't18.2025.01.15',\n",
    "   't18.2025.01.21',\n",
    "   't18.2025.01.22',\n",
    "   't18.2025.02.04',\n",
    "   't18.2025.02.05',\n",
    "     ]\n",
    "\n",
    "participant = 't18'\n",
    "bin_compression_factor = 2\n",
    "channels_to_exclude = list(range(0,0)) \n",
    "channels_to_zero = list(range(0,0)) #[] # leave empty to not zero anything\n",
    "\n",
    "for session in sessions:\n",
    "    \n",
    "    trials_to_remove, block_nums, num_test_trials = get_session_info(session)\n",
    "    session_path = str(Path('../../Data', participant, session, 'Typing'))\n",
    "    tfdata_path = str(Path(session_path, 'tfdata_20ms'))\n",
    "\n",
    "    print(f'Sesison path: {session_path}')\n",
    "    print(f'tfdata path: {session_path}')\n",
    "    print('\\n')\n",
    "\n",
    "    args = {\n",
    "        'session_mat_path': session_path,\n",
    "        'block_nums': block_nums,\n",
    "        'num_test_trials': num_test_trials,\n",
    "        'trials_to_remove': trials_to_remove,\n",
    "        'channels_to_exclude': channels_to_exclude,\n",
    "        'channels_to_zero': channels_to_zero,\n",
    "        'include_thresh_crossings': True,\n",
    "        'include_spike_power': True,\n",
    "        'spike_pow_max': 50000,\n",
    "        'z_score_data': True,\n",
    "        'global_std': True,\n",
    "        'bin_compression_factor': bin_compression_factor,\n",
    "        'save_path': tfdata_path,\n",
    "    }\n",
    "\n",
    "    utils.mat_to_tfrecord.main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the decoder. Remember to Restart the notebook first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 21:53:27.464924: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-13 21:53:27.487658: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-13 21:53:27.487676: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-13 21:53:27.487706: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-13 21:53:27.493027: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-13 21:53:27.880270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "2025-03-13 21:53:28,550: Using GPU #: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 21:53:28.556281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-13 21:53:28.603219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-13 21:53:28.630640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-13 21:53:28.635779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-13 21:53:28.635899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-13 21:53:28.635986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-13 21:53:28.693363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-13 21:53:28.693477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-13 21:53:28.693568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-13 21:53:28.693652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21035 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/justin/miniconda3/envs/tf-gpu/lib/python3.11/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "2025-03-13 21:53:28.822339: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "/home/justin/miniconda3/envs/tf-gpu/lib/python3.11/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer Orthogonal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gru\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_1 (GRU)                 multiple                  6687744   \n",
      "                                                                 \n",
      " gru_2 (GRU)                 multiple                  1575936   \n",
      "                                                                 \n",
      " gru_3 (GRU)                 multiple                  1575936   \n",
      "                                                                 \n",
      " gru_4 (GRU)                 multiple                  1575936   \n",
      "                                                                 \n",
      " gru_5 (GRU)                 multiple                  1575936   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  15903     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13007903 (49.62 MB)\n",
      "Trainable params: 13007903 (49.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "2025-03-13 21:53:29,321: None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "2025-03-13 21:53:29,410: Initialized decoding model and input networks.\n",
      "2025-03-13 21:53:29,744: Initialized optimizer.\n",
      "2025-03-13 21:53:29,744: Set trainable variables.\n",
      "2025-03-13 21:53:29,753: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2024.12.04/Typing/tfdata_20ms/train\n",
      "2025-03-13 21:53:30,233: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2024.12.04/Typing/tfdata_20ms/test\n",
      "2025-03-13 21:53:30,241: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2024.12.05/Typing/tfdata_20ms/train\n",
      "2025-03-13 21:53:30,258: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2024.12.05/Typing/tfdata_20ms/test\n",
      "2025-03-13 21:53:30,267: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.14/Typing/tfdata_20ms/train\n",
      "2025-03-13 21:53:30,284: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.14/Typing/tfdata_20ms/test\n",
      "2025-03-13 21:53:30,294: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.15/Typing/tfdata_20ms/train\n",
      "2025-03-13 21:53:30,311: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.15/Typing/tfdata_20ms/test\n",
      "2025-03-13 21:53:30,323: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.21/Typing/tfdata_20ms/train\n",
      "2025-03-13 21:53:30,339: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.21/Typing/tfdata_20ms/test\n",
      "2025-03-13 21:53:30,352: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.22/Typing/tfdata_20ms/train\n",
      "2025-03-13 21:53:30,369: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.22/Typing/tfdata_20ms/test\n",
      "2025-03-13 21:53:30,384: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.02.04/Typing/tfdata_20ms/train\n",
      "2025-03-13 21:53:30,401: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.02.04/Typing/tfdata_20ms/test\n",
      "2025-03-13 21:53:30,419: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.02.05/Typing/tfdata_20ms/train\n",
      "2025-03-13 21:53:30,438: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.02.05/Typing/tfdata_20ms/test\n",
      "2025-03-13 21:53:30,453: Loaded all data and created datasets and iterators.\n",
      "2025-03-13 21:53:30,454: Saved dataset info to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/brainToText_decoder_tfdata_params.json\n",
      "2025-03-13 21:53:30,454: Beginning training for 10000 batches.\n",
      "2025-03-13 21:53:30,454: I will stop training after 10000 batches or when the validation CER hasn't improved in 1000 batches.\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 21:53:36.420949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n",
      "2025-03-13 21:53:36.686271: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x789e563c54f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-03-13 21:53:36.686288: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2025-03-13 21:53:36.690423: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-03-13 21:53:36.748521: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 21:53:37,642: Train batch 0: loss: 652.16 gradNorm: 241.18 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:53:37,987: Val batch: CER (t18.2025.01.15): 0.955\n",
      "2025-03-13 21:53:37,988: Val batch 0: CER (avg): 0.955 \n",
      "2025-03-13 21:53:38,153: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:53:38,157: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:53:38,158: Batches since validation CER improved: 0\n",
      "2025-03-13 21:53:38,565: Train batch 1: loss: 453.56 gradNorm: 207.28 \n",
      "2025-03-13 21:53:39,084: Train batch 2: loss: 774.79 gradNorm: 498.93 \n",
      "2025-03-13 21:53:39,309: Train batch 3: loss: 729.95 gradNorm: 578.95 \n",
      "2025-03-13 21:53:40,616: Train batch 4: loss: 1511.95 gradNorm: 1700.73 \n",
      "2025-03-13 21:53:41,537: Train batch 5: loss: 787.55 gradNorm: 1052.58 \n",
      "2025-03-13 21:53:41,620: Train batch 6: loss: 311.75 gradNorm: 447.67 \n",
      "2025-03-13 21:53:42,964: Train batch 7: loss: 768.04 gradNorm: 1777.29 \n",
      "2025-03-13 21:53:43,234: Train batch 8: loss: 709.73 gradNorm: 1861.97 \n",
      "2025-03-13 21:53:43,484: Train batch 9: loss: 238.85 gradNorm: 881.95 \n",
      "2025-03-13 21:53:43,575: Train batch 10: loss: 121.05 gradNorm: 221.81 \n",
      "2025-03-13 21:53:43,902: Train batch 11: loss: 112.27 gradNorm: 130.92 \n",
      "2025-03-13 21:53:43,980: Train batch 12: loss: 115.63 gradNorm: 203.10 \n",
      "2025-03-13 21:53:44,678: Train batch 13: loss: 141.95 gradNorm: 307.89 \n",
      "2025-03-13 21:53:44,824: Train batch 14: loss: 140.23 gradNorm: 298.73 \n",
      "2025-03-13 21:53:44,921: Train batch 15: loss: 151.72 gradNorm: 361.77 \n",
      "2025-03-13 21:53:44,991: Train batch 16: loss: 133.44 gradNorm: 269.24 \n",
      "2025-03-13 21:53:45,063: Train batch 17: loss: 126.44 gradNorm: 166.97 \n",
      "2025-03-13 21:53:45,245: Train batch 18: loss: 119.01 gradNorm: 186.31 \n",
      "2025-03-13 21:53:45,339: Train batch 19: loss: 109.74 gradNorm: 64.68 \n",
      "2025-03-13 21:53:45,498: Train batch 20: loss: 121.90 gradNorm: 278.70 \n",
      "2025-03-13 21:53:45,585: Train batch 21: loss: 119.27 gradNorm: 189.22 \n",
      "2025-03-13 21:53:45,961: Train batch 22: loss: 233.72 gradNorm: 1024.53 \n",
      "2025-03-13 21:53:46,431: Train batch 23: loss: 149.18 gradNorm: 606.53 \n",
      "2025-03-13 21:53:46,652: Train batch 24: loss: 137.82 gradNorm: 517.61 \n",
      "2025-03-13 21:53:47,046: Train batch 25: loss: 108.19 gradNorm: 227.68 \n",
      "2025-03-13 21:53:47,332: Train batch 26: loss: 101.32 gradNorm: 59.39 \n",
      "2025-03-13 21:53:47,615: Train batch 27: loss: 101.70 gradNorm: 99.55 \n",
      "2025-03-13 21:53:47,744: Train batch 28: loss: 118.55 gradNorm: 210.27 \n",
      "2025-03-13 21:53:47,840: Train batch 29: loss: 110.86 gradNorm: 179.26 \n",
      "2025-03-13 21:53:48,144: Train batch 30: loss: 113.93 gradNorm: 392.14 \n",
      "2025-03-13 21:53:48,320: Train batch 31: loss: 96.96 gradNorm: 53.82 \n",
      "2025-03-13 21:53:48,431: Train batch 32: loss: 102.29 gradNorm: 74.15 \n",
      "2025-03-13 21:53:48,690: Train batch 33: loss: 120.45 gradNorm: 439.44 \n",
      "2025-03-13 21:53:48,962: Train batch 34: loss: 114.16 gradNorm: 350.53 \n",
      "2025-03-13 21:53:49,101: Train batch 35: loss: 99.88 gradNorm: 90.58 \n",
      "2025-03-13 21:53:49,250: Train batch 36: loss: 99.74 gradNorm: 90.12 \n",
      "2025-03-13 21:53:49,329: Train batch 37: loss: 94.30 gradNorm: 107.20 \n",
      "2025-03-13 21:53:49,593: Train batch 38: loss: 120.11 gradNorm: 392.64 \n",
      "2025-03-13 21:53:49,680: Train batch 39: loss: 92.99 gradNorm: 13.56 \n",
      "2025-03-13 21:53:50,046: Train batch 40: loss: 150.64 gradNorm: 640.36 \n",
      "2025-03-13 21:53:50,316: Train batch 41: loss: 158.57 gradNorm: 699.04 \n",
      "2025-03-13 21:53:50,424: Train batch 42: loss: 96.38 gradNorm: 53.53 \n",
      "2025-03-13 21:53:50,779: Train batch 43: loss: 107.69 gradNorm: 265.37 \n",
      "2025-03-13 21:53:51,092: Train batch 44: loss: 94.58 gradNorm: 61.09 \n",
      "2025-03-13 21:53:51,174: Train batch 45: loss: 128.39 gradNorm: 240.46 \n",
      "2025-03-13 21:53:51,373: Train batch 46: loss: 133.72 gradNorm: 270.25 \n",
      "2025-03-13 21:53:51,494: Train batch 47: loss: 148.78 gradNorm: 282.59 \n",
      "2025-03-13 21:53:51,568: Train batch 48: loss: 148.08 gradNorm: 267.91 \n",
      "2025-03-13 21:53:51,648: Train batch 49: loss: 130.59 gradNorm: 238.37 \n",
      "2025-03-13 21:53:51,747: Train batch 50: loss: 116.68 gradNorm: 203.16 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:53:51,961: Val batch: CER (t18.2025.01.15): 1.000\n",
      "2025-03-13 21:53:51,962: Val batch 50: CER (avg): 1.000 \n",
      "2025-03-13 21:53:51,962: Batches since validation CER improved: 50\n",
      "2025-03-13 21:53:52,037: Train batch 51: loss: 98.25 gradNorm: 131.14 \n",
      "2025-03-13 21:53:52,164: Train batch 52: loss: 100.20 gradNorm: 135.70 \n",
      "2025-03-13 21:53:52,340: Train batch 53: loss: 140.85 gradNorm: 484.33 \n",
      "2025-03-13 21:53:52,422: Train batch 54: loss: 97.66 gradNorm: 142.57 \n",
      "2025-03-13 21:53:52,577: Train batch 55: loss: 127.64 gradNorm: 349.57 \n",
      "2025-03-13 21:53:52,659: Train batch 56: loss: 96.12 gradNorm: 111.31 \n",
      "2025-03-13 21:53:52,791: Train batch 57: loss: 98.62 gradNorm: 96.79 \n",
      "2025-03-13 21:53:52,940: Train batch 58: loss: 93.65 gradNorm: 8.37 \n",
      "2025-03-13 21:53:53,036: Train batch 59: loss: 101.75 gradNorm: 128.54 \n",
      "2025-03-13 21:53:53,136: Train batch 60: loss: 105.36 gradNorm: 148.19 \n",
      "2025-03-13 21:53:53,226: Train batch 61: loss: 109.37 gradNorm: 164.33 \n",
      "2025-03-13 21:53:53,358: Train batch 62: loss: 101.70 gradNorm: 115.86 \n",
      "2025-03-13 21:53:53,529: Train batch 63: loss: 95.70 gradNorm: 35.64 \n",
      "2025-03-13 21:53:53,622: Train batch 64: loss: 94.51 gradNorm: 30.31 \n",
      "2025-03-13 21:53:53,705: Train batch 65: loss: 87.46 gradNorm: 12.28 \n",
      "2025-03-13 21:53:53,986: Train batch 66: loss: 161.76 gradNorm: 602.99 \n",
      "2025-03-13 21:53:54,076: Train batch 67: loss: 98.53 gradNorm: 126.70 \n",
      "2025-03-13 21:53:54,452: Train batch 68: loss: 141.48 gradNorm: 475.74 \n",
      "2025-03-13 21:53:54,578: Train batch 69: loss: 94.31 gradNorm: 22.41 \n",
      "2025-03-13 21:53:54,651: Train batch 70: loss: 98.54 gradNorm: 102.92 \n",
      "2025-03-13 21:53:54,731: Train batch 71: loss: 98.43 gradNorm: 108.07 \n",
      "2025-03-13 21:53:54,801: Train batch 72: loss: 96.25 gradNorm: 117.75 \n",
      "2025-03-13 21:53:54,984: Train batch 73: loss: 94.74 gradNorm: 42.15 \n",
      "2025-03-13 21:53:55,065: Train batch 74: loss: 89.00 gradNorm: 34.76 \n",
      "2025-03-13 21:53:55,339: Train batch 75: loss: 164.50 gradNorm: 628.05 \n",
      "2025-03-13 21:53:55,420: Train batch 76: loss: 99.16 gradNorm: 123.68 \n",
      "2025-03-13 21:53:55,489: Train batch 77: loss: 90.05 gradNorm: 116.54 \n",
      "2025-03-13 21:53:55,671: Train batch 78: loss: 114.01 gradNorm: 254.16 \n",
      "2025-03-13 21:53:55,757: Train batch 79: loss: 87.28 gradNorm: 38.11 \n",
      "2025-03-13 21:53:55,827: Train batch 80: loss: 88.78 gradNorm: 51.25 \n",
      "2025-03-13 21:53:55,938: Train batch 81: loss: 92.22 gradNorm: 60.03 \n",
      "2025-03-13 21:53:56,104: Train batch 82: loss: 93.20 gradNorm: 35.36 \n",
      "2025-03-13 21:53:56,241: Train batch 83: loss: 91.10 gradNorm: 53.10 \n",
      "2025-03-13 21:53:56,391: Train batch 84: loss: 89.49 gradNorm: 33.31 \n",
      "2025-03-13 21:53:56,587: Train batch 85: loss: 91.77 gradNorm: 87.80 \n",
      "2025-03-13 21:53:56,818: Train batch 86: loss: 103.27 gradNorm: 251.71 \n",
      "2025-03-13 21:53:56,917: Train batch 87: loss: 88.18 gradNorm: 42.31 \n",
      "2025-03-13 21:53:57,274: Train batch 88: loss: 99.13 gradNorm: 171.45 \n",
      "2025-03-13 21:53:57,443: Train batch 89: loss: 87.77 gradNorm: 24.27 \n",
      "2025-03-13 21:53:57,611: Train batch 90: loss: 86.96 gradNorm: 53.86 \n",
      "2025-03-13 21:53:57,800: Train batch 91: loss: 88.11 gradNorm: 29.50 \n",
      "2025-03-13 21:53:57,969: Train batch 92: loss: 86.86 gradNorm: 59.75 \n",
      "2025-03-13 21:53:58,202: Train batch 93: loss: 91.29 gradNorm: 105.23 \n",
      "2025-03-13 21:53:58,293: Train batch 94: loss: 86.63 gradNorm: 62.25 \n",
      "2025-03-13 21:53:58,472: Train batch 95: loss: 85.13 gradNorm: 20.92 \n",
      "2025-03-13 21:53:58,546: Train batch 96: loss: 84.78 gradNorm: 56.71 \n",
      "2025-03-13 21:53:58,659: Train batch 97: loss: 79.87 gradNorm: 41.92 \n",
      "2025-03-13 21:53:58,939: Train batch 98: loss: 137.65 gradNorm: 533.03 \n",
      "2025-03-13 21:53:59,068: Train batch 99: loss: 95.39 gradNorm: 197.99 \n",
      "2025-03-13 21:53:59,219: Train batch 100: loss: 87.18 gradNorm: 108.94 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:53:59,412: Val batch: CER (t18.2025.01.15): 0.916\n",
      "2025-03-13 21:53:59,413: Val batch 100: CER (avg): 0.916 \n",
      "2025-03-13 21:53:59,637: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:53:59,641: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:53:59,641: Batches since validation CER improved: 0\n",
      "2025-03-13 21:54:00,003: Train batch 101: loss: 92.42 gradNorm: 140.06 \n",
      "2025-03-13 21:54:00,176: Train batch 102: loss: 85.34 gradNorm: 80.82 \n",
      "2025-03-13 21:54:00,461: Train batch 103: loss: 93.43 gradNorm: 109.09 \n",
      "2025-03-13 21:54:00,543: Train batch 104: loss: 101.40 gradNorm: 171.59 \n",
      "2025-03-13 21:54:00,803: Train batch 105: loss: 93.09 gradNorm: 61.61 \n",
      "2025-03-13 21:54:00,910: Train batch 106: loss: 89.66 gradNorm: 108.77 \n",
      "2025-03-13 21:54:01,309: Train batch 107: loss: 98.68 gradNorm: 255.45 \n",
      "2025-03-13 21:54:01,486: Train batch 108: loss: 84.98 gradNorm: 82.77 \n",
      "2025-03-13 21:54:01,643: Train batch 109: loss: 81.55 gradNorm: 53.18 \n",
      "2025-03-13 21:54:01,801: Train batch 110: loss: 81.37 gradNorm: 42.21 \n",
      "2025-03-13 21:54:02,157: Train batch 111: loss: 91.99 gradNorm: 172.94 \n",
      "2025-03-13 21:54:02,380: Train batch 112: loss: 91.04 gradNorm: 177.27 \n",
      "2025-03-13 21:54:02,551: Train batch 113: loss: 85.83 gradNorm: 105.81 \n",
      "2025-03-13 21:54:02,706: Train batch 114: loss: 88.18 gradNorm: 111.71 \n",
      "2025-03-13 21:54:02,896: Train batch 115: loss: 82.12 gradNorm: 78.16 \n",
      "2025-03-13 21:54:03,047: Train batch 116: loss: 77.85 gradNorm: 57.62 \n",
      "2025-03-13 21:54:03,207: Train batch 117: loss: 79.78 gradNorm: 59.63 \n",
      "2025-03-13 21:54:03,284: Train batch 118: loss: 77.93 gradNorm: 73.17 \n",
      "2025-03-13 21:54:03,382: Train batch 119: loss: 90.22 gradNorm: 176.91 \n",
      "2025-03-13 21:54:03,461: Train batch 120: loss: 85.45 gradNorm: 138.57 \n",
      "2025-03-13 21:54:03,548: Train batch 121: loss: 95.28 gradNorm: 188.93 \n",
      "2025-03-13 21:54:03,743: Train batch 122: loss: 123.19 gradNorm: 327.50 \n",
      "2025-03-13 21:54:03,901: Train batch 123: loss: 87.05 gradNorm: 119.75 \n",
      "2025-03-13 21:54:04,005: Train batch 124: loss: 84.13 gradNorm: 89.95 \n",
      "2025-03-13 21:54:04,118: Train batch 125: loss: 82.34 gradNorm: 90.35 \n",
      "2025-03-13 21:54:04,210: Train batch 126: loss: 87.75 gradNorm: 126.97 \n",
      "2025-03-13 21:54:04,280: Train batch 127: loss: 85.17 gradNorm: 94.82 \n",
      "2025-03-13 21:54:04,650: Train batch 128: loss: 117.28 gradNorm: 512.90 \n",
      "2025-03-13 21:54:04,920: Train batch 129: loss: 141.74 gradNorm: 640.73 \n",
      "2025-03-13 21:54:05,006: Train batch 130: loss: 80.86 gradNorm: 65.39 \n",
      "2025-03-13 21:54:05,088: Train batch 131: loss: 75.49 gradNorm: 47.88 \n",
      "2025-03-13 21:54:05,359: Train batch 132: loss: 106.11 gradNorm: 297.56 \n",
      "2025-03-13 21:54:05,430: Train batch 133: loss: 79.90 gradNorm: 74.48 \n",
      "2025-03-13 21:54:05,511: Train batch 134: loss: 77.94 gradNorm: 75.27 \n",
      "2025-03-13 21:54:05,581: Train batch 135: loss: 76.86 gradNorm: 56.08 \n",
      "2025-03-13 21:54:05,751: Train batch 136: loss: 107.09 gradNorm: 292.84 \n",
      "2025-03-13 21:54:06,044: Train batch 137: loss: 132.57 gradNorm: 446.04 \n",
      "2025-03-13 21:54:06,188: Train batch 138: loss: 84.52 gradNorm: 158.12 \n",
      "2025-03-13 21:54:06,261: Train batch 139: loss: 73.55 gradNorm: 41.23 \n",
      "2025-03-13 21:54:06,337: Train batch 140: loss: 72.50 gradNorm: 50.32 \n",
      "2025-03-13 21:54:06,434: Train batch 141: loss: 74.97 gradNorm: 51.92 \n",
      "2025-03-13 21:54:06,536: Train batch 142: loss: 73.96 gradNorm: 37.54 \n",
      "2025-03-13 21:54:06,640: Train batch 143: loss: 73.05 gradNorm: 53.39 \n",
      "2025-03-13 21:54:06,721: Train batch 144: loss: 74.70 gradNorm: 45.48 \n",
      "2025-03-13 21:54:06,856: Train batch 145: loss: 72.47 gradNorm: 32.15 \n",
      "2025-03-13 21:54:07,125: Train batch 146: loss: 95.16 gradNorm: 171.83 \n",
      "2025-03-13 21:54:07,208: Train batch 147: loss: 76.36 gradNorm: 87.50 \n",
      "2025-03-13 21:54:07,322: Train batch 148: loss: 72.67 gradNorm: 48.44 \n",
      "2025-03-13 21:54:07,480: Train batch 149: loss: 71.50 gradNorm: 57.48 \n",
      "2025-03-13 21:54:07,554: Train batch 150: loss: 72.05 gradNorm: 69.61 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:54:07,763: Val batch: CER (t18.2025.01.15): 0.755\n",
      "2025-03-13 21:54:07,764: Val batch 150: CER (avg): 0.755 \n",
      "2025-03-13 21:54:07,986: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:07,990: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:07,990: Batches since validation CER improved: 0\n",
      "2025-03-13 21:54:08,075: Train batch 151: loss: 68.91 gradNorm: 50.73 \n",
      "2025-03-13 21:54:08,154: Train batch 152: loss: 69.51 gradNorm: 34.76 \n",
      "2025-03-13 21:54:08,235: Train batch 153: loss: 72.13 gradNorm: 69.40 \n",
      "2025-03-13 21:54:08,367: Train batch 154: loss: 73.68 gradNorm: 121.09 \n",
      "2025-03-13 21:54:08,471: Train batch 155: loss: 69.17 gradNorm: 71.15 \n",
      "2025-03-13 21:54:08,725: Train batch 156: loss: 86.32 gradNorm: 75.97 \n",
      "2025-03-13 21:54:08,888: Train batch 157: loss: 73.18 gradNorm: 80.34 \n",
      "2025-03-13 21:54:08,964: Train batch 158: loss: 79.44 gradNorm: 121.81 \n",
      "2025-03-13 21:54:09,063: Train batch 159: loss: 75.81 gradNorm: 104.01 \n",
      "2025-03-13 21:54:09,211: Train batch 160: loss: 74.09 gradNorm: 79.28 \n",
      "2025-03-13 21:54:09,381: Train batch 161: loss: 73.07 gradNorm: 164.76 \n",
      "2025-03-13 21:54:09,688: Train batch 162: loss: 90.96 gradNorm: 193.05 \n",
      "2025-03-13 21:54:09,988: Train batch 163: loss: 77.71 gradNorm: 83.08 \n",
      "2025-03-13 21:54:10,070: Train batch 164: loss: 66.59 gradNorm: 74.60 \n",
      "2025-03-13 21:54:10,203: Train batch 165: loss: 67.20 gradNorm: 75.03 \n",
      "2025-03-13 21:54:10,558: Train batch 166: loss: 77.30 gradNorm: 77.71 \n",
      "2025-03-13 21:54:10,628: Train batch 167: loss: 66.04 gradNorm: 71.68 \n",
      "2025-03-13 21:54:10,712: Train batch 168: loss: 64.66 gradNorm: 54.31 \n",
      "2025-03-13 21:54:10,792: Train batch 169: loss: 61.74 gradNorm: 47.51 \n",
      "2025-03-13 21:54:11,071: Train batch 170: loss: 83.89 gradNorm: 231.98 \n",
      "2025-03-13 21:54:11,179: Train batch 171: loss: 66.91 gradNorm: 60.19 \n",
      "2025-03-13 21:54:11,375: Train batch 172: loss: 78.41 gradNorm: 106.38 \n",
      "2025-03-13 21:54:11,454: Train batch 173: loss: 68.43 gradNorm: 93.53 \n",
      "2025-03-13 21:54:11,619: Train batch 174: loss: 67.84 gradNorm: 70.51 \n",
      "2025-03-13 21:54:11,728: Train batch 175: loss: 66.71 gradNorm: 82.32 \n",
      "2025-03-13 21:54:11,978: Train batch 176: loss: 84.10 gradNorm: 197.91 \n",
      "2025-03-13 21:54:12,158: Train batch 177: loss: 69.68 gradNorm: 161.75 \n",
      "2025-03-13 21:54:12,552: Train batch 178: loss: 77.71 gradNorm: 86.95 \n",
      "2025-03-13 21:54:12,828: Train batch 179: loss: 79.65 gradNorm: 115.03 \n",
      "2025-03-13 21:54:12,988: Train batch 180: loss: 72.22 gradNorm: 109.93 \n",
      "2025-03-13 21:54:13,070: Train batch 181: loss: 65.01 gradNorm: 76.93 \n",
      "2025-03-13 21:54:13,161: Train batch 182: loss: 61.20 gradNorm: 58.94 \n",
      "2025-03-13 21:54:13,245: Train batch 183: loss: 65.90 gradNorm: 113.64 \n",
      "2025-03-13 21:54:13,385: Train batch 184: loss: 60.67 gradNorm: 50.22 \n",
      "2025-03-13 21:54:13,462: Train batch 185: loss: 63.33 gradNorm: 82.41 \n",
      "2025-03-13 21:54:13,843: Train batch 186: loss: 76.31 gradNorm: 154.68 \n",
      "2025-03-13 21:54:14,141: Train batch 187: loss: 69.50 gradNorm: 80.63 \n",
      "2025-03-13 21:54:14,222: Train batch 188: loss: 61.08 gradNorm: 88.96 \n",
      "2025-03-13 21:54:14,301: Train batch 189: loss: 65.75 gradNorm: 77.44 \n",
      "2025-03-13 21:54:14,579: Train batch 190: loss: 86.71 gradNorm: 152.55 \n",
      "2025-03-13 21:54:14,663: Train batch 191: loss: 58.67 gradNorm: 61.35 \n",
      "2025-03-13 21:54:15,020: Train batch 192: loss: 73.98 gradNorm: 107.43 \n",
      "2025-03-13 21:54:15,147: Train batch 193: loss: 63.66 gradNorm: 69.00 \n",
      "2025-03-13 21:54:15,246: Train batch 194: loss: 61.13 gradNorm: 64.71 \n",
      "2025-03-13 21:54:15,320: Train batch 195: loss: 59.02 gradNorm: 69.71 \n",
      "2025-03-13 21:54:15,400: Train batch 196: loss: 56.44 gradNorm: 43.60 \n",
      "2025-03-13 21:54:15,487: Train batch 197: loss: 65.45 gradNorm: 152.93 \n",
      "2025-03-13 21:54:15,591: Train batch 198: loss: 62.54 gradNorm: 153.92 \n",
      "2025-03-13 21:54:15,876: Train batch 199: loss: 80.74 gradNorm: 154.12 \n",
      "2025-03-13 21:54:15,973: Train batch 200: loss: 54.14 gradNorm: 43.20 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:54:16,198: Val batch: CER (t18.2025.01.15): 0.564\n",
      "2025-03-13 21:54:16,199: Val batch 200: CER (avg): 0.564 \n",
      "2025-03-13 21:54:16,423: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:16,426: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:16,427: Batches since validation CER improved: 0\n",
      "2025-03-13 21:54:16,506: Train batch 201: loss: 58.66 gradNorm: 76.04 \n",
      "2025-03-13 21:54:16,610: Train batch 202: loss: 58.07 gradNorm: 46.01 \n",
      "2025-03-13 21:54:16,785: Train batch 203: loss: 54.60 gradNorm: 57.12 \n",
      "2025-03-13 21:54:17,185: Train batch 204: loss: 77.19 gradNorm: 131.05 \n",
      "2025-03-13 21:54:17,285: Train batch 205: loss: 57.24 gradNorm: 50.46 \n",
      "2025-03-13 21:54:17,367: Train batch 206: loss: 54.81 gradNorm: 52.94 \n",
      "2025-03-13 21:54:17,539: Train batch 207: loss: 60.60 gradNorm: 61.55 \n",
      "2025-03-13 21:54:17,614: Train batch 208: loss: 53.89 gradNorm: 57.16 \n",
      "2025-03-13 21:54:17,685: Train batch 209: loss: 50.57 gradNorm: 50.44 \n",
      "2025-03-13 21:54:17,868: Train batch 210: loss: 63.97 gradNorm: 147.89 \n",
      "2025-03-13 21:54:18,151: Train batch 211: loss: 75.41 gradNorm: 97.34 \n",
      "2025-03-13 21:54:18,235: Train batch 212: loss: 52.45 gradNorm: 54.41 \n",
      "2025-03-13 21:54:18,407: Train batch 213: loss: 58.74 gradNorm: 75.73 \n",
      "2025-03-13 21:54:18,595: Train batch 214: loss: 57.04 gradNorm: 52.92 \n",
      "2025-03-13 21:54:18,677: Train batch 215: loss: 50.64 gradNorm: 61.50 \n",
      "2025-03-13 21:54:18,776: Train batch 216: loss: 49.53 gradNorm: 46.51 \n",
      "2025-03-13 21:54:19,004: Train batch 217: loss: 62.13 gradNorm: 80.58 \n",
      "2025-03-13 21:54:19,278: Train batch 218: loss: 71.62 gradNorm: 113.21 \n",
      "2025-03-13 21:54:19,396: Train batch 219: loss: 53.07 gradNorm: 115.93 \n",
      "2025-03-13 21:54:19,483: Train batch 220: loss: 50.21 gradNorm: 50.94 \n",
      "2025-03-13 21:54:19,672: Train batch 221: loss: 70.21 gradNorm: 164.46 \n",
      "2025-03-13 21:54:19,808: Train batch 222: loss: 50.38 gradNorm: 80.25 \n",
      "2025-03-13 21:54:19,878: Train batch 223: loss: 57.52 gradNorm: 78.70 \n",
      "2025-03-13 21:54:20,187: Train batch 224: loss: 72.45 gradNorm: 101.76 \n",
      "2025-03-13 21:54:20,354: Train batch 225: loss: 53.89 gradNorm: 75.51 \n",
      "2025-03-13 21:54:20,484: Train batch 226: loss: 56.76 gradNorm: 92.90 \n",
      "2025-03-13 21:54:20,573: Train batch 227: loss: 47.06 gradNorm: 69.22 \n",
      "2025-03-13 21:54:20,661: Train batch 228: loss: 44.83 gradNorm: 52.44 \n",
      "2025-03-13 21:54:21,023: Train batch 229: loss: 70.76 gradNorm: 129.00 \n",
      "2025-03-13 21:54:21,109: Train batch 230: loss: 48.10 gradNorm: 65.27 \n",
      "2025-03-13 21:54:21,228: Train batch 231: loss: 54.11 gradNorm: 78.22 \n",
      "2025-03-13 21:54:21,318: Train batch 232: loss: 50.59 gradNorm: 67.32 \n",
      "2025-03-13 21:54:21,388: Train batch 233: loss: 49.49 gradNorm: 51.51 \n",
      "2025-03-13 21:54:21,670: Train batch 234: loss: 73.69 gradNorm: 161.57 \n",
      "2025-03-13 21:54:21,757: Train batch 235: loss: 45.77 gradNorm: 54.16 \n",
      "2025-03-13 21:54:21,906: Train batch 236: loss: 52.82 gradNorm: 63.85 \n",
      "2025-03-13 21:54:22,100: Train batch 237: loss: 52.05 gradNorm: 52.63 \n",
      "2025-03-13 21:54:22,173: Train batch 238: loss: 46.39 gradNorm: 42.62 \n",
      "2025-03-13 21:54:22,350: Train batch 239: loss: 50.60 gradNorm: 46.40 \n",
      "2025-03-13 21:54:22,516: Train batch 240: loss: 50.48 gradNorm: 54.74 \n",
      "2025-03-13 21:54:22,690: Train batch 241: loss: 50.55 gradNorm: 45.20 \n",
      "2025-03-13 21:54:23,009: Train batch 242: loss: 64.09 gradNorm: 95.89 \n",
      "2025-03-13 21:54:23,093: Train batch 243: loss: 45.31 gradNorm: 72.37 \n",
      "2025-03-13 21:54:23,347: Train batch 244: loss: 65.50 gradNorm: 90.54 \n",
      "2025-03-13 21:54:23,623: Train batch 245: loss: 63.48 gradNorm: 73.08 \n",
      "2025-03-13 21:54:23,720: Train batch 246: loss: 49.92 gradNorm: 52.07 \n",
      "2025-03-13 21:54:23,913: Train batch 247: loss: 51.44 gradNorm: 126.16 \n",
      "2025-03-13 21:54:24,066: Train batch 248: loss: 52.13 gradNorm: 86.24 \n",
      "2025-03-13 21:54:24,424: Train batch 249: loss: 62.59 gradNorm: 159.95 \n",
      "2025-03-13 21:54:24,497: Train batch 250: loss: 44.80 gradNorm: 73.79 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:54:24,701: Val batch: CER (t18.2025.01.15): 0.411\n",
      "2025-03-13 21:54:24,702: Val batch 250: CER (avg): 0.411 \n",
      "2025-03-13 21:54:24,927: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:24,931: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:24,932: Batches since validation CER improved: 0\n",
      "2025-03-13 21:54:25,236: Train batch 251: loss: 67.27 gradNorm: 103.84 \n",
      "2025-03-13 21:54:25,364: Train batch 252: loss: 53.43 gradNorm: 113.05 \n",
      "2025-03-13 21:54:25,535: Train batch 253: loss: 61.07 gradNorm: 161.26 \n",
      "2025-03-13 21:54:25,626: Train batch 254: loss: 44.59 gradNorm: 52.23 \n",
      "2025-03-13 21:54:25,784: Train batch 255: loss: 50.41 gradNorm: 69.48 \n",
      "2025-03-13 21:54:25,865: Train batch 256: loss: 44.06 gradNorm: 45.46 \n",
      "2025-03-13 21:54:25,943: Train batch 257: loss: 45.79 gradNorm: 54.53 \n",
      "2025-03-13 21:54:26,315: Train batch 258: loss: 70.98 gradNorm: 226.40 \n",
      "2025-03-13 21:54:26,390: Train batch 259: loss: 43.72 gradNorm: 70.53 \n",
      "2025-03-13 21:54:26,590: Train batch 260: loss: 50.33 gradNorm: 84.07 \n",
      "2025-03-13 21:54:26,748: Train batch 261: loss: 47.51 gradNorm: 70.14 \n",
      "2025-03-13 21:54:26,845: Train batch 262: loss: 45.52 gradNorm: 59.99 \n",
      "2025-03-13 21:54:27,008: Train batch 263: loss: 46.58 gradNorm: 73.21 \n",
      "2025-03-13 21:54:27,170: Train batch 264: loss: 47.25 gradNorm: 80.62 \n",
      "2025-03-13 21:54:27,298: Train batch 265: loss: 44.74 gradNorm: 58.07 \n",
      "2025-03-13 21:54:27,379: Train batch 266: loss: 40.13 gradNorm: 50.04 \n",
      "2025-03-13 21:54:27,477: Train batch 267: loss: 41.46 gradNorm: 45.40 \n",
      "2025-03-13 21:54:27,557: Train batch 268: loss: 42.68 gradNorm: 39.77 \n",
      "2025-03-13 21:54:27,663: Train batch 269: loss: 40.01 gradNorm: 36.92 \n",
      "2025-03-13 21:54:27,766: Train batch 270: loss: 42.19 gradNorm: 35.48 \n",
      "2025-03-13 21:54:27,846: Train batch 271: loss: 39.36 gradNorm: 50.70 \n",
      "2025-03-13 21:54:27,987: Train batch 272: loss: 43.56 gradNorm: 60.75 \n",
      "2025-03-13 21:54:28,193: Train batch 273: loss: 58.59 gradNorm: 79.38 \n",
      "2025-03-13 21:54:28,550: Train batch 274: loss: 83.22 gradNorm: 306.86 \n",
      "2025-03-13 21:54:28,726: Train batch 275: loss: 51.78 gradNorm: 111.14 \n",
      "2025-03-13 21:54:28,895: Train batch 276: loss: 47.90 gradNorm: 77.79 \n",
      "2025-03-13 21:54:28,971: Train batch 277: loss: 48.34 gradNorm: 109.01 \n",
      "2025-03-13 21:54:29,061: Train batch 278: loss: 44.53 gradNorm: 73.98 \n",
      "2025-03-13 21:54:29,225: Train batch 279: loss: 50.60 gradNorm: 141.74 \n",
      "2025-03-13 21:54:29,425: Train batch 280: loss: 46.80 gradNorm: 137.46 \n",
      "2025-03-13 21:54:29,526: Train batch 281: loss: 38.18 gradNorm: 52.92 \n",
      "2025-03-13 21:54:29,796: Train batch 282: loss: 62.03 gradNorm: 97.39 \n",
      "2025-03-13 21:54:29,894: Train batch 283: loss: 42.33 gradNorm: 66.29 \n",
      "2025-03-13 21:54:30,055: Train batch 284: loss: 46.33 gradNorm: 70.73 \n",
      "2025-03-13 21:54:30,135: Train batch 285: loss: 45.13 gradNorm: 83.95 \n",
      "2025-03-13 21:54:30,239: Train batch 286: loss: 41.57 gradNorm: 52.93 \n",
      "2025-03-13 21:54:30,331: Train batch 287: loss: 37.75 gradNorm: 50.54 \n",
      "2025-03-13 21:54:30,428: Train batch 288: loss: 39.18 gradNorm: 71.92 \n",
      "2025-03-13 21:54:30,508: Train batch 289: loss: 42.59 gradNorm: 69.91 \n",
      "2025-03-13 21:54:30,700: Train batch 290: loss: 46.17 gradNorm: 82.46 \n",
      "2025-03-13 21:54:31,064: Train batch 291: loss: 63.49 gradNorm: 133.51 \n",
      "2025-03-13 21:54:31,291: Train batch 292: loss: 53.81 gradNorm: 79.05 \n",
      "2025-03-13 21:54:31,390: Train batch 293: loss: 41.31 gradNorm: 55.40 \n",
      "2025-03-13 21:54:31,520: Train batch 294: loss: 38.98 gradNorm: 50.76 \n",
      "2025-03-13 21:54:31,608: Train batch 295: loss: 41.16 gradNorm: 45.12 \n",
      "2025-03-13 21:54:31,691: Train batch 296: loss: 39.17 gradNorm: 47.95 \n",
      "2025-03-13 21:54:31,830: Train batch 297: loss: 36.18 gradNorm: 39.09 \n",
      "2025-03-13 21:54:31,999: Train batch 298: loss: 39.40 gradNorm: 50.43 \n",
      "2025-03-13 21:54:32,087: Train batch 299: loss: 36.65 gradNorm: 39.51 \n",
      "2025-03-13 21:54:32,246: Train batch 300: loss: 37.16 gradNorm: 54.06 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:54:32,436: Val batch: CER (t18.2025.01.15): 0.254\n",
      "2025-03-13 21:54:32,437: Val batch 300: CER (avg): 0.254 \n",
      "2025-03-13 21:54:32,656: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:32,660: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:32,660: Batches since validation CER improved: 0\n",
      "2025-03-13 21:54:32,762: Train batch 301: loss: 35.22 gradNorm: 43.42 \n",
      "2025-03-13 21:54:32,921: Train batch 302: loss: 36.42 gradNorm: 39.63 \n",
      "2025-03-13 21:54:33,033: Train batch 303: loss: 36.78 gradNorm: 43.56 \n",
      "2025-03-13 21:54:33,135: Train batch 304: loss: 36.66 gradNorm: 42.70 \n",
      "2025-03-13 21:54:33,511: Train batch 305: loss: 83.99 gradNorm: 319.75 \n",
      "2025-03-13 21:54:33,888: Train batch 306: loss: 71.88 gradNorm: 197.31 \n",
      "2025-03-13 21:54:33,968: Train batch 307: loss: 42.16 gradNorm: 87.26 \n",
      "2025-03-13 21:54:34,043: Train batch 308: loss: 38.99 gradNorm: 64.24 \n",
      "2025-03-13 21:54:34,361: Train batch 309: loss: 86.03 gradNorm: 196.23 \n",
      "2025-03-13 21:54:34,444: Train batch 310: loss: 38.74 gradNorm: 70.78 \n",
      "2025-03-13 21:54:34,707: Train batch 311: loss: 61.10 gradNorm: 120.31 \n",
      "2025-03-13 21:54:34,790: Train batch 312: loss: 38.91 gradNorm: 56.45 \n",
      "2025-03-13 21:54:34,975: Train batch 313: loss: 45.98 gradNorm: 93.77 \n",
      "2025-03-13 21:54:35,044: Train batch 314: loss: 36.53 gradNorm: 69.28 \n",
      "2025-03-13 21:54:35,145: Train batch 315: loss: 36.58 gradNorm: 68.07 \n",
      "2025-03-13 21:54:35,249: Train batch 316: loss: 36.31 gradNorm: 38.89 \n",
      "2025-03-13 21:54:35,330: Train batch 317: loss: 34.32 gradNorm: 40.66 \n",
      "2025-03-13 21:54:35,420: Train batch 318: loss: 34.40 gradNorm: 44.91 \n",
      "2025-03-13 21:54:35,590: Train batch 319: loss: 42.98 gradNorm: 71.17 \n",
      "2025-03-13 21:54:35,699: Train batch 320: loss: 35.66 gradNorm: 54.67 \n",
      "2025-03-13 21:54:35,793: Train batch 321: loss: 34.35 gradNorm: 42.40 \n",
      "2025-03-13 21:54:36,050: Train batch 322: loss: 64.95 gradNorm: 145.27 \n",
      "2025-03-13 21:54:36,331: Train batch 323: loss: 60.61 gradNorm: 144.86 \n",
      "2025-03-13 21:54:36,508: Train batch 324: loss: 38.71 gradNorm: 65.07 \n",
      "2025-03-13 21:54:36,586: Train batch 325: loss: 41.55 gradNorm: 67.37 \n",
      "2025-03-13 21:54:36,835: Train batch 326: loss: 67.77 gradNorm: 189.77 \n",
      "2025-03-13 21:54:37,216: Train batch 327: loss: 55.15 gradNorm: 140.05 \n",
      "2025-03-13 21:54:37,308: Train batch 328: loss: 33.13 gradNorm: 47.51 \n",
      "2025-03-13 21:54:37,503: Train batch 329: loss: 38.61 gradNorm: 56.20 \n",
      "2025-03-13 21:54:37,633: Train batch 330: loss: 36.66 gradNorm: 50.49 \n",
      "2025-03-13 21:54:37,720: Train batch 331: loss: 36.16 gradNorm: 53.98 \n",
      "2025-03-13 21:54:37,833: Train batch 332: loss: 34.69 gradNorm: 58.72 \n",
      "2025-03-13 21:54:38,193: Train batch 333: loss: 49.99 gradNorm: 125.25 \n",
      "2025-03-13 21:54:38,382: Train batch 334: loss: 35.61 gradNorm: 57.44 \n",
      "2025-03-13 21:54:38,478: Train batch 335: loss: 32.76 gradNorm: 58.19 \n",
      "2025-03-13 21:54:38,560: Train batch 336: loss: 36.78 gradNorm: 60.43 \n",
      "2025-03-13 21:54:38,848: Train batch 337: loss: 58.97 gradNorm: 137.95 \n",
      "2025-03-13 21:54:39,151: Train batch 338: loss: 58.39 gradNorm: 121.43 \n",
      "2025-03-13 21:54:39,425: Train batch 339: loss: 53.69 gradNorm: 82.60 \n",
      "2025-03-13 21:54:39,578: Train batch 340: loss: 38.08 gradNorm: 78.66 \n",
      "2025-03-13 21:54:39,690: Train batch 341: loss: 30.77 gradNorm: 46.80 \n",
      "2025-03-13 21:54:39,791: Train batch 342: loss: 32.67 gradNorm: 48.92 \n",
      "2025-03-13 21:54:39,889: Train batch 343: loss: 32.61 gradNorm: 37.89 \n",
      "2025-03-13 21:54:39,968: Train batch 344: loss: 38.20 gradNorm: 73.43 \n",
      "2025-03-13 21:54:40,137: Train batch 345: loss: 36.60 gradNorm: 71.94 \n",
      "2025-03-13 21:54:40,267: Train batch 346: loss: 31.41 gradNorm: 46.69 \n",
      "2025-03-13 21:54:40,336: Train batch 347: loss: 34.38 gradNorm: 45.86 \n",
      "2025-03-13 21:54:40,448: Train batch 348: loss: 32.68 gradNorm: 38.72 \n",
      "2025-03-13 21:54:40,829: Train batch 349: loss: 58.53 gradNorm: 103.88 \n",
      "2025-03-13 21:54:41,107: Train batch 350: loss: 45.10 gradNorm: 71.35 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:54:41,300: Val batch: CER (t18.2025.01.15): 0.249\n",
      "2025-03-13 21:54:41,301: Val batch 350: CER (avg): 0.249 \n",
      "2025-03-13 21:54:41,518: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:41,522: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:41,522: Batches since validation CER improved: 0\n",
      "2025-03-13 21:54:41,674: Train batch 351: loss: 37.72 gradNorm: 88.79 \n",
      "2025-03-13 21:54:41,862: Train batch 352: loss: 36.74 gradNorm: 51.39 \n",
      "2025-03-13 21:54:41,958: Train batch 353: loss: 34.55 gradNorm: 51.87 \n",
      "2025-03-13 21:54:42,335: Train batch 354: loss: 53.35 gradNorm: 100.12 \n",
      "2025-03-13 21:54:42,581: Train batch 355: loss: 77.50 gradNorm: 145.63 \n",
      "2025-03-13 21:54:42,768: Train batch 356: loss: 43.61 gradNorm: 103.45 \n",
      "2025-03-13 21:54:42,959: Train batch 357: loss: 43.54 gradNorm: 100.47 \n",
      "2025-03-13 21:54:43,071: Train batch 358: loss: 41.19 gradNorm: 82.58 \n",
      "2025-03-13 21:54:43,152: Train batch 359: loss: 36.65 gradNorm: 63.10 \n",
      "2025-03-13 21:54:43,260: Train batch 360: loss: 36.30 gradNorm: 64.23 \n",
      "2025-03-13 21:54:43,345: Train batch 361: loss: 36.89 gradNorm: 81.85 \n",
      "2025-03-13 21:54:43,523: Train batch 362: loss: 56.83 gradNorm: 191.79 \n",
      "2025-03-13 21:54:43,638: Train batch 363: loss: 33.98 gradNorm: 52.44 \n",
      "2025-03-13 21:54:43,946: Train batch 364: loss: 76.44 gradNorm: 186.36 \n",
      "2025-03-13 21:54:44,223: Train batch 365: loss: 65.42 gradNorm: 123.33 \n",
      "2025-03-13 21:54:44,323: Train batch 366: loss: 40.16 gradNorm: 86.99 \n",
      "2025-03-13 21:54:44,429: Train batch 367: loss: 39.15 gradNorm: 80.82 \n",
      "2025-03-13 21:54:44,515: Train batch 368: loss: 39.24 gradNorm: 86.76 \n",
      "2025-03-13 21:54:44,699: Train batch 369: loss: 37.92 gradNorm: 67.61 \n",
      "2025-03-13 21:54:44,850: Train batch 370: loss: 34.93 gradNorm: 49.63 \n",
      "2025-03-13 21:54:45,110: Train batch 371: loss: 63.97 gradNorm: 209.11 \n",
      "2025-03-13 21:54:45,296: Train batch 372: loss: 42.51 gradNorm: 87.39 \n",
      "2025-03-13 21:54:45,667: Train batch 373: loss: 54.57 gradNorm: 120.33 \n",
      "2025-03-13 21:54:45,793: Train batch 374: loss: 39.95 gradNorm: 71.95 \n",
      "2025-03-13 21:54:46,088: Train batch 375: loss: 43.34 gradNorm: 74.03 \n",
      "2025-03-13 21:54:46,168: Train batch 376: loss: 45.22 gradNorm: 85.99 \n",
      "2025-03-13 21:54:46,362: Train batch 377: loss: 37.28 gradNorm: 62.62 \n",
      "2025-03-13 21:54:46,433: Train batch 378: loss: 36.52 gradNorm: 63.91 \n",
      "2025-03-13 21:54:46,534: Train batch 379: loss: 32.13 gradNorm: 44.91 \n",
      "2025-03-13 21:54:46,624: Train batch 380: loss: 30.98 gradNorm: 33.36 \n",
      "2025-03-13 21:54:46,708: Train batch 381: loss: 30.76 gradNorm: 37.53 \n",
      "2025-03-13 21:54:46,787: Train batch 382: loss: 37.34 gradNorm: 47.69 \n",
      "2025-03-13 21:54:46,889: Train batch 383: loss: 30.66 gradNorm: 34.00 \n",
      "2025-03-13 21:54:46,960: Train batch 384: loss: 31.19 gradNorm: 35.04 \n",
      "2025-03-13 21:54:47,128: Train batch 385: loss: 30.65 gradNorm: 40.60 \n",
      "2025-03-13 21:54:47,229: Train batch 386: loss: 30.72 gradNorm: 41.25 \n",
      "2025-03-13 21:54:47,377: Train batch 387: loss: 30.91 gradNorm: 36.09 \n",
      "2025-03-13 21:54:47,629: Train batch 388: loss: 47.52 gradNorm: 88.43 \n",
      "2025-03-13 21:54:47,918: Train batch 389: loss: 64.76 gradNorm: 134.44 \n",
      "2025-03-13 21:54:48,005: Train batch 390: loss: 30.22 gradNorm: 45.73 \n",
      "2025-03-13 21:54:48,093: Train batch 391: loss: 29.64 gradNorm: 44.27 \n",
      "2025-03-13 21:54:48,195: Train batch 392: loss: 29.75 gradNorm: 35.06 \n",
      "2025-03-13 21:54:48,271: Train batch 393: loss: 31.87 gradNorm: 49.66 \n",
      "2025-03-13 21:54:48,421: Train batch 394: loss: 32.34 gradNorm: 49.99 \n",
      "2025-03-13 21:54:48,811: Train batch 395: loss: 54.14 gradNorm: 126.07 \n",
      "2025-03-13 21:54:48,915: Train batch 396: loss: 34.37 gradNorm: 69.87 \n",
      "2025-03-13 21:54:48,999: Train batch 397: loss: 38.83 gradNorm: 79.11 \n",
      "2025-03-13 21:54:49,090: Train batch 398: loss: 31.45 gradNorm: 56.75 \n",
      "2025-03-13 21:54:49,212: Train batch 399: loss: 32.86 gradNorm: 60.73 \n",
      "2025-03-13 21:54:49,293: Train batch 400: loss: 36.31 gradNorm: 56.65 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:54:49,488: Val batch: CER (t18.2025.01.15): 0.238\n",
      "2025-03-13 21:54:49,489: Val batch 400: CER (avg): 0.238 \n",
      "2025-03-13 21:54:49,682: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:49,686: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:49,686: Batches since validation CER improved: 0\n",
      "2025-03-13 21:54:49,841: Train batch 401: loss: 36.76 gradNorm: 98.65 \n",
      "2025-03-13 21:54:49,920: Train batch 402: loss: 31.71 gradNorm: 57.87 \n",
      "2025-03-13 21:54:50,186: Train batch 403: loss: 60.64 gradNorm: 187.24 \n",
      "2025-03-13 21:54:50,275: Train batch 404: loss: 31.32 gradNorm: 50.31 \n",
      "2025-03-13 21:54:50,421: Train batch 405: loss: 34.75 gradNorm: 50.57 \n",
      "2025-03-13 21:54:50,849: Train batch 406: loss: 44.67 gradNorm: 84.52 \n",
      "2025-03-13 21:54:51,006: Train batch 407: loss: 31.53 gradNorm: 61.35 \n",
      "2025-03-13 21:54:51,306: Train batch 408: loss: 61.99 gradNorm: 145.04 \n",
      "2025-03-13 21:54:51,472: Train batch 409: loss: 31.54 gradNorm: 60.94 \n",
      "2025-03-13 21:54:51,589: Train batch 410: loss: 30.70 gradNorm: 59.13 \n",
      "2025-03-13 21:54:51,746: Train batch 411: loss: 28.43 gradNorm: 42.10 \n",
      "2025-03-13 21:54:51,828: Train batch 412: loss: 34.54 gradNorm: 57.84 \n",
      "2025-03-13 21:54:51,987: Train batch 413: loss: 34.98 gradNorm: 63.07 \n",
      "2025-03-13 21:54:52,148: Train batch 414: loss: 28.06 gradNorm: 44.53 \n",
      "2025-03-13 21:54:52,245: Train batch 415: loss: 28.68 gradNorm: 39.54 \n",
      "2025-03-13 21:54:52,441: Train batch 416: loss: 27.89 gradNorm: 34.47 \n",
      "2025-03-13 21:54:52,536: Train batch 417: loss: 25.14 gradNorm: 32.63 \n",
      "2025-03-13 21:54:52,697: Train batch 418: loss: 30.59 gradNorm: 33.25 \n",
      "2025-03-13 21:54:53,103: Train batch 419: loss: 44.95 gradNorm: 82.87 \n",
      "2025-03-13 21:54:53,188: Train batch 420: loss: 28.17 gradNorm: 32.34 \n",
      "2025-03-13 21:54:53,275: Train batch 421: loss: 29.39 gradNorm: 51.78 \n",
      "2025-03-13 21:54:53,551: Train batch 422: loss: 62.26 gradNorm: 140.97 \n",
      "2025-03-13 21:54:53,822: Train batch 423: loss: 36.84 gradNorm: 65.28 \n",
      "2025-03-13 21:54:53,996: Train batch 424: loss: 25.19 gradNorm: 46.41 \n",
      "2025-03-13 21:54:54,245: Train batch 425: loss: 43.72 gradNorm: 64.56 \n",
      "2025-03-13 21:54:54,349: Train batch 426: loss: 24.93 gradNorm: 32.67 \n",
      "2025-03-13 21:54:54,423: Train batch 427: loss: 29.88 gradNorm: 42.70 \n",
      "2025-03-13 21:54:54,517: Train batch 428: loss: 27.39 gradNorm: 33.63 \n",
      "2025-03-13 21:54:54,604: Train batch 429: loss: 26.03 gradNorm: 32.29 \n",
      "2025-03-13 21:54:54,714: Train batch 430: loss: 30.73 gradNorm: 64.52 \n",
      "2025-03-13 21:54:54,841: Train batch 431: loss: 26.66 gradNorm: 30.12 \n",
      "2025-03-13 21:54:54,924: Train batch 432: loss: 30.13 gradNorm: 45.34 \n",
      "2025-03-13 21:54:55,040: Train batch 433: loss: 27.51 gradNorm: 42.62 \n",
      "2025-03-13 21:54:55,121: Train batch 434: loss: 29.04 gradNorm: 39.07 \n",
      "2025-03-13 21:54:55,205: Train batch 435: loss: 25.79 gradNorm: 39.46 \n",
      "2025-03-13 21:54:55,277: Train batch 436: loss: 29.69 gradNorm: 40.55 \n",
      "2025-03-13 21:54:55,394: Train batch 437: loss: 28.04 gradNorm: 47.96 \n",
      "2025-03-13 21:54:55,597: Train batch 438: loss: 31.32 gradNorm: 54.47 \n",
      "2025-03-13 21:54:55,674: Train batch 439: loss: 28.77 gradNorm: 57.17 \n",
      "2025-03-13 21:54:55,745: Train batch 440: loss: 30.71 gradNorm: 53.45 \n",
      "2025-03-13 21:54:56,112: Train batch 441: loss: 49.44 gradNorm: 103.73 \n",
      "2025-03-13 21:54:56,240: Train batch 442: loss: 27.62 gradNorm: 44.51 \n",
      "2025-03-13 21:54:56,368: Train batch 443: loss: 22.97 gradNorm: 34.35 \n",
      "2025-03-13 21:54:56,673: Train batch 444: loss: 102.14 gradNorm: 541.12 \n",
      "2025-03-13 21:54:56,828: Train batch 445: loss: 34.28 gradNorm: 65.57 \n",
      "2025-03-13 21:54:56,915: Train batch 446: loss: 29.83 gradNorm: 60.06 \n",
      "2025-03-13 21:54:57,009: Train batch 447: loss: 40.07 gradNorm: 77.39 \n",
      "2025-03-13 21:54:57,389: Train batch 448: loss: 47.22 gradNorm: 94.80 \n",
      "2025-03-13 21:54:57,466: Train batch 449: loss: 25.62 gradNorm: 37.55 \n",
      "2025-03-13 21:54:57,627: Train batch 450: loss: 36.73 gradNorm: 81.74 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:54:57,846: Val batch: CER (t18.2025.01.15): 0.217\n",
      "2025-03-13 21:54:57,847: Val batch 450: CER (avg): 0.217 \n",
      "2025-03-13 21:54:58,067: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:58,071: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:54:58,071: Batches since validation CER improved: 0\n",
      "2025-03-13 21:54:58,182: Train batch 451: loss: 26.48 gradNorm: 58.35 \n",
      "2025-03-13 21:54:58,440: Train batch 452: loss: 42.65 gradNorm: 93.46 \n",
      "2025-03-13 21:54:58,543: Train batch 453: loss: 26.52 gradNorm: 41.56 \n",
      "2025-03-13 21:54:58,640: Train batch 454: loss: 27.38 gradNorm: 42.44 \n",
      "2025-03-13 21:54:58,851: Train batch 455: loss: 31.21 gradNorm: 66.31 \n",
      "2025-03-13 21:54:59,028: Train batch 456: loss: 30.15 gradNorm: 45.83 \n",
      "2025-03-13 21:54:59,214: Train batch 457: loss: 27.83 gradNorm: 55.12 \n",
      "2025-03-13 21:54:59,358: Train batch 458: loss: 29.33 gradNorm: 58.80 \n",
      "2025-03-13 21:54:59,459: Train batch 459: loss: 25.57 gradNorm: 39.31 \n",
      "2025-03-13 21:54:59,732: Train batch 460: loss: 60.82 gradNorm: 169.64 \n",
      "2025-03-13 21:54:59,885: Train batch 461: loss: 23.58 gradNorm: 49.51 \n",
      "2025-03-13 21:55:00,115: Train batch 462: loss: 43.42 gradNorm: 73.50 \n",
      "2025-03-13 21:55:00,306: Train batch 463: loss: 32.41 gradNorm: 58.87 \n",
      "2025-03-13 21:55:00,434: Train batch 464: loss: 29.45 gradNorm: 50.74 \n",
      "2025-03-13 21:55:00,569: Train batch 465: loss: 26.92 gradNorm: 44.01 \n",
      "2025-03-13 21:55:00,863: Train batch 466: loss: 50.94 gradNorm: 105.66 \n",
      "2025-03-13 21:55:01,010: Train batch 467: loss: 30.19 gradNorm: 67.69 \n",
      "2025-03-13 21:55:01,366: Train batch 468: loss: 40.60 gradNorm: 88.11 \n",
      "2025-03-13 21:55:01,562: Train batch 469: loss: 29.28 gradNorm: 53.44 \n",
      "2025-03-13 21:55:01,641: Train batch 470: loss: 31.84 gradNorm: 54.36 \n",
      "2025-03-13 21:55:01,997: Train batch 471: loss: 33.49 gradNorm: 49.49 \n",
      "2025-03-13 21:55:02,080: Train batch 472: loss: 29.68 gradNorm: 41.45 \n",
      "2025-03-13 21:55:02,358: Train batch 473: loss: 59.25 gradNorm: 142.98 \n",
      "2025-03-13 21:55:02,518: Train batch 474: loss: 32.63 gradNorm: 68.95 \n",
      "2025-03-13 21:55:02,851: Train batch 475: loss: 45.97 gradNorm: 76.13 \n",
      "2025-03-13 21:55:02,932: Train batch 476: loss: 27.16 gradNorm: 43.59 \n",
      "2025-03-13 21:55:03,233: Train batch 477: loss: 41.75 gradNorm: 83.73 \n",
      "2025-03-13 21:55:03,306: Train batch 478: loss: 29.13 gradNorm: 49.82 \n",
      "2025-03-13 21:55:03,413: Train batch 479: loss: 27.71 gradNorm: 37.85 \n",
      "2025-03-13 21:55:03,515: Train batch 480: loss: 23.79 gradNorm: 29.52 \n",
      "2025-03-13 21:55:03,604: Train batch 481: loss: 29.90 gradNorm: 59.14 \n",
      "2025-03-13 21:55:03,754: Train batch 482: loss: 29.95 gradNorm: 59.13 \n",
      "2025-03-13 21:55:03,868: Train batch 483: loss: 24.78 gradNorm: 39.56 \n",
      "2025-03-13 21:55:03,976: Train batch 484: loss: 25.69 gradNorm: 38.25 \n",
      "2025-03-13 21:55:04,246: Train batch 485: loss: 60.13 gradNorm: 156.31 \n",
      "2025-03-13 21:55:04,358: Train batch 486: loss: 24.86 gradNorm: 37.55 \n",
      "2025-03-13 21:55:04,431: Train batch 487: loss: 27.88 gradNorm: 49.17 \n",
      "2025-03-13 21:55:04,696: Train batch 488: loss: 45.72 gradNorm: 110.46 \n",
      "2025-03-13 21:55:04,813: Train batch 489: loss: 26.91 gradNorm: 41.91 \n",
      "2025-03-13 21:55:05,142: Train batch 490: loss: 46.65 gradNorm: 83.90 \n",
      "2025-03-13 21:55:05,249: Train batch 491: loss: 24.15 gradNorm: 39.47 \n",
      "2025-03-13 21:55:05,433: Train batch 492: loss: 28.51 gradNorm: 44.43 \n",
      "2025-03-13 21:55:05,516: Train batch 493: loss: 25.17 gradNorm: 37.11 \n",
      "2025-03-13 21:55:05,586: Train batch 494: loss: 26.24 gradNorm: 40.28 \n",
      "2025-03-13 21:55:05,686: Train batch 495: loss: 23.31 gradNorm: 32.91 \n",
      "2025-03-13 21:55:05,782: Train batch 496: loss: 19.41 gradNorm: 25.53 \n",
      "2025-03-13 21:55:05,949: Train batch 497: loss: 24.34 gradNorm: 43.90 \n",
      "2025-03-13 21:55:06,030: Train batch 498: loss: 29.63 gradNorm: 58.57 \n",
      "2025-03-13 21:55:06,117: Train batch 499: loss: 26.92 gradNorm: 52.41 \n",
      "2025-03-13 21:55:06,203: Train batch 500: loss: 28.33 gradNorm: 53.24 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:55:06,409: Val batch: CER (t18.2025.01.15): 0.190\n",
      "2025-03-13 21:55:06,411: Val batch 500: CER (avg): 0.190 \n",
      "2025-03-13 21:55:06,621: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:55:06,624: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:55:06,624: Batches since validation CER improved: 0\n",
      "2025-03-13 21:55:06,775: Train batch 501: loss: 25.43 gradNorm: 56.63 \n",
      "2025-03-13 21:55:06,858: Train batch 502: loss: 24.78 gradNorm: 53.19 \n",
      "2025-03-13 21:55:07,052: Train batch 503: loss: 29.43 gradNorm: 64.33 \n",
      "2025-03-13 21:55:07,320: Train batch 504: loss: 37.25 gradNorm: 90.65 \n",
      "2025-03-13 21:55:07,504: Train batch 505: loss: 28.79 gradNorm: 47.25 \n",
      "2025-03-13 21:55:07,645: Train batch 506: loss: 27.34 gradNorm: 58.46 \n",
      "2025-03-13 21:55:07,718: Train batch 507: loss: 26.81 gradNorm: 45.38 \n",
      "2025-03-13 21:55:07,805: Train batch 508: loss: 24.42 gradNorm: 36.76 \n",
      "2025-03-13 21:55:07,949: Train batch 509: loss: 24.79 gradNorm: 48.01 \n",
      "2025-03-13 21:55:08,018: Train batch 510: loss: 23.02 gradNorm: 39.63 \n",
      "2025-03-13 21:55:08,128: Train batch 511: loss: 27.81 gradNorm: 51.72 \n",
      "2025-03-13 21:55:08,226: Train batch 512: loss: 23.71 gradNorm: 38.30 \n",
      "2025-03-13 21:55:08,315: Train batch 513: loss: 22.09 gradNorm: 34.10 \n",
      "2025-03-13 21:55:08,480: Train batch 514: loss: 27.37 gradNorm: 47.45 \n",
      "2025-03-13 21:55:08,568: Train batch 515: loss: 19.95 gradNorm: 34.77 \n",
      "2025-03-13 21:55:08,658: Train batch 516: loss: 26.62 gradNorm: 45.89 \n",
      "2025-03-13 21:55:08,811: Train batch 517: loss: 23.04 gradNorm: 36.77 \n",
      "2025-03-13 21:55:08,984: Train batch 518: loss: 26.60 gradNorm: 35.16 \n",
      "2025-03-13 21:55:09,138: Train batch 519: loss: 24.07 gradNorm: 39.86 \n",
      "2025-03-13 21:55:09,239: Train batch 520: loss: 24.73 gradNorm: 50.84 \n",
      "2025-03-13 21:55:09,506: Train batch 521: loss: 52.03 gradNorm: 120.56 \n",
      "2025-03-13 21:55:09,661: Train batch 522: loss: 22.73 gradNorm: 43.49 \n",
      "2025-03-13 21:55:09,814: Train batch 523: loss: 22.77 gradNorm: 41.01 \n",
      "2025-03-13 21:55:09,942: Train batch 524: loss: 23.69 gradNorm: 37.06 \n",
      "2025-03-13 21:55:10,114: Train batch 525: loss: 27.88 gradNorm: 54.55 \n",
      "2025-03-13 21:55:10,247: Train batch 526: loss: 22.10 gradNorm: 56.03 \n",
      "2025-03-13 21:55:10,335: Train batch 527: loss: 23.79 gradNorm: 38.28 \n",
      "2025-03-13 21:55:10,429: Train batch 528: loss: 19.51 gradNorm: 33.64 \n",
      "2025-03-13 21:55:10,688: Train batch 529: loss: 39.10 gradNorm: 71.59 \n",
      "2025-03-13 21:55:10,958: Train batch 530: loss: 35.10 gradNorm: 45.62 \n",
      "2025-03-13 21:55:11,029: Train batch 531: loss: 28.26 gradNorm: 56.92 \n",
      "2025-03-13 21:55:11,231: Train batch 532: loss: 26.93 gradNorm: 64.05 \n",
      "2025-03-13 21:55:11,330: Train batch 533: loss: 26.54 gradNorm: 46.89 \n",
      "2025-03-13 21:55:11,427: Train batch 534: loss: 22.64 gradNorm: 33.57 \n",
      "2025-03-13 21:55:11,797: Train batch 535: loss: 37.94 gradNorm: 77.61 \n",
      "2025-03-13 21:55:11,886: Train batch 536: loss: 21.32 gradNorm: 38.98 \n",
      "2025-03-13 21:55:12,117: Train batch 537: loss: 34.05 gradNorm: 55.91 \n",
      "2025-03-13 21:55:12,213: Train batch 538: loss: 29.24 gradNorm: 60.73 \n",
      "2025-03-13 21:55:12,383: Train batch 539: loss: 21.85 gradNorm: 44.95 \n",
      "2025-03-13 21:55:12,497: Train batch 540: loss: 23.01 gradNorm: 43.56 \n",
      "2025-03-13 21:55:12,596: Train batch 541: loss: 27.55 gradNorm: 61.57 \n",
      "2025-03-13 21:55:12,731: Train batch 542: loss: 26.26 gradNorm: 46.19 \n",
      "2025-03-13 21:55:12,833: Train batch 543: loss: 28.23 gradNorm: 80.40 \n",
      "2025-03-13 21:55:13,121: Train batch 544: loss: 57.57 gradNorm: 142.20 \n",
      "2025-03-13 21:55:13,201: Train batch 545: loss: 23.07 gradNorm: 38.96 \n",
      "2025-03-13 21:55:13,330: Train batch 546: loss: 20.62 gradNorm: 37.16 \n",
      "2025-03-13 21:55:13,708: Train batch 547: loss: 39.80 gradNorm: 73.12 \n",
      "2025-03-13 21:55:13,797: Train batch 548: loss: 25.66 gradNorm: 34.70 \n",
      "2025-03-13 21:55:14,084: Train batch 549: loss: 36.65 gradNorm: 61.15 \n",
      "2025-03-13 21:55:14,170: Train batch 550: loss: 25.92 gradNorm: 41.28 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:55:14,368: Val batch: CER (t18.2025.01.15): 0.173\n",
      "2025-03-13 21:55:14,370: Val batch 550: CER (avg): 0.173 \n",
      "2025-03-13 21:55:14,588: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:55:14,592: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:55:14,592: Batches since validation CER improved: 0\n",
      "2025-03-13 21:55:14,762: Train batch 551: loss: 21.07 gradNorm: 34.78 \n",
      "2025-03-13 21:55:15,017: Train batch 552: loss: 41.33 gradNorm: 74.32 \n",
      "2025-03-13 21:55:15,113: Train batch 553: loss: 22.78 gradNorm: 35.33 \n",
      "2025-03-13 21:55:15,244: Train batch 554: loss: 29.76 gradNorm: 112.88 \n",
      "2025-03-13 21:55:15,359: Train batch 555: loss: 19.67 gradNorm: 42.29 \n",
      "2025-03-13 21:55:15,636: Train batch 556: loss: 36.39 gradNorm: 74.76 \n",
      "2025-03-13 21:55:15,838: Train batch 557: loss: 28.35 gradNorm: 51.23 \n",
      "2025-03-13 21:55:15,925: Train batch 558: loss: 20.59 gradNorm: 38.22 \n",
      "2025-03-13 21:55:16,005: Train batch 559: loss: 28.90 gradNorm: 45.52 \n",
      "2025-03-13 21:55:16,120: Train batch 560: loss: 26.46 gradNorm: 36.41 \n",
      "2025-03-13 21:55:16,211: Train batch 561: loss: 28.00 gradNorm: 47.99 \n",
      "2025-03-13 21:55:16,312: Train batch 562: loss: 23.71 gradNorm: 33.63 \n",
      "2025-03-13 21:55:16,398: Train batch 563: loss: 22.89 gradNorm: 35.38 \n",
      "2025-03-13 21:55:16,479: Train batch 564: loss: 23.09 gradNorm: 31.37 \n",
      "2025-03-13 21:55:16,788: Train batch 565: loss: 61.29 gradNorm: 223.94 \n",
      "2025-03-13 21:55:17,188: Train batch 566: loss: 43.50 gradNorm: 122.68 \n",
      "2025-03-13 21:55:17,277: Train batch 567: loss: 23.07 gradNorm: 42.06 \n",
      "2025-03-13 21:55:17,369: Train batch 568: loss: 27.88 gradNorm: 44.11 \n",
      "2025-03-13 21:55:17,500: Train batch 569: loss: 27.99 gradNorm: 55.22 \n",
      "2025-03-13 21:55:17,590: Train batch 570: loss: 20.03 gradNorm: 39.36 \n",
      "2025-03-13 21:55:17,896: Train batch 571: loss: 44.15 gradNorm: 137.63 \n",
      "2025-03-13 21:55:17,975: Train batch 572: loss: 34.54 gradNorm: 95.56 \n",
      "2025-03-13 21:55:18,259: Train batch 573: loss: 42.67 gradNorm: 91.47 \n",
      "2025-03-13 21:55:18,394: Train batch 574: loss: 23.57 gradNorm: 38.91 \n",
      "2025-03-13 21:55:18,671: Train batch 575: loss: 35.02 gradNorm: 58.56 \n",
      "2025-03-13 21:55:18,758: Train batch 576: loss: 28.85 gradNorm: 60.88 \n",
      "2025-03-13 21:55:18,928: Train batch 577: loss: 29.16 gradNorm: 54.67 \n",
      "2025-03-13 21:55:19,073: Train batch 578: loss: 23.14 gradNorm: 43.94 \n",
      "2025-03-13 21:55:19,187: Train batch 579: loss: 24.46 gradNorm: 39.49 \n",
      "2025-03-13 21:55:19,559: Train batch 580: loss: 41.46 gradNorm: 96.15 \n",
      "2025-03-13 21:55:19,632: Train batch 581: loss: 29.49 gradNorm: 68.99 \n",
      "2025-03-13 21:55:19,731: Train batch 582: loss: 23.73 gradNorm: 44.55 \n",
      "2025-03-13 21:55:19,926: Train batch 583: loss: 30.20 gradNorm: 68.10 \n",
      "2025-03-13 21:55:20,227: Train batch 584: loss: 35.00 gradNorm: 73.35 \n",
      "2025-03-13 21:55:20,507: Train batch 585: loss: 47.15 gradNorm: 100.50 \n",
      "2025-03-13 21:55:20,654: Train batch 586: loss: 25.63 gradNorm: 49.18 \n",
      "2025-03-13 21:55:20,830: Train batch 587: loss: 22.37 gradNorm: 40.50 \n",
      "2025-03-13 21:55:20,952: Train batch 588: loss: 27.96 gradNorm: 52.49 \n",
      "2025-03-13 21:55:21,114: Train batch 589: loss: 20.49 gradNorm: 33.33 \n",
      "2025-03-13 21:55:21,405: Train batch 590: loss: 32.86 gradNorm: 79.13 \n",
      "2025-03-13 21:55:21,518: Train batch 591: loss: 18.25 gradNorm: 29.69 \n",
      "2025-03-13 21:55:21,827: Train batch 592: loss: 35.77 gradNorm: 62.65 \n",
      "2025-03-13 21:55:21,897: Train batch 593: loss: 22.51 gradNorm: 40.49 \n",
      "2025-03-13 21:55:22,157: Train batch 594: loss: 31.35 gradNorm: 49.21 \n",
      "2025-03-13 21:55:22,234: Train batch 595: loss: 25.68 gradNorm: 40.70 \n",
      "2025-03-13 21:55:22,514: Train batch 596: loss: 31.38 gradNorm: 86.53 \n",
      "2025-03-13 21:55:22,594: Train batch 597: loss: 24.70 gradNorm: 34.40 \n",
      "2025-03-13 21:55:22,949: Train batch 598: loss: 30.91 gradNorm: 52.22 \n",
      "2025-03-13 21:55:23,232: Train batch 599: loss: 32.77 gradNorm: 66.13 \n",
      "2025-03-13 21:55:23,328: Train batch 600: loss: 25.43 gradNorm: 50.40 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:55:23,549: Val batch: CER (t18.2025.01.15): 0.183\n",
      "2025-03-13 21:55:23,551: Val batch 600: CER (avg): 0.183 \n",
      "2025-03-13 21:55:23,551: Batches since validation CER improved: 50\n",
      "2025-03-13 21:55:23,621: Train batch 601: loss: 22.82 gradNorm: 37.97 \n",
      "2025-03-13 21:55:23,718: Train batch 602: loss: 20.91 gradNorm: 34.34 \n",
      "2025-03-13 21:55:23,796: Train batch 603: loss: 20.14 gradNorm: 30.67 \n",
      "2025-03-13 21:55:23,882: Train batch 604: loss: 21.86 gradNorm: 38.88 \n",
      "2025-03-13 21:55:24,103: Train batch 605: loss: 37.73 gradNorm: 82.11 \n",
      "2025-03-13 21:55:24,200: Train batch 606: loss: 22.70 gradNorm: 35.24 \n",
      "2025-03-13 21:55:24,304: Train batch 607: loss: 26.95 gradNorm: 64.42 \n",
      "2025-03-13 21:55:24,575: Train batch 608: loss: 28.16 gradNorm: 51.43 \n",
      "2025-03-13 21:55:24,929: Train batch 609: loss: 29.02 gradNorm: 56.96 \n",
      "2025-03-13 21:55:25,013: Train batch 610: loss: 24.23 gradNorm: 45.09 \n",
      "2025-03-13 21:55:25,119: Train batch 611: loss: 18.09 gradNorm: 28.73 \n",
      "2025-03-13 21:55:25,402: Train batch 612: loss: 27.68 gradNorm: 45.18 \n",
      "2025-03-13 21:55:25,475: Train batch 613: loss: 22.87 gradNorm: 41.73 \n",
      "2025-03-13 21:55:25,603: Train batch 614: loss: 19.54 gradNorm: 33.07 \n",
      "2025-03-13 21:55:25,702: Train batch 615: loss: 17.16 gradNorm: 26.50 \n",
      "2025-03-13 21:55:25,770: Train batch 616: loss: 21.82 gradNorm: 33.32 \n",
      "2025-03-13 21:55:25,895: Train batch 617: loss: 23.57 gradNorm: 41.74 \n",
      "2025-03-13 21:55:25,977: Train batch 618: loss: 19.82 gradNorm: 29.45 \n",
      "2025-03-13 21:55:26,129: Train batch 619: loss: 23.18 gradNorm: 53.06 \n",
      "2025-03-13 21:55:26,211: Train batch 620: loss: 20.31 gradNorm: 29.70 \n",
      "2025-03-13 21:55:26,286: Train batch 621: loss: 21.99 gradNorm: 34.32 \n",
      "2025-03-13 21:55:26,385: Train batch 622: loss: 20.41 gradNorm: 31.98 \n",
      "2025-03-13 21:55:26,544: Train batch 623: loss: 21.85 gradNorm: 39.14 \n",
      "2025-03-13 21:55:26,778: Train batch 624: loss: 22.60 gradNorm: 29.33 \n",
      "2025-03-13 21:55:26,866: Train batch 625: loss: 20.87 gradNorm: 44.99 \n",
      "2025-03-13 21:55:26,963: Train batch 626: loss: 22.23 gradNorm: 51.47 \n",
      "2025-03-13 21:55:27,052: Train batch 627: loss: 22.17 gradNorm: 40.92 \n",
      "2025-03-13 21:55:27,258: Train batch 628: loss: 36.06 gradNorm: 105.14 \n",
      "2025-03-13 21:55:27,395: Train batch 629: loss: 18.72 gradNorm: 35.26 \n",
      "2025-03-13 21:55:27,470: Train batch 630: loss: 20.23 gradNorm: 35.97 \n",
      "2025-03-13 21:55:27,792: Train batch 631: loss: 38.16 gradNorm: 92.67 \n",
      "2025-03-13 21:55:27,894: Train batch 632: loss: 23.53 gradNorm: 43.34 \n",
      "2025-03-13 21:55:28,020: Train batch 633: loss: 21.36 gradNorm: 38.35 \n",
      "2025-03-13 21:55:28,087: Train batch 634: loss: 22.73 gradNorm: 38.64 \n",
      "2025-03-13 21:55:28,358: Train batch 635: loss: 31.28 gradNorm: 86.05 \n",
      "2025-03-13 21:55:28,443: Train batch 636: loss: 18.48 gradNorm: 31.18 \n",
      "2025-03-13 21:55:28,832: Train batch 637: loss: 30.88 gradNorm: 56.00 \n",
      "2025-03-13 21:55:28,982: Train batch 638: loss: 17.64 gradNorm: 33.42 \n",
      "2025-03-13 21:55:29,060: Train batch 639: loss: 20.88 gradNorm: 40.81 \n",
      "2025-03-13 21:55:29,333: Train batch 640: loss: 30.24 gradNorm: 72.72 \n",
      "2025-03-13 21:55:29,471: Train batch 641: loss: 18.77 gradNorm: 28.30 \n",
      "2025-03-13 21:55:29,567: Train batch 642: loss: 19.37 gradNorm: 38.80 \n",
      "2025-03-13 21:55:29,649: Train batch 643: loss: 20.39 gradNorm: 36.85 \n",
      "2025-03-13 21:55:29,724: Train batch 644: loss: 20.11 gradNorm: 35.67 \n",
      "2025-03-13 21:55:30,025: Train batch 645: loss: 30.52 gradNorm: 66.72 \n",
      "2025-03-13 21:55:30,123: Train batch 646: loss: 17.06 gradNorm: 25.43 \n",
      "2025-03-13 21:55:30,249: Train batch 647: loss: 17.30 gradNorm: 28.72 \n",
      "2025-03-13 21:55:30,318: Train batch 648: loss: 20.13 gradNorm: 34.36 \n",
      "2025-03-13 21:55:30,708: Train batch 649: loss: 27.69 gradNorm: 51.86 \n",
      "2025-03-13 21:55:31,003: Train batch 650: loss: 26.73 gradNorm: 40.15 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:55:31,196: Val batch: CER (t18.2025.01.15): 0.137\n",
      "2025-03-13 21:55:31,197: Val batch 650: CER (avg): 0.137 \n",
      "2025-03-13 21:55:31,405: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:55:31,408: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:55:31,408: Batches since validation CER improved: 0\n",
      "2025-03-13 21:55:31,518: Train batch 651: loss: 19.47 gradNorm: 34.54 \n",
      "2025-03-13 21:55:31,898: Train batch 652: loss: 28.61 gradNorm: 48.17 \n",
      "2025-03-13 21:55:32,050: Train batch 653: loss: 20.58 gradNorm: 42.83 \n",
      "2025-03-13 21:55:32,303: Train batch 654: loss: 29.79 gradNorm: 50.33 \n",
      "2025-03-13 21:55:32,572: Train batch 655: loss: 27.92 gradNorm: 51.78 \n",
      "2025-03-13 21:55:32,665: Train batch 656: loss: 24.02 gradNorm: 47.19 \n",
      "2025-03-13 21:55:32,745: Train batch 657: loss: 19.67 gradNorm: 37.14 \n",
      "2025-03-13 21:55:32,825: Train batch 658: loss: 19.64 gradNorm: 27.03 \n",
      "2025-03-13 21:55:33,102: Train batch 659: loss: 32.02 gradNorm: 98.35 \n",
      "2025-03-13 21:55:33,173: Train batch 660: loss: 19.45 gradNorm: 35.69 \n",
      "2025-03-13 21:55:33,258: Train batch 661: loss: 18.35 gradNorm: 32.31 \n",
      "2025-03-13 21:55:33,358: Train batch 662: loss: 18.67 gradNorm: 28.33 \n",
      "2025-03-13 21:55:33,628: Train batch 663: loss: 27.17 gradNorm: 52.35 \n",
      "2025-03-13 21:55:34,016: Train batch 664: loss: 25.52 gradNorm: 56.30 \n",
      "2025-03-13 21:55:34,369: Train batch 665: loss: 25.33 gradNorm: 39.09 \n",
      "2025-03-13 21:55:34,522: Train batch 666: loss: 18.72 gradNorm: 39.70 \n",
      "2025-03-13 21:55:34,598: Train batch 667: loss: 20.89 gradNorm: 40.16 \n",
      "2025-03-13 21:55:34,772: Train batch 668: loss: 15.88 gradNorm: 27.96 \n",
      "2025-03-13 21:55:35,074: Train batch 669: loss: 31.27 gradNorm: 87.70 \n",
      "2025-03-13 21:55:35,307: Train batch 670: loss: 25.73 gradNorm: 59.30 \n",
      "2025-03-13 21:55:35,499: Train batch 671: loss: 25.46 gradNorm: 48.85 \n",
      "2025-03-13 21:55:35,598: Train batch 672: loss: 20.21 gradNorm: 38.44 \n",
      "2025-03-13 21:55:35,672: Train batch 673: loss: 22.04 gradNorm: 47.78 \n",
      "2025-03-13 21:55:35,832: Train batch 674: loss: 21.20 gradNorm: 53.53 \n",
      "2025-03-13 21:55:35,998: Train batch 675: loss: 19.16 gradNorm: 33.54 \n",
      "2025-03-13 21:55:36,259: Train batch 676: loss: 28.86 gradNorm: 54.84 \n",
      "2025-03-13 21:55:36,346: Train batch 677: loss: 20.03 gradNorm: 32.07 \n",
      "2025-03-13 21:55:36,435: Train batch 678: loss: 19.16 gradNorm: 26.52 \n",
      "2025-03-13 21:55:36,618: Train batch 679: loss: 18.92 gradNorm: 36.32 \n",
      "2025-03-13 21:55:36,714: Train batch 680: loss: 16.16 gradNorm: 27.50 \n",
      "2025-03-13 21:55:36,881: Train batch 681: loss: 22.12 gradNorm: 30.27 \n",
      "2025-03-13 21:55:36,974: Train batch 682: loss: 21.03 gradNorm: 31.95 \n",
      "2025-03-13 21:55:37,141: Train batch 683: loss: 17.03 gradNorm: 29.68 \n",
      "2025-03-13 21:55:37,490: Train batch 684: loss: 29.83 gradNorm: 67.93 \n",
      "2025-03-13 21:55:37,571: Train batch 685: loss: 22.75 gradNorm: 45.25 \n",
      "2025-03-13 21:55:37,650: Train batch 686: loss: 21.03 gradNorm: 38.41 \n",
      "2025-03-13 21:55:37,875: Train batch 687: loss: 23.70 gradNorm: 40.98 \n",
      "2025-03-13 21:55:38,146: Train batch 688: loss: 25.69 gradNorm: 48.57 \n",
      "2025-03-13 21:55:38,377: Train batch 689: loss: 25.83 gradNorm: 41.44 \n",
      "2025-03-13 21:55:38,473: Train batch 690: loss: 17.76 gradNorm: 33.47 \n",
      "2025-03-13 21:55:38,542: Train batch 691: loss: 20.39 gradNorm: 32.73 \n",
      "2025-03-13 21:55:38,655: Train batch 692: loss: 17.44 gradNorm: 26.64 \n",
      "2025-03-13 21:55:39,003: Train batch 693: loss: 26.18 gradNorm: 39.80 \n",
      "2025-03-13 21:55:39,077: Train batch 694: loss: 19.27 gradNorm: 35.45 \n",
      "2025-03-13 21:55:39,164: Train batch 695: loss: 19.47 gradNorm: 36.33 \n",
      "2025-03-13 21:55:39,244: Train batch 696: loss: 16.48 gradNorm: 25.72 \n",
      "2025-03-13 21:55:39,440: Train batch 697: loss: 25.11 gradNorm: 47.62 \n",
      "2025-03-13 21:55:39,550: Train batch 698: loss: 15.83 gradNorm: 26.46 \n",
      "2025-03-13 21:55:39,745: Train batch 699: loss: 27.48 gradNorm: 70.63 \n",
      "2025-03-13 21:55:39,813: Train batch 700: loss: 16.68 gradNorm: 30.93 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:55:40,019: Val batch: CER (t18.2025.01.15): 0.155\n",
      "2025-03-13 21:55:40,020: Val batch 700: CER (avg): 0.155 \n",
      "2025-03-13 21:55:40,021: Batches since validation CER improved: 50\n",
      "2025-03-13 21:55:40,107: Train batch 701: loss: 16.80 gradNorm: 31.34 \n",
      "2025-03-13 21:55:40,174: Train batch 702: loss: 16.18 gradNorm: 27.67 \n",
      "2025-03-13 21:55:40,365: Train batch 703: loss: 18.60 gradNorm: 37.27 \n",
      "2025-03-13 21:55:40,444: Train batch 704: loss: 16.65 gradNorm: 26.01 \n",
      "2025-03-13 21:55:40,555: Train batch 705: loss: 15.80 gradNorm: 26.56 \n",
      "2025-03-13 21:55:40,659: Train batch 706: loss: 17.06 gradNorm: 29.91 \n",
      "2025-03-13 21:55:40,760: Train batch 707: loss: 19.80 gradNorm: 29.63 \n",
      "2025-03-13 21:55:41,024: Train batch 708: loss: 64.47 gradNorm: 274.99 \n",
      "2025-03-13 21:55:41,101: Train batch 709: loss: 19.40 gradNorm: 41.13 \n",
      "2025-03-13 21:55:41,277: Train batch 710: loss: 20.83 gradNorm: 40.66 \n",
      "2025-03-13 21:55:41,360: Train batch 711: loss: 19.55 gradNorm: 34.02 \n",
      "2025-03-13 21:55:41,493: Train batch 712: loss: 20.28 gradNorm: 36.24 \n",
      "2025-03-13 21:55:41,680: Train batch 713: loss: 18.54 gradNorm: 41.14 \n",
      "2025-03-13 21:55:41,951: Train batch 714: loss: 39.18 gradNorm: 96.87 \n",
      "2025-03-13 21:55:42,344: Train batch 715: loss: 29.93 gradNorm: 75.97 \n",
      "2025-03-13 21:55:42,431: Train batch 716: loss: 22.15 gradNorm: 38.82 \n",
      "2025-03-13 21:55:42,511: Train batch 717: loss: 27.20 gradNorm: 66.41 \n",
      "2025-03-13 21:55:42,605: Train batch 718: loss: 18.45 gradNorm: 38.01 \n",
      "2025-03-13 21:55:42,694: Train batch 719: loss: 20.78 gradNorm: 42.91 \n",
      "2025-03-13 21:55:43,066: Train batch 720: loss: 28.30 gradNorm: 71.03 \n",
      "2025-03-13 21:55:43,330: Train batch 721: loss: 36.60 gradNorm: 85.06 \n",
      "2025-03-13 21:55:43,526: Train batch 722: loss: 23.14 gradNorm: 44.98 \n",
      "2025-03-13 21:55:43,607: Train batch 723: loss: 23.26 gradNorm: 55.16 \n",
      "2025-03-13 21:55:43,704: Train batch 724: loss: 25.57 gradNorm: 59.63 \n",
      "2025-03-13 21:55:43,769: Train batch 725: loss: 21.88 gradNorm: 52.36 \n",
      "2025-03-13 21:55:44,027: Train batch 726: loss: 35.15 gradNorm: 81.45 \n",
      "2025-03-13 21:55:44,312: Train batch 727: loss: 32.94 gradNorm: 87.01 \n",
      "2025-03-13 21:55:44,585: Train batch 728: loss: 28.56 gradNorm: 55.34 \n",
      "2025-03-13 21:55:44,739: Train batch 729: loss: 20.75 gradNorm: 50.76 \n",
      "2025-03-13 21:55:44,899: Train batch 730: loss: 26.42 gradNorm: 65.35 \n",
      "2025-03-13 21:55:45,095: Train batch 731: loss: 22.25 gradNorm: 53.79 \n",
      "2025-03-13 21:55:45,216: Train batch 732: loss: 19.94 gradNorm: 41.85 \n",
      "2025-03-13 21:55:45,290: Train batch 733: loss: 18.43 gradNorm: 31.51 \n",
      "2025-03-13 21:55:45,371: Train batch 734: loss: 18.93 gradNorm: 32.90 \n",
      "2025-03-13 21:55:45,633: Train batch 735: loss: 29.31 gradNorm: 97.30 \n",
      "2025-03-13 21:55:46,059: Train batch 736: loss: 35.11 gradNorm: 108.68 \n",
      "2025-03-13 21:55:46,193: Train batch 737: loss: 20.43 gradNorm: 37.37 \n",
      "2025-03-13 21:55:46,261: Train batch 738: loss: 20.03 gradNorm: 39.84 \n",
      "2025-03-13 21:55:46,361: Train batch 739: loss: 18.91 gradNorm: 34.24 \n",
      "2025-03-13 21:55:46,620: Train batch 740: loss: 41.81 gradNorm: 97.21 \n",
      "2025-03-13 21:55:46,702: Train batch 741: loss: 20.09 gradNorm: 33.38 \n",
      "2025-03-13 21:55:46,896: Train batch 742: loss: 22.86 gradNorm: 45.75 \n",
      "2025-03-13 21:55:47,005: Train batch 743: loss: 20.74 gradNorm: 41.11 \n",
      "2025-03-13 21:55:47,280: Train batch 744: loss: 34.35 gradNorm: 82.10 \n",
      "2025-03-13 21:55:47,346: Train batch 745: loss: 17.06 gradNorm: 33.85 \n",
      "2025-03-13 21:55:47,716: Train batch 746: loss: 30.32 gradNorm: 78.73 \n",
      "2025-03-13 21:55:47,841: Train batch 747: loss: 17.44 gradNorm: 32.65 \n",
      "2025-03-13 21:55:47,929: Train batch 748: loss: 21.26 gradNorm: 49.42 \n",
      "2025-03-13 21:55:48,042: Train batch 749: loss: 18.48 gradNorm: 33.59 \n",
      "2025-03-13 21:55:48,453: Train batch 750: loss: 30.13 gradNorm: 69.54 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:55:48,653: Val batch: CER (t18.2025.01.15): 0.152\n",
      "2025-03-13 21:55:48,654: Val batch 750: CER (avg): 0.152 \n",
      "2025-03-13 21:55:48,654: Batches since validation CER improved: 100\n",
      "2025-03-13 21:55:48,782: Train batch 751: loss: 16.02 gradNorm: 29.52 \n",
      "2025-03-13 21:55:48,881: Train batch 752: loss: 19.17 gradNorm: 26.79 \n",
      "2025-03-13 21:55:48,964: Train batch 753: loss: 18.02 gradNorm: 33.67 \n",
      "2025-03-13 21:55:49,060: Train batch 754: loss: 17.66 gradNorm: 29.46 \n",
      "2025-03-13 21:55:49,266: Train batch 755: loss: 22.23 gradNorm: 40.54 \n",
      "2025-03-13 21:55:49,607: Train batch 756: loss: 32.56 gradNorm: 109.31 \n",
      "2025-03-13 21:55:49,760: Train batch 757: loss: 16.68 gradNorm: 34.43 \n",
      "2025-03-13 21:55:49,859: Train batch 758: loss: 16.38 gradNorm: 32.55 \n",
      "2025-03-13 21:55:49,958: Train batch 759: loss: 16.73 gradNorm: 31.40 \n",
      "2025-03-13 21:55:50,039: Train batch 760: loss: 17.90 gradNorm: 31.39 \n",
      "2025-03-13 21:55:50,192: Train batch 761: loss: 16.64 gradNorm: 27.68 \n",
      "2025-03-13 21:55:50,255: Train batch 762: loss: 16.45 gradNorm: 28.82 \n",
      "2025-03-13 21:55:50,387: Train batch 763: loss: 14.82 gradNorm: 26.21 \n",
      "2025-03-13 21:55:50,501: Train batch 764: loss: 18.63 gradNorm: 27.09 \n",
      "2025-03-13 21:55:50,678: Train batch 765: loss: 13.56 gradNorm: 26.40 \n",
      "2025-03-13 21:55:50,952: Train batch 766: loss: 27.97 gradNorm: 61.69 \n",
      "2025-03-13 21:55:51,026: Train batch 767: loss: 22.48 gradNorm: 51.83 \n",
      "2025-03-13 21:55:51,137: Train batch 768: loss: 16.41 gradNorm: 31.29 \n",
      "2025-03-13 21:55:51,235: Train batch 769: loss: 15.96 gradNorm: 26.73 \n",
      "2025-03-13 21:55:51,396: Train batch 770: loss: 22.41 gradNorm: 41.77 \n",
      "2025-03-13 21:55:51,813: Train batch 771: loss: 25.99 gradNorm: 61.76 \n",
      "2025-03-13 21:55:52,053: Train batch 772: loss: 23.17 gradNorm: 51.91 \n",
      "2025-03-13 21:55:52,135: Train batch 773: loss: 18.84 gradNorm: 39.87 \n",
      "2025-03-13 21:55:52,232: Train batch 774: loss: 18.28 gradNorm: 38.53 \n",
      "2025-03-13 21:55:52,323: Train batch 775: loss: 17.23 gradNorm: 32.21 \n",
      "2025-03-13 21:55:52,394: Train batch 776: loss: 15.74 gradNorm: 23.81 \n",
      "2025-03-13 21:55:52,688: Train batch 777: loss: 24.13 gradNorm: 46.45 \n",
      "2025-03-13 21:55:52,778: Train batch 778: loss: 17.31 gradNorm: 31.29 \n",
      "2025-03-13 21:55:52,914: Train batch 779: loss: 15.87 gradNorm: 33.82 \n",
      "2025-03-13 21:55:53,123: Train batch 780: loss: 22.27 gradNorm: 37.59 \n",
      "2025-03-13 21:55:53,543: Train batch 781: loss: 25.46 gradNorm: 40.09 \n",
      "2025-03-13 21:55:53,697: Train batch 782: loss: 15.32 gradNorm: 32.56 \n",
      "2025-03-13 21:55:53,847: Train batch 783: loss: 14.34 gradNorm: 27.83 \n",
      "2025-03-13 21:55:53,975: Train batch 784: loss: 14.50 gradNorm: 25.70 \n",
      "2025-03-13 21:55:54,277: Train batch 785: loss: 27.36 gradNorm: 56.53 \n",
      "2025-03-13 21:55:54,361: Train batch 786: loss: 17.12 gradNorm: 33.98 \n",
      "2025-03-13 21:55:54,623: Train batch 787: loss: 29.35 gradNorm: 85.96 \n",
      "2025-03-13 21:55:54,962: Train batch 788: loss: 24.14 gradNorm: 41.88 \n",
      "2025-03-13 21:55:55,051: Train batch 789: loss: 15.85 gradNorm: 30.21 \n",
      "2025-03-13 21:55:55,146: Train batch 790: loss: 16.90 gradNorm: 25.42 \n",
      "2025-03-13 21:55:55,225: Train batch 791: loss: 21.06 gradNorm: 47.02 \n",
      "2025-03-13 21:55:55,336: Train batch 792: loss: 14.39 gradNorm: 26.65 \n",
      "2025-03-13 21:55:55,752: Train batch 793: loss: 26.73 gradNorm: 45.59 \n",
      "2025-03-13 21:55:55,865: Train batch 794: loss: 13.98 gradNorm: 22.57 \n",
      "2025-03-13 21:55:56,060: Train batch 795: loss: 18.87 gradNorm: 34.15 \n",
      "2025-03-13 21:55:56,189: Train batch 796: loss: 15.41 gradNorm: 22.95 \n",
      "2025-03-13 21:55:56,348: Train batch 797: loss: 18.26 gradNorm: 28.49 \n",
      "2025-03-13 21:55:56,452: Train batch 798: loss: 17.44 gradNorm: 27.68 \n",
      "2025-03-13 21:55:56,557: Train batch 799: loss: 22.12 gradNorm: 36.61 \n",
      "2025-03-13 21:55:56,715: Train batch 800: loss: 18.32 gradNorm: 48.93 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:55:56,914: Val batch: CER (t18.2025.01.15): 0.132\n",
      "2025-03-13 21:55:56,915: Val batch 800: CER (avg): 0.132 \n",
      "2025-03-13 21:55:57,136: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:55:57,139: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:55:57,140: Batches since validation CER improved: 0\n",
      "2025-03-13 21:55:57,229: Train batch 801: loss: 16.81 gradNorm: 34.74 \n",
      "2025-03-13 21:55:57,313: Train batch 802: loss: 14.61 gradNorm: 28.49 \n",
      "2025-03-13 21:55:57,415: Train batch 803: loss: 12.91 gradNorm: 21.25 \n",
      "2025-03-13 21:55:57,545: Train batch 804: loss: 12.10 gradNorm: 28.15 \n",
      "2025-03-13 21:55:57,635: Train batch 805: loss: 14.53 gradNorm: 21.96 \n",
      "2025-03-13 21:55:57,708: Train batch 806: loss: 15.48 gradNorm: 26.22 \n",
      "2025-03-13 21:55:57,831: Train batch 807: loss: 15.00 gradNorm: 23.60 \n",
      "2025-03-13 21:55:57,906: Train batch 808: loss: 16.28 gradNorm: 26.17 \n",
      "2025-03-13 21:55:58,207: Train batch 809: loss: 29.76 gradNorm: 71.50 \n",
      "2025-03-13 21:55:58,375: Train batch 810: loss: 14.31 gradNorm: 30.77 \n",
      "2025-03-13 21:55:58,455: Train batch 811: loss: 15.10 gradNorm: 30.64 \n",
      "2025-03-13 21:55:58,527: Train batch 812: loss: 14.56 gradNorm: 24.90 \n",
      "2025-03-13 21:55:58,740: Train batch 813: loss: 26.00 gradNorm: 74.21 \n",
      "2025-03-13 21:55:58,872: Train batch 814: loss: 17.26 gradNorm: 37.13 \n",
      "2025-03-13 21:55:58,961: Train batch 815: loss: 16.11 gradNorm: 29.76 \n",
      "2025-03-13 21:55:59,057: Train batch 816: loss: 15.14 gradNorm: 27.69 \n",
      "2025-03-13 21:55:59,139: Train batch 817: loss: 16.27 gradNorm: 32.40 \n",
      "2025-03-13 21:55:59,324: Train batch 818: loss: 19.58 gradNorm: 38.81 \n",
      "2025-03-13 21:55:59,410: Train batch 819: loss: 13.16 gradNorm: 27.01 \n",
      "2025-03-13 21:55:59,519: Train batch 820: loss: 17.06 gradNorm: 27.09 \n",
      "2025-03-13 21:55:59,600: Train batch 821: loss: 18.79 gradNorm: 35.42 \n",
      "2025-03-13 21:55:59,779: Train batch 822: loss: 15.34 gradNorm: 24.26 \n",
      "2025-03-13 21:56:00,200: Train batch 823: loss: 21.49 gradNorm: 62.83 \n",
      "2025-03-13 21:56:00,302: Train batch 824: loss: 12.70 gradNorm: 22.24 \n",
      "2025-03-13 21:56:00,656: Train batch 825: loss: 23.83 gradNorm: 32.80 \n",
      "2025-03-13 21:56:00,757: Train batch 826: loss: 15.13 gradNorm: 23.08 \n",
      "2025-03-13 21:56:00,840: Train batch 827: loss: 13.45 gradNorm: 24.20 \n",
      "2025-03-13 21:56:00,941: Train batch 828: loss: 13.65 gradNorm: 24.79 \n",
      "2025-03-13 21:56:01,041: Train batch 829: loss: 13.65 gradNorm: 24.67 \n",
      "2025-03-13 21:56:01,122: Train batch 830: loss: 15.21 gradNorm: 20.93 \n",
      "2025-03-13 21:56:01,445: Train batch 831: loss: 27.40 gradNorm: 85.54 \n",
      "2025-03-13 21:56:01,544: Train batch 832: loss: 12.70 gradNorm: 27.01 \n",
      "2025-03-13 21:56:01,846: Train batch 833: loss: 23.78 gradNorm: 46.87 \n",
      "2025-03-13 21:56:01,931: Train batch 834: loss: 15.61 gradNorm: 34.61 \n",
      "2025-03-13 21:56:02,013: Train batch 835: loss: 15.05 gradNorm: 26.38 \n",
      "2025-03-13 21:56:02,115: Train batch 836: loss: 19.85 gradNorm: 74.25 \n",
      "2025-03-13 21:56:02,194: Train batch 837: loss: 13.84 gradNorm: 25.60 \n",
      "2025-03-13 21:56:02,440: Train batch 838: loss: 21.31 gradNorm: 50.86 \n",
      "2025-03-13 21:56:02,510: Train batch 839: loss: 13.90 gradNorm: 27.35 \n",
      "2025-03-13 21:56:02,812: Train batch 840: loss: 22.60 gradNorm: 51.51 \n",
      "2025-03-13 21:56:03,200: Train batch 841: loss: 23.52 gradNorm: 51.20 \n",
      "2025-03-13 21:56:03,368: Train batch 842: loss: 15.61 gradNorm: 35.33 \n",
      "2025-03-13 21:56:03,563: Train batch 843: loss: 17.03 gradNorm: 33.39 \n",
      "2025-03-13 21:56:03,662: Train batch 844: loss: 19.08 gradNorm: 47.54 \n",
      "2025-03-13 21:56:03,745: Train batch 845: loss: 16.26 gradNorm: 37.58 \n",
      "2025-03-13 21:56:03,824: Train batch 846: loss: 17.12 gradNorm: 31.83 \n",
      "2025-03-13 21:56:04,020: Train batch 847: loss: 20.43 gradNorm: 55.72 \n",
      "2025-03-13 21:56:04,136: Train batch 848: loss: 21.59 gradNorm: 46.61 \n",
      "2025-03-13 21:56:04,226: Train batch 849: loss: 16.45 gradNorm: 33.85 \n",
      "2025-03-13 21:56:04,487: Train batch 850: loss: 29.44 gradNorm: 92.61 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:56:04,697: Val batch: CER (t18.2025.01.15): 0.166\n",
      "2025-03-13 21:56:04,699: Val batch 850: CER (avg): 0.166 \n",
      "2025-03-13 21:56:04,699: Batches since validation CER improved: 50\n",
      "2025-03-13 21:56:04,786: Train batch 851: loss: 12.64 gradNorm: 24.80 \n",
      "2025-03-13 21:56:05,058: Train batch 852: loss: 26.04 gradNorm: 47.13 \n",
      "2025-03-13 21:56:05,213: Train batch 853: loss: 16.36 gradNorm: 38.14 \n",
      "2025-03-13 21:56:05,347: Train batch 854: loss: 13.00 gradNorm: 25.41 \n",
      "2025-03-13 21:56:05,502: Train batch 855: loss: 10.64 gradNorm: 20.39 \n",
      "2025-03-13 21:56:05,681: Train batch 856: loss: 18.52 gradNorm: 38.08 \n",
      "2025-03-13 21:56:05,940: Train batch 857: loss: 38.28 gradNorm: 108.87 \n",
      "2025-03-13 21:56:06,041: Train batch 858: loss: 17.71 gradNorm: 35.00 \n",
      "2025-03-13 21:56:06,149: Train batch 859: loss: 16.61 gradNorm: 28.13 \n",
      "2025-03-13 21:56:06,424: Train batch 860: loss: 29.00 gradNorm: 66.51 \n",
      "2025-03-13 21:56:06,701: Train batch 861: loss: 26.78 gradNorm: 56.84 \n",
      "2025-03-13 21:56:06,776: Train batch 862: loss: 18.93 gradNorm: 43.08 \n",
      "2025-03-13 21:56:06,898: Train batch 863: loss: 13.78 gradNorm: 33.58 \n",
      "2025-03-13 21:56:06,976: Train batch 864: loss: 17.92 gradNorm: 41.43 \n",
      "2025-03-13 21:56:07,057: Train batch 865: loss: 16.71 gradNorm: 32.73 \n",
      "2025-03-13 21:56:07,160: Train batch 866: loss: 26.17 gradNorm: 67.09 \n",
      "2025-03-13 21:56:07,546: Train batch 867: loss: 34.95 gradNorm: 111.84 \n",
      "2025-03-13 21:56:07,637: Train batch 868: loss: 15.99 gradNorm: 36.19 \n",
      "2025-03-13 21:56:07,718: Train batch 869: loss: 17.17 gradNorm: 34.07 \n",
      "2025-03-13 21:56:07,809: Train batch 870: loss: 16.84 gradNorm: 31.41 \n",
      "2025-03-13 21:56:07,899: Train batch 871: loss: 16.05 gradNorm: 30.43 \n",
      "2025-03-13 21:56:08,040: Train batch 872: loss: 19.69 gradNorm: 38.16 \n",
      "2025-03-13 21:56:08,336: Train batch 873: loss: 17.93 gradNorm: 33.35 \n",
      "2025-03-13 21:56:08,587: Train batch 874: loss: 24.75 gradNorm: 50.81 \n",
      "2025-03-13 21:56:08,667: Train batch 875: loss: 18.20 gradNorm: 37.45 \n",
      "2025-03-13 21:56:08,830: Train batch 876: loss: 29.49 gradNorm: 81.03 \n",
      "2025-03-13 21:56:09,186: Train batch 877: loss: 21.48 gradNorm: 50.39 \n",
      "2025-03-13 21:56:09,301: Train batch 878: loss: 17.07 gradNorm: 32.89 \n",
      "2025-03-13 21:56:09,378: Train batch 879: loss: 16.17 gradNorm: 32.07 \n",
      "2025-03-13 21:56:09,636: Train batch 880: loss: 20.19 gradNorm: 38.93 \n",
      "2025-03-13 21:56:09,735: Train batch 881: loss: 14.37 gradNorm: 29.38 \n",
      "2025-03-13 21:56:09,831: Train batch 882: loss: 14.52 gradNorm: 25.44 \n",
      "2025-03-13 21:56:09,962: Train batch 883: loss: 14.86 gradNorm: 32.18 \n",
      "2025-03-13 21:56:10,317: Train batch 884: loss: 23.45 gradNorm: 40.26 \n",
      "2025-03-13 21:56:10,397: Train batch 885: loss: 17.07 gradNorm: 35.92 \n",
      "2025-03-13 21:56:10,477: Train batch 886: loss: 15.96 gradNorm: 27.12 \n",
      "2025-03-13 21:56:10,604: Train batch 887: loss: 11.67 gradNorm: 27.57 \n",
      "2025-03-13 21:56:10,692: Train batch 888: loss: 16.21 gradNorm: 31.37 \n",
      "2025-03-13 21:56:10,785: Train batch 889: loss: 14.78 gradNorm: 27.34 \n",
      "2025-03-13 21:56:10,860: Train batch 890: loss: 13.54 gradNorm: 25.54 \n",
      "2025-03-13 21:56:10,941: Train batch 891: loss: 14.05 gradNorm: 24.48 \n",
      "2025-03-13 21:56:11,113: Train batch 892: loss: 17.09 gradNorm: 35.65 \n",
      "2025-03-13 21:56:11,218: Train batch 893: loss: 15.31 gradNorm: 26.09 \n",
      "2025-03-13 21:56:11,314: Train batch 894: loss: 12.85 gradNorm: 22.53 \n",
      "2025-03-13 21:56:11,508: Train batch 895: loss: 15.01 gradNorm: 25.89 \n",
      "2025-03-13 21:56:11,593: Train batch 896: loss: 15.19 gradNorm: 26.16 \n",
      "2025-03-13 21:56:11,691: Train batch 897: loss: 12.91 gradNorm: 24.19 \n",
      "2025-03-13 21:56:11,791: Train batch 898: loss: 10.54 gradNorm: 16.80 \n",
      "2025-03-13 21:56:11,903: Train batch 899: loss: 16.24 gradNorm: 28.36 \n",
      "2025-03-13 21:56:11,997: Train batch 900: loss: 14.45 gradNorm: 28.69 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:56:12,200: Val batch: CER (t18.2025.01.15): 0.128\n",
      "2025-03-13 21:56:12,201: Val batch 900: CER (avg): 0.128 \n",
      "2025-03-13 21:56:12,425: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:56:12,429: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:56:12,429: Batches since validation CER improved: 0\n",
      "2025-03-13 21:56:12,503: Train batch 901: loss: 15.57 gradNorm: 25.95 \n",
      "2025-03-13 21:56:12,683: Train batch 902: loss: 14.83 gradNorm: 39.59 \n",
      "2025-03-13 21:56:12,790: Train batch 903: loss: 11.84 gradNorm: 22.30 \n",
      "2025-03-13 21:56:13,024: Train batch 904: loss: 21.96 gradNorm: 48.63 \n",
      "2025-03-13 21:56:13,105: Train batch 905: loss: 14.16 gradNorm: 25.72 \n",
      "2025-03-13 21:56:13,261: Train batch 906: loss: 12.66 gradNorm: 25.23 \n",
      "2025-03-13 21:56:13,349: Train batch 907: loss: 13.52 gradNorm: 25.86 \n",
      "2025-03-13 21:56:13,464: Train batch 908: loss: 11.79 gradNorm: 22.11 \n",
      "2025-03-13 21:56:13,817: Train batch 909: loss: 16.70 gradNorm: 38.41 \n",
      "2025-03-13 21:56:13,905: Train batch 910: loss: 13.30 gradNorm: 22.73 \n",
      "2025-03-13 21:56:14,224: Train batch 911: loss: 23.23 gradNorm: 51.06 \n",
      "2025-03-13 21:56:14,372: Train batch 912: loss: 11.62 gradNorm: 29.63 \n",
      "2025-03-13 21:56:14,445: Train batch 913: loss: 13.20 gradNorm: 27.21 \n",
      "2025-03-13 21:56:14,539: Train batch 914: loss: 13.58 gradNorm: 28.70 \n",
      "2025-03-13 21:56:14,679: Train batch 915: loss: 19.17 gradNorm: 42.73 \n",
      "2025-03-13 21:56:14,843: Train batch 916: loss: 12.22 gradNorm: 26.44 \n",
      "2025-03-13 21:56:14,920: Train batch 917: loss: 14.65 gradNorm: 28.55 \n",
      "2025-03-13 21:56:15,041: Train batch 918: loss: 12.87 gradNorm: 22.36 \n",
      "2025-03-13 21:56:15,153: Train batch 919: loss: 11.19 gradNorm: 18.87 \n",
      "2025-03-13 21:56:15,251: Train batch 920: loss: 12.44 gradNorm: 23.65 \n",
      "2025-03-13 21:56:15,330: Train batch 921: loss: 17.16 gradNorm: 39.03 \n",
      "2025-03-13 21:56:15,421: Train batch 922: loss: 11.85 gradNorm: 23.37 \n",
      "2025-03-13 21:56:15,519: Train batch 923: loss: 13.62 gradNorm: 24.66 \n",
      "2025-03-13 21:56:15,864: Train batch 924: loss: 21.08 gradNorm: 44.22 \n",
      "2025-03-13 21:56:16,070: Train batch 925: loss: 18.16 gradNorm: 36.35 \n",
      "2025-03-13 21:56:16,307: Train batch 926: loss: 15.87 gradNorm: 30.51 \n",
      "2025-03-13 21:56:16,467: Train batch 927: loss: 13.15 gradNorm: 25.29 \n",
      "2025-03-13 21:56:16,550: Train batch 928: loss: 17.58 gradNorm: 47.52 \n",
      "2025-03-13 21:56:16,655: Train batch 929: loss: 13.87 gradNorm: 29.80 \n",
      "2025-03-13 21:56:16,758: Train batch 930: loss: 14.36 gradNorm: 27.03 \n",
      "2025-03-13 21:56:16,838: Train batch 931: loss: 13.28 gradNorm: 28.76 \n",
      "2025-03-13 21:56:17,206: Train batch 932: loss: 22.99 gradNorm: 56.06 \n",
      "2025-03-13 21:56:17,576: Train batch 933: loss: 23.57 gradNorm: 39.23 \n",
      "2025-03-13 21:56:17,847: Train batch 934: loss: 29.85 gradNorm: 78.19 \n",
      "2025-03-13 21:56:17,938: Train batch 935: loss: 16.58 gradNorm: 40.26 \n",
      "2025-03-13 21:56:18,212: Train batch 936: loss: 23.95 gradNorm: 45.38 \n",
      "2025-03-13 21:56:18,575: Train batch 937: loss: 19.51 gradNorm: 41.53 \n",
      "2025-03-13 21:56:18,701: Train batch 938: loss: 15.36 gradNorm: 38.88 \n",
      "2025-03-13 21:56:18,783: Train batch 939: loss: 15.44 gradNorm: 37.61 \n",
      "2025-03-13 21:56:18,860: Train batch 940: loss: 14.37 gradNorm: 30.37 \n",
      "2025-03-13 21:56:19,132: Train batch 941: loss: 33.56 gradNorm: 110.67 \n",
      "2025-03-13 21:56:19,290: Train batch 942: loss: 13.06 gradNorm: 35.77 \n",
      "2025-03-13 21:56:19,401: Train batch 943: loss: 12.47 gradNorm: 27.47 \n",
      "2025-03-13 21:56:19,596: Train batch 944: loss: 14.38 gradNorm: 29.47 \n",
      "2025-03-13 21:56:19,666: Train batch 945: loss: 15.58 gradNorm: 34.51 \n",
      "2025-03-13 21:56:19,772: Train batch 946: loss: 14.80 gradNorm: 36.57 \n",
      "2025-03-13 21:56:19,996: Train batch 947: loss: 20.71 gradNorm: 47.89 \n",
      "2025-03-13 21:56:20,101: Train batch 948: loss: 13.98 gradNorm: 25.47 \n",
      "2025-03-13 21:56:20,258: Train batch 949: loss: 13.49 gradNorm: 33.64 \n",
      "2025-03-13 21:56:20,370: Train batch 950: loss: 12.13 gradNorm: 23.92 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:56:20,561: Val batch: CER (t18.2025.01.15): 0.110\n",
      "2025-03-13 21:56:20,562: Val batch 950: CER (avg): 0.110 \n",
      "2025-03-13 21:56:20,776: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:56:20,780: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:56:20,780: Batches since validation CER improved: 0\n",
      "2025-03-13 21:56:20,866: Train batch 951: loss: 13.95 gradNorm: 27.38 \n",
      "2025-03-13 21:56:20,953: Train batch 952: loss: 12.66 gradNorm: 27.19 \n",
      "2025-03-13 21:56:21,055: Train batch 953: loss: 11.84 gradNorm: 23.85 \n",
      "2025-03-13 21:56:21,135: Train batch 954: loss: 11.85 gradNorm: 25.22 \n",
      "2025-03-13 21:56:21,325: Train batch 955: loss: 15.74 gradNorm: 31.47 \n",
      "2025-03-13 21:56:21,610: Train batch 956: loss: 19.95 gradNorm: 45.93 \n",
      "2025-03-13 21:56:21,699: Train batch 957: loss: 12.57 gradNorm: 26.92 \n",
      "2025-03-13 21:56:21,801: Train batch 958: loss: 13.29 gradNorm: 27.34 \n",
      "2025-03-13 21:56:21,897: Train batch 959: loss: 13.58 gradNorm: 32.19 \n",
      "2025-03-13 21:56:22,066: Train batch 960: loss: 12.91 gradNorm: 33.10 \n",
      "2025-03-13 21:56:22,487: Train batch 961: loss: 21.44 gradNorm: 42.10 \n",
      "2025-03-13 21:56:22,837: Train batch 962: loss: 14.45 gradNorm: 29.52 \n",
      "2025-03-13 21:56:23,101: Train batch 963: loss: 16.40 gradNorm: 27.92 \n",
      "2025-03-13 21:56:23,292: Train batch 964: loss: 15.18 gradNorm: 25.87 \n",
      "2025-03-13 21:56:23,375: Train batch 965: loss: 14.39 gradNorm: 36.10 \n",
      "2025-03-13 21:56:23,552: Train batch 966: loss: 14.85 gradNorm: 31.80 \n",
      "2025-03-13 21:56:23,618: Train batch 967: loss: 14.31 gradNorm: 31.20 \n",
      "2025-03-13 21:56:23,708: Train batch 968: loss: 11.31 gradNorm: 22.66 \n",
      "2025-03-13 21:56:23,791: Train batch 969: loss: 10.72 gradNorm: 21.33 \n",
      "2025-03-13 21:56:23,895: Train batch 970: loss: 12.05 gradNorm: 24.28 \n",
      "2025-03-13 21:56:23,971: Train batch 971: loss: 10.58 gradNorm: 23.74 \n",
      "2025-03-13 21:56:24,068: Train batch 972: loss: 11.71 gradNorm: 22.32 \n",
      "2025-03-13 21:56:24,139: Train batch 973: loss: 13.03 gradNorm: 25.60 \n",
      "2025-03-13 21:56:24,312: Train batch 974: loss: 11.37 gradNorm: 32.69 \n",
      "2025-03-13 21:56:24,690: Train batch 975: loss: 32.27 gradNorm: 80.98 \n",
      "2025-03-13 21:56:24,820: Train batch 976: loss: 11.20 gradNorm: 22.85 \n",
      "2025-03-13 21:56:24,900: Train batch 977: loss: 14.92 gradNorm: 27.49 \n",
      "2025-03-13 21:56:25,174: Train batch 978: loss: 20.50 gradNorm: 38.56 \n",
      "2025-03-13 21:56:25,521: Train batch 979: loss: 31.63 gradNorm: 75.09 \n",
      "2025-03-13 21:56:25,672: Train batch 980: loss: 13.37 gradNorm: 37.11 \n",
      "2025-03-13 21:56:25,947: Train batch 981: loss: 20.91 gradNorm: 41.31 \n",
      "2025-03-13 21:56:26,034: Train batch 982: loss: 10.30 gradNorm: 21.61 \n",
      "2025-03-13 21:56:26,225: Train batch 983: loss: 22.79 gradNorm: 51.59 \n",
      "2025-03-13 21:56:26,565: Train batch 984: loss: 17.11 gradNorm: 37.39 \n",
      "2025-03-13 21:56:26,696: Train batch 985: loss: 12.79 gradNorm: 24.25 \n",
      "2025-03-13 21:56:26,846: Train batch 986: loss: 10.62 gradNorm: 28.93 \n",
      "2025-03-13 21:56:26,930: Train batch 987: loss: 14.93 gradNorm: 36.41 \n",
      "2025-03-13 21:56:27,010: Train batch 988: loss: 18.03 gradNorm: 42.04 \n",
      "2025-03-13 21:56:27,145: Train batch 989: loss: 10.85 gradNorm: 24.92 \n",
      "2025-03-13 21:56:27,288: Train batch 990: loss: 17.87 gradNorm: 43.82 \n",
      "2025-03-13 21:56:27,390: Train batch 991: loss: 10.58 gradNorm: 23.26 \n",
      "2025-03-13 21:56:27,585: Train batch 992: loss: 13.44 gradNorm: 24.68 \n",
      "2025-03-13 21:56:27,745: Train batch 993: loss: 10.78 gradNorm: 25.46 \n",
      "2025-03-13 21:56:27,834: Train batch 994: loss: 12.42 gradNorm: 27.08 \n",
      "2025-03-13 21:56:28,007: Train batch 995: loss: 12.74 gradNorm: 25.47 \n",
      "2025-03-13 21:56:28,088: Train batch 996: loss: 10.42 gradNorm: 23.11 \n",
      "2025-03-13 21:56:28,216: Train batch 997: loss: 12.31 gradNorm: 25.21 \n",
      "2025-03-13 21:56:28,303: Train batch 998: loss: 10.30 gradNorm: 22.96 \n",
      "2025-03-13 21:56:28,578: Train batch 999: loss: 24.31 gradNorm: 59.88 \n",
      "2025-03-13 21:56:28,743: Train batch 1000: loss: 12.58 gradNorm: 34.56 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:56:28,938: Val batch: CER (t18.2025.01.15): 0.120\n",
      "2025-03-13 21:56:28,940: Val batch 1000: CER (avg): 0.120 \n",
      "2025-03-13 21:56:28,940: Batches since validation CER improved: 50\n",
      "2025-03-13 21:56:29,292: Train batch 1001: loss: 18.42 gradNorm: 43.64 \n",
      "2025-03-13 21:56:29,405: Train batch 1002: loss: 12.43 gradNorm: 27.11 \n",
      "2025-03-13 21:56:29,519: Train batch 1003: loss: 7.98 gradNorm: 20.11 \n",
      "2025-03-13 21:56:29,706: Train batch 1004: loss: 10.15 gradNorm: 23.76 \n",
      "2025-03-13 21:56:29,818: Train batch 1005: loss: 12.25 gradNorm: 20.50 \n",
      "2025-03-13 21:56:29,888: Train batch 1006: loss: 12.67 gradNorm: 30.01 \n",
      "2025-03-13 21:56:30,040: Train batch 1007: loss: 9.35 gradNorm: 26.87 \n",
      "2025-03-13 21:56:30,191: Train batch 1008: loss: 8.64 gradNorm: 19.59 \n",
      "2025-03-13 21:56:30,293: Train batch 1009: loss: 12.32 gradNorm: 27.28 \n",
      "2025-03-13 21:56:30,646: Train batch 1010: loss: 18.88 gradNorm: 35.05 \n",
      "2025-03-13 21:56:30,787: Train batch 1011: loss: 12.12 gradNorm: 39.53 \n",
      "2025-03-13 21:56:30,876: Train batch 1012: loss: 12.63 gradNorm: 32.75 \n",
      "2025-03-13 21:56:31,045: Train batch 1013: loss: 14.33 gradNorm: 33.11 \n",
      "2025-03-13 21:56:31,125: Train batch 1014: loss: 10.21 gradNorm: 22.75 \n",
      "2025-03-13 21:56:31,327: Train batch 1015: loss: 14.45 gradNorm: 28.46 \n",
      "2025-03-13 21:56:31,623: Train batch 1016: loss: 18.38 gradNorm: 49.77 \n",
      "2025-03-13 21:56:31,718: Train batch 1017: loss: 14.11 gradNorm: 24.70 \n",
      "2025-03-13 21:56:31,793: Train batch 1018: loss: 13.66 gradNorm: 26.42 \n",
      "2025-03-13 21:56:31,966: Train batch 1019: loss: 11.28 gradNorm: 23.99 \n",
      "2025-03-13 21:56:32,074: Train batch 1020: loss: 14.06 gradNorm: 30.51 \n",
      "2025-03-13 21:56:32,351: Train batch 1021: loss: 25.06 gradNorm: 58.45 \n",
      "2025-03-13 21:56:32,435: Train batch 1022: loss: 10.24 gradNorm: 27.66 \n",
      "2025-03-13 21:56:32,632: Train batch 1023: loss: 12.80 gradNorm: 28.51 \n",
      "2025-03-13 21:56:32,985: Train batch 1024: loss: 21.37 gradNorm: 42.60 \n",
      "2025-03-13 21:56:33,152: Train batch 1025: loss: 13.64 gradNorm: 29.12 \n",
      "2025-03-13 21:56:33,309: Train batch 1026: loss: 9.83 gradNorm: 23.84 \n",
      "2025-03-13 21:56:33,388: Train batch 1027: loss: 13.37 gradNorm: 27.82 \n",
      "2025-03-13 21:56:33,474: Train batch 1028: loss: 11.57 gradNorm: 28.01 \n",
      "2025-03-13 21:56:33,660: Train batch 1029: loss: 13.86 gradNorm: 31.42 \n",
      "2025-03-13 21:56:34,039: Train batch 1030: loss: 12.82 gradNorm: 29.82 \n",
      "2025-03-13 21:56:34,207: Train batch 1031: loss: 10.99 gradNorm: 25.76 \n",
      "2025-03-13 21:56:34,374: Train batch 1032: loss: 9.17 gradNorm: 23.49 \n",
      "2025-03-13 21:56:34,447: Train batch 1033: loss: 11.49 gradNorm: 24.85 \n",
      "2025-03-13 21:56:34,620: Train batch 1034: loss: 7.94 gradNorm: 18.77 \n",
      "2025-03-13 21:56:34,722: Train batch 1035: loss: 11.68 gradNorm: 27.93 \n",
      "2025-03-13 21:56:34,920: Train batch 1036: loss: 12.09 gradNorm: 27.24 \n",
      "2025-03-13 21:56:35,072: Train batch 1037: loss: 8.91 gradNorm: 31.63 \n",
      "2025-03-13 21:56:35,219: Train batch 1038: loss: 8.67 gradNorm: 22.72 \n",
      "2025-03-13 21:56:35,388: Train batch 1039: loss: 8.57 gradNorm: 22.43 \n",
      "2025-03-13 21:56:35,662: Train batch 1040: loss: 21.89 gradNorm: 53.28 \n",
      "2025-03-13 21:56:35,751: Train batch 1041: loss: 13.19 gradNorm: 28.82 \n",
      "2025-03-13 21:56:35,826: Train batch 1042: loss: 13.57 gradNorm: 32.35 \n",
      "2025-03-13 21:56:36,008: Train batch 1043: loss: 14.08 gradNorm: 30.21 \n",
      "2025-03-13 21:56:36,116: Train batch 1044: loss: 9.07 gradNorm: 20.56 \n",
      "2025-03-13 21:56:36,352: Train batch 1045: loss: 19.20 gradNorm: 40.76 \n",
      "2025-03-13 21:56:36,441: Train batch 1046: loss: 11.04 gradNorm: 26.41 \n",
      "2025-03-13 21:56:36,532: Train batch 1047: loss: 11.49 gradNorm: 23.26 \n",
      "2025-03-13 21:56:36,724: Train batch 1048: loss: 10.13 gradNorm: 24.76 \n",
      "2025-03-13 21:56:36,803: Train batch 1049: loss: 11.61 gradNorm: 22.44 \n",
      "2025-03-13 21:56:36,985: Train batch 1050: loss: 11.67 gradNorm: 25.81 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:56:37,181: Val batch: CER (t18.2025.01.15): 0.133\n",
      "2025-03-13 21:56:37,182: Val batch 1050: CER (avg): 0.133 \n",
      "2025-03-13 21:56:37,182: Batches since validation CER improved: 100\n",
      "2025-03-13 21:56:37,537: Train batch 1051: loss: 17.07 gradNorm: 32.56 \n",
      "2025-03-13 21:56:37,650: Train batch 1052: loss: 9.26 gradNorm: 20.86 \n",
      "2025-03-13 21:56:37,780: Train batch 1053: loss: 12.69 gradNorm: 36.24 \n",
      "2025-03-13 21:56:38,092: Train batch 1054: loss: 12.69 gradNorm: 30.59 \n",
      "2025-03-13 21:56:38,173: Train batch 1055: loss: 13.64 gradNorm: 33.98 \n",
      "2025-03-13 21:56:38,272: Train batch 1056: loss: 15.52 gradNorm: 41.12 \n",
      "2025-03-13 21:56:38,454: Train batch 1057: loss: 12.11 gradNorm: 27.53 \n",
      "2025-03-13 21:56:38,521: Train batch 1058: loss: 11.26 gradNorm: 31.31 \n",
      "2025-03-13 21:56:38,606: Train batch 1059: loss: 12.81 gradNorm: 30.28 \n",
      "2025-03-13 21:56:38,693: Train batch 1060: loss: 12.18 gradNorm: 29.14 \n",
      "2025-03-13 21:56:38,865: Train batch 1061: loss: 12.04 gradNorm: 22.99 \n",
      "2025-03-13 21:56:39,033: Train batch 1062: loss: 9.31 gradNorm: 24.94 \n",
      "2025-03-13 21:56:39,164: Train batch 1063: loss: 12.16 gradNorm: 19.49 \n",
      "2025-03-13 21:56:39,239: Train batch 1064: loss: 11.13 gradNorm: 24.78 \n",
      "2025-03-13 21:56:39,342: Train batch 1065: loss: 13.55 gradNorm: 32.83 \n",
      "2025-03-13 21:56:39,498: Train batch 1066: loss: 9.37 gradNorm: 30.04 \n",
      "2025-03-13 21:56:39,578: Train batch 1067: loss: 9.38 gradNorm: 19.82 \n",
      "2025-03-13 21:56:39,850: Train batch 1068: loss: 21.25 gradNorm: 54.52 \n",
      "2025-03-13 21:56:39,940: Train batch 1069: loss: 11.65 gradNorm: 24.52 \n",
      "2025-03-13 21:56:40,149: Train batch 1070: loss: 20.92 gradNorm: 39.62 \n",
      "2025-03-13 21:56:40,505: Train batch 1071: loss: 18.28 gradNorm: 41.12 \n",
      "2025-03-13 21:56:40,576: Train batch 1072: loss: 10.16 gradNorm: 24.83 \n",
      "2025-03-13 21:56:40,745: Train batch 1073: loss: 11.89 gradNorm: 28.70 \n",
      "2025-03-13 21:56:40,893: Train batch 1074: loss: 9.50 gradNorm: 21.39 \n",
      "2025-03-13 21:56:40,976: Train batch 1075: loss: 10.23 gradNorm: 23.29 \n",
      "2025-03-13 21:56:41,061: Train batch 1076: loss: 9.61 gradNorm: 20.56 \n",
      "2025-03-13 21:56:41,174: Train batch 1077: loss: 10.73 gradNorm: 18.57 \n",
      "2025-03-13 21:56:41,462: Train batch 1078: loss: 17.69 gradNorm: 35.52 \n",
      "2025-03-13 21:56:41,591: Train batch 1079: loss: 9.04 gradNorm: 26.51 \n",
      "2025-03-13 21:56:41,663: Train batch 1080: loss: 14.13 gradNorm: 36.15 \n",
      "2025-03-13 21:56:42,021: Train batch 1081: loss: 16.69 gradNorm: 34.15 \n",
      "2025-03-13 21:56:42,267: Train batch 1082: loss: 16.62 gradNorm: 37.98 \n",
      "2025-03-13 21:56:42,660: Train batch 1083: loss: 16.15 gradNorm: 28.01 \n",
      "2025-03-13 21:56:42,864: Train batch 1084: loss: 12.23 gradNorm: 24.44 \n",
      "2025-03-13 21:56:42,986: Train batch 1085: loss: 9.66 gradNorm: 26.25 \n",
      "2025-03-13 21:56:43,065: Train batch 1086: loss: 12.01 gradNorm: 24.85 \n",
      "2025-03-13 21:56:43,178: Train batch 1087: loss: 9.75 gradNorm: 20.78 \n",
      "2025-03-13 21:56:43,250: Train batch 1088: loss: 10.70 gradNorm: 25.53 \n",
      "2025-03-13 21:56:43,345: Train batch 1089: loss: 16.33 gradNorm: 39.05 \n",
      "2025-03-13 21:56:43,678: Train batch 1090: loss: 23.23 gradNorm: 63.15 \n",
      "2025-03-13 21:56:43,751: Train batch 1091: loss: 12.93 gradNorm: 29.58 \n",
      "2025-03-13 21:56:43,851: Train batch 1092: loss: 13.02 gradNorm: 28.67 \n",
      "2025-03-13 21:56:43,934: Train batch 1093: loss: 14.50 gradNorm: 37.16 \n",
      "2025-03-13 21:56:44,206: Train batch 1094: loss: 17.70 gradNorm: 35.59 \n",
      "2025-03-13 21:56:44,310: Train batch 1095: loss: 11.81 gradNorm: 26.07 \n",
      "2025-03-13 21:56:44,394: Train batch 1096: loss: 12.02 gradNorm: 29.99 \n",
      "2025-03-13 21:56:44,478: Train batch 1097: loss: 10.32 gradNorm: 22.56 \n",
      "2025-03-13 21:56:44,603: Train batch 1098: loss: 10.90 gradNorm: 28.67 \n",
      "2025-03-13 21:56:44,708: Train batch 1099: loss: 12.12 gradNorm: 26.60 \n",
      "2025-03-13 21:56:44,788: Train batch 1100: loss: 11.33 gradNorm: 23.52 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:56:44,989: Val batch: CER (t18.2025.01.15): 0.119\n",
      "2025-03-13 21:56:44,990: Val batch 1100: CER (avg): 0.119 \n",
      "2025-03-13 21:56:44,990: Batches since validation CER improved: 150\n",
      "2025-03-13 21:56:45,096: Train batch 1101: loss: 11.97 gradNorm: 30.05 \n",
      "2025-03-13 21:56:45,164: Train batch 1102: loss: 9.87 gradNorm: 21.55 \n",
      "2025-03-13 21:56:45,251: Train batch 1103: loss: 9.67 gradNorm: 24.85 \n",
      "2025-03-13 21:56:45,413: Train batch 1104: loss: 9.05 gradNorm: 26.69 \n",
      "2025-03-13 21:56:45,551: Train batch 1105: loss: 8.42 gradNorm: 22.94 \n",
      "2025-03-13 21:56:45,819: Train batch 1106: loss: 17.39 gradNorm: 38.14 \n",
      "2025-03-13 21:56:45,898: Train batch 1107: loss: 10.01 gradNorm: 25.82 \n",
      "2025-03-13 21:56:46,025: Train batch 1108: loss: 10.38 gradNorm: 23.88 \n",
      "2025-03-13 21:56:46,121: Train batch 1109: loss: 9.29 gradNorm: 19.16 \n",
      "2025-03-13 21:56:46,335: Train batch 1110: loss: 11.83 gradNorm: 26.16 \n",
      "2025-03-13 21:56:46,439: Train batch 1111: loss: 10.66 gradNorm: 20.54 \n",
      "2025-03-13 21:56:46,550: Train batch 1112: loss: 11.07 gradNorm: 27.11 \n",
      "2025-03-13 21:56:46,803: Train batch 1113: loss: 18.18 gradNorm: 47.40 \n",
      "2025-03-13 21:56:46,958: Train batch 1114: loss: 9.25 gradNorm: 25.76 \n",
      "2025-03-13 21:56:47,113: Train batch 1115: loss: 7.93 gradNorm: 19.91 \n",
      "2025-03-13 21:56:47,486: Train batch 1116: loss: 21.41 gradNorm: 55.94 \n",
      "2025-03-13 21:56:47,646: Train batch 1117: loss: 7.49 gradNorm: 20.90 \n",
      "2025-03-13 21:56:47,808: Train batch 1118: loss: 7.69 gradNorm: 22.36 \n",
      "2025-03-13 21:56:47,972: Train batch 1119: loss: 7.90 gradNorm: 22.29 \n",
      "2025-03-13 21:56:48,059: Train batch 1120: loss: 10.01 gradNorm: 25.15 \n",
      "2025-03-13 21:56:48,429: Train batch 1121: loss: 16.02 gradNorm: 32.63 \n",
      "2025-03-13 21:56:48,536: Train batch 1122: loss: 10.20 gradNorm: 29.37 \n",
      "2025-03-13 21:56:48,853: Train batch 1123: loss: 20.48 gradNorm: 53.17 \n",
      "2025-03-13 21:56:49,081: Train batch 1124: loss: 12.91 gradNorm: 30.20 \n",
      "2025-03-13 21:56:49,352: Train batch 1125: loss: 15.97 gradNorm: 31.97 \n",
      "2025-03-13 21:56:49,426: Train batch 1126: loss: 14.89 gradNorm: 40.83 \n",
      "2025-03-13 21:56:49,673: Train batch 1127: loss: 15.32 gradNorm: 31.38 \n",
      "2025-03-13 21:56:49,754: Train batch 1128: loss: 12.52 gradNorm: 27.52 \n",
      "2025-03-13 21:56:49,901: Train batch 1129: loss: 8.35 gradNorm: 25.32 \n",
      "2025-03-13 21:56:50,057: Train batch 1130: loss: 6.99 gradNorm: 18.86 \n",
      "2025-03-13 21:56:50,212: Train batch 1131: loss: 5.82 gradNorm: 15.63 \n",
      "2025-03-13 21:56:50,292: Train batch 1132: loss: 13.98 gradNorm: 34.09 \n",
      "2025-03-13 21:56:50,372: Train batch 1133: loss: 17.66 gradNorm: 42.54 \n",
      "2025-03-13 21:56:50,540: Train batch 1134: loss: 13.65 gradNorm: 33.60 \n",
      "2025-03-13 21:56:50,838: Train batch 1135: loss: 21.87 gradNorm: 60.93 \n",
      "2025-03-13 21:56:50,996: Train batch 1136: loss: 8.89 gradNorm: 27.90 \n",
      "2025-03-13 21:56:51,298: Train batch 1137: loss: 20.00 gradNorm: 57.02 \n",
      "2025-03-13 21:56:51,482: Train batch 1138: loss: 12.09 gradNorm: 31.15 \n",
      "2025-03-13 21:56:51,586: Train batch 1139: loss: 13.98 gradNorm: 33.14 \n",
      "2025-03-13 21:56:51,682: Train batch 1140: loss: 12.95 gradNorm: 29.69 \n",
      "2025-03-13 21:56:51,850: Train batch 1141: loss: 7.81 gradNorm: 21.05 \n",
      "2025-03-13 21:56:52,209: Train batch 1142: loss: 14.64 gradNorm: 34.96 \n",
      "2025-03-13 21:56:52,288: Train batch 1143: loss: 11.62 gradNorm: 26.32 \n",
      "2025-03-13 21:56:52,374: Train batch 1144: loss: 10.89 gradNorm: 29.99 \n",
      "2025-03-13 21:56:52,542: Train batch 1145: loss: 11.88 gradNorm: 27.98 \n",
      "2025-03-13 21:56:52,820: Train batch 1146: loss: 14.31 gradNorm: 33.72 \n",
      "2025-03-13 21:56:53,084: Train batch 1147: loss: 13.47 gradNorm: 27.79 \n",
      "2025-03-13 21:56:53,376: Train batch 1148: loss: 17.60 gradNorm: 43.21 \n",
      "2025-03-13 21:56:53,619: Train batch 1149: loss: 15.46 gradNorm: 37.48 \n",
      "2025-03-13 21:56:53,932: Train batch 1150: loss: 13.81 gradNorm: 30.12 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:56:54,126: Val batch: CER (t18.2025.01.15): 0.112\n",
      "2025-03-13 21:56:54,128: Val batch 1150: CER (avg): 0.112 \n",
      "2025-03-13 21:56:54,128: Batches since validation CER improved: 200\n",
      "2025-03-13 21:56:54,281: Train batch 1151: loss: 7.35 gradNorm: 26.47 \n",
      "2025-03-13 21:56:54,407: Train batch 1152: loss: 11.32 gradNorm: 29.02 \n",
      "2025-03-13 21:56:54,478: Train batch 1153: loss: 9.58 gradNorm: 23.28 \n",
      "2025-03-13 21:56:54,588: Train batch 1154: loss: 14.27 gradNorm: 27.83 \n",
      "2025-03-13 21:56:54,701: Train batch 1155: loss: 11.12 gradNorm: 27.41 \n",
      "2025-03-13 21:56:55,074: Train batch 1156: loss: 19.71 gradNorm: 60.33 \n",
      "2025-03-13 21:56:55,158: Train batch 1157: loss: 9.85 gradNorm: 25.48 \n",
      "2025-03-13 21:56:55,264: Train batch 1158: loss: 10.79 gradNorm: 27.32 \n",
      "2025-03-13 21:56:55,390: Train batch 1159: loss: 8.85 gradNorm: 21.36 \n",
      "2025-03-13 21:56:55,478: Train batch 1160: loss: 10.58 gradNorm: 23.63 \n",
      "2025-03-13 21:56:55,891: Train batch 1161: loss: 14.35 gradNorm: 36.17 \n",
      "2025-03-13 21:56:55,978: Train batch 1162: loss: 12.19 gradNorm: 31.54 \n",
      "2025-03-13 21:56:56,248: Train batch 1163: loss: 15.39 gradNorm: 33.29 \n",
      "2025-03-13 21:56:56,433: Train batch 1164: loss: 11.16 gradNorm: 26.57 \n",
      "2025-03-13 21:56:56,704: Train batch 1165: loss: 18.03 gradNorm: 50.37 \n",
      "2025-03-13 21:56:56,774: Train batch 1166: loss: 11.76 gradNorm: 30.77 \n",
      "2025-03-13 21:56:56,962: Train batch 1167: loss: 8.75 gradNorm: 23.53 \n",
      "2025-03-13 21:56:57,328: Train batch 1168: loss: 14.86 gradNorm: 36.44 \n",
      "2025-03-13 21:56:57,461: Train batch 1169: loss: 8.03 gradNorm: 24.68 \n",
      "2025-03-13 21:56:57,540: Train batch 1170: loss: 8.97 gradNorm: 20.47 \n",
      "2025-03-13 21:56:57,733: Train batch 1171: loss: 12.33 gradNorm: 33.50 \n",
      "2025-03-13 21:56:57,967: Train batch 1172: loss: 13.36 gradNorm: 32.10 \n",
      "2025-03-13 21:56:58,317: Train batch 1173: loss: 14.75 gradNorm: 32.39 \n",
      "2025-03-13 21:56:58,465: Train batch 1174: loss: 7.70 gradNorm: 25.83 \n",
      "2025-03-13 21:56:58,619: Train batch 1175: loss: 6.34 gradNorm: 19.03 \n",
      "2025-03-13 21:56:58,762: Train batch 1176: loss: 10.99 gradNorm: 24.35 \n",
      "2025-03-13 21:56:58,872: Train batch 1177: loss: 12.18 gradNorm: 27.06 \n",
      "2025-03-13 21:56:58,968: Train batch 1178: loss: 12.82 gradNorm: 39.95 \n",
      "2025-03-13 21:56:59,143: Train batch 1179: loss: 6.90 gradNorm: 19.26 \n",
      "2025-03-13 21:56:59,336: Train batch 1180: loss: 10.97 gradNorm: 25.52 \n",
      "2025-03-13 21:56:59,406: Train batch 1181: loss: 10.26 gradNorm: 26.08 \n",
      "2025-03-13 21:56:59,515: Train batch 1182: loss: 9.23 gradNorm: 17.96 \n",
      "2025-03-13 21:56:59,585: Train batch 1183: loss: 10.28 gradNorm: 25.76 \n",
      "2025-03-13 21:56:59,732: Train batch 1184: loss: 6.54 gradNorm: 21.33 \n",
      "2025-03-13 21:56:59,957: Train batch 1185: loss: 16.93 gradNorm: 42.65 \n",
      "2025-03-13 21:57:00,221: Train batch 1186: loss: 11.32 gradNorm: 33.97 \n",
      "2025-03-13 21:57:00,331: Train batch 1187: loss: 10.64 gradNorm: 25.65 \n",
      "2025-03-13 21:57:00,614: Train batch 1188: loss: 20.02 gradNorm: 54.24 \n",
      "2025-03-13 21:57:00,994: Train batch 1189: loss: 14.29 gradNorm: 29.17 \n",
      "2025-03-13 21:57:01,073: Train batch 1190: loss: 10.92 gradNorm: 24.47 \n",
      "2025-03-13 21:57:01,425: Train batch 1191: loss: 14.57 gradNorm: 32.64 \n",
      "2025-03-13 21:57:01,596: Train batch 1192: loss: 11.95 gradNorm: 33.06 \n",
      "2025-03-13 21:57:01,757: Train batch 1193: loss: 7.13 gradNorm: 21.47 \n",
      "2025-03-13 21:57:02,035: Train batch 1194: loss: 15.45 gradNorm: 43.78 \n",
      "2025-03-13 21:57:02,121: Train batch 1195: loss: 10.80 gradNorm: 27.01 \n",
      "2025-03-13 21:57:02,190: Train batch 1196: loss: 8.77 gradNorm: 23.68 \n",
      "2025-03-13 21:57:02,268: Train batch 1197: loss: 10.61 gradNorm: 26.29 \n",
      "2025-03-13 21:57:02,466: Train batch 1198: loss: 9.03 gradNorm: 24.09 \n",
      "2025-03-13 21:57:02,743: Train batch 1199: loss: 15.06 gradNorm: 35.68 \n",
      "2025-03-13 21:57:02,920: Train batch 1200: loss: 8.57 gradNorm: 21.64 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:57:03,123: Val batch: CER (t18.2025.01.15): 0.107\n",
      "2025-03-13 21:57:03,125: Val batch 1200: CER (avg): 0.107 \n",
      "2025-03-13 21:57:03,351: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:57:03,354: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:57:03,355: Batches since validation CER improved: 0\n",
      "2025-03-13 21:57:03,458: Train batch 1201: loss: 9.80 gradNorm: 29.07 \n",
      "2025-03-13 21:57:03,723: Train batch 1202: loss: 12.07 gradNorm: 34.95 \n",
      "2025-03-13 21:57:03,869: Train batch 1203: loss: 11.53 gradNorm: 29.79 \n",
      "2025-03-13 21:57:04,075: Train batch 1204: loss: 10.26 gradNorm: 26.72 \n",
      "2025-03-13 21:57:04,471: Train batch 1205: loss: 13.51 gradNorm: 29.38 \n",
      "2025-03-13 21:57:04,721: Train batch 1206: loss: 13.59 gradNorm: 31.96 \n",
      "2025-03-13 21:57:04,995: Train batch 1207: loss: 13.54 gradNorm: 31.50 \n",
      "2025-03-13 21:57:05,096: Train batch 1208: loss: 8.29 gradNorm: 26.86 \n",
      "2025-03-13 21:57:05,198: Train batch 1209: loss: 11.42 gradNorm: 26.51 \n",
      "2025-03-13 21:57:05,473: Train batch 1210: loss: 12.78 gradNorm: 36.51 \n",
      "2025-03-13 21:57:05,564: Train batch 1211: loss: 11.13 gradNorm: 33.27 \n",
      "2025-03-13 21:57:05,642: Train batch 1212: loss: 9.78 gradNorm: 23.51 \n",
      "2025-03-13 21:57:05,735: Train batch 1213: loss: 12.57 gradNorm: 29.85 \n",
      "2025-03-13 21:57:06,109: Train batch 1214: loss: 14.91 gradNorm: 40.72 \n",
      "2025-03-13 21:57:06,459: Train batch 1215: loss: 12.40 gradNorm: 34.15 \n",
      "2025-03-13 21:57:06,529: Train batch 1216: loss: 12.46 gradNorm: 34.02 \n",
      "2025-03-13 21:57:06,697: Train batch 1217: loss: 8.98 gradNorm: 26.51 \n",
      "2025-03-13 21:57:06,778: Train batch 1218: loss: 10.09 gradNorm: 24.59 \n",
      "2025-03-13 21:57:06,889: Train batch 1219: loss: 9.53 gradNorm: 24.54 \n",
      "2025-03-13 21:57:07,001: Train batch 1220: loss: 10.39 gradNorm: 20.77 \n",
      "2025-03-13 21:57:07,168: Train batch 1221: loss: 12.38 gradNorm: 35.48 \n",
      "2025-03-13 21:57:07,262: Train batch 1222: loss: 11.77 gradNorm: 30.51 \n",
      "2025-03-13 21:57:07,327: Train batch 1223: loss: 11.75 gradNorm: 39.68 \n",
      "2025-03-13 21:57:07,634: Train batch 1224: loss: 26.58 gradNorm: 89.88 \n",
      "2025-03-13 21:57:07,720: Train batch 1225: loss: 8.42 gradNorm: 22.68 \n",
      "2025-03-13 21:57:08,034: Train batch 1226: loss: 26.52 gradNorm: 85.40 \n",
      "2025-03-13 21:57:08,185: Train batch 1227: loss: 10.88 gradNorm: 27.87 \n",
      "2025-03-13 21:57:08,293: Train batch 1228: loss: 7.27 gradNorm: 21.83 \n",
      "2025-03-13 21:57:08,449: Train batch 1229: loss: 6.65 gradNorm: 19.77 \n",
      "2025-03-13 21:57:08,530: Train batch 1230: loss: 12.07 gradNorm: 32.29 \n",
      "2025-03-13 21:57:08,702: Train batch 1231: loss: 10.24 gradNorm: 26.62 \n",
      "2025-03-13 21:57:08,901: Train batch 1232: loss: 10.04 gradNorm: 32.83 \n",
      "2025-03-13 21:57:09,288: Train batch 1233: loss: 15.73 gradNorm: 31.91 \n",
      "2025-03-13 21:57:09,553: Train batch 1234: loss: 16.40 gradNorm: 49.58 \n",
      "2025-03-13 21:57:09,622: Train batch 1235: loss: 9.62 gradNorm: 24.78 \n",
      "2025-03-13 21:57:09,819: Train batch 1236: loss: 12.48 gradNorm: 33.21 \n",
      "2025-03-13 21:57:10,064: Train batch 1237: loss: 11.89 gradNorm: 30.06 \n",
      "2025-03-13 21:57:10,146: Train batch 1238: loss: 10.00 gradNorm: 27.94 \n",
      "2025-03-13 21:57:10,456: Train batch 1239: loss: 13.23 gradNorm: 34.84 \n",
      "2025-03-13 21:57:10,612: Train batch 1240: loss: 6.96 gradNorm: 21.58 \n",
      "2025-03-13 21:57:10,695: Train batch 1241: loss: 9.06 gradNorm: 21.80 \n",
      "2025-03-13 21:57:10,842: Train batch 1242: loss: 6.39 gradNorm: 17.73 \n",
      "2025-03-13 21:57:11,118: Train batch 1243: loss: 19.90 gradNorm: 58.59 \n",
      "2025-03-13 21:57:11,253: Train batch 1244: loss: 11.13 gradNorm: 32.36 \n",
      "2025-03-13 21:57:11,365: Train batch 1245: loss: 11.85 gradNorm: 26.78 \n",
      "2025-03-13 21:57:11,453: Train batch 1246: loss: 10.59 gradNorm: 23.21 \n",
      "2025-03-13 21:57:11,582: Train batch 1247: loss: 9.23 gradNorm: 22.28 \n",
      "2025-03-13 21:57:11,733: Train batch 1248: loss: 7.26 gradNorm: 26.58 \n",
      "2025-03-13 21:57:11,801: Train batch 1249: loss: 9.90 gradNorm: 25.12 \n",
      "2025-03-13 21:57:12,110: Train batch 1250: loss: 19.46 gradNorm: 54.65 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:57:12,306: Val batch: CER (t18.2025.01.15): 0.105\n",
      "2025-03-13 21:57:12,307: Val batch 1250: CER (avg): 0.105 \n",
      "2025-03-13 21:57:12,528: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:57:12,532: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:57:12,532: Batches since validation CER improved: 0\n",
      "2025-03-13 21:57:12,725: Train batch 1251: loss: 10.16 gradNorm: 28.44 \n",
      "2025-03-13 21:57:12,899: Train batch 1252: loss: 8.44 gradNorm: 21.53 \n",
      "2025-03-13 21:57:12,999: Train batch 1253: loss: 11.06 gradNorm: 21.25 \n",
      "2025-03-13 21:57:13,115: Train batch 1254: loss: 6.09 gradNorm: 19.19 \n",
      "2025-03-13 21:57:13,218: Train batch 1255: loss: 9.36 gradNorm: 23.76 \n",
      "2025-03-13 21:57:13,299: Train batch 1256: loss: 11.28 gradNorm: 23.46 \n",
      "2025-03-13 21:57:13,388: Train batch 1257: loss: 14.04 gradNorm: 34.65 \n",
      "2025-03-13 21:57:13,468: Train batch 1258: loss: 10.38 gradNorm: 31.12 \n",
      "2025-03-13 21:57:13,553: Train batch 1259: loss: 9.21 gradNorm: 24.89 \n",
      "2025-03-13 21:57:13,825: Train batch 1260: loss: 13.98 gradNorm: 40.01 \n",
      "2025-03-13 21:57:13,895: Train batch 1261: loss: 10.80 gradNorm: 27.42 \n",
      "2025-03-13 21:57:14,068: Train batch 1262: loss: 12.41 gradNorm: 34.25 \n",
      "2025-03-13 21:57:14,415: Train batch 1263: loss: 10.90 gradNorm: 28.86 \n",
      "2025-03-13 21:57:14,494: Train batch 1264: loss: 11.50 gradNorm: 30.67 \n",
      "2025-03-13 21:57:14,881: Train batch 1265: loss: 11.08 gradNorm: 28.37 \n",
      "2025-03-13 21:57:15,048: Train batch 1266: loss: 9.97 gradNorm: 25.18 \n",
      "2025-03-13 21:57:15,177: Train batch 1267: loss: 9.46 gradNorm: 22.57 \n",
      "2025-03-13 21:57:15,316: Train batch 1268: loss: 9.64 gradNorm: 24.11 \n",
      "2025-03-13 21:57:15,413: Train batch 1269: loss: 9.05 gradNorm: 24.63 \n",
      "2025-03-13 21:57:15,565: Train batch 1270: loss: 8.05 gradNorm: 25.33 \n",
      "2025-03-13 21:57:15,759: Train batch 1271: loss: 9.96 gradNorm: 29.91 \n",
      "2025-03-13 21:57:15,956: Train batch 1272: loss: 13.08 gradNorm: 34.53 \n",
      "2025-03-13 21:57:16,162: Train batch 1273: loss: 9.08 gradNorm: 23.80 \n",
      "2025-03-13 21:57:16,480: Train batch 1274: loss: 15.77 gradNorm: 38.52 \n",
      "2025-03-13 21:57:16,834: Train batch 1275: loss: 9.15 gradNorm: 23.25 \n",
      "2025-03-13 21:57:16,905: Train batch 1276: loss: 9.24 gradNorm: 23.28 \n",
      "2025-03-13 21:57:16,979: Train batch 1277: loss: 10.37 gradNorm: 27.34 \n",
      "2025-03-13 21:57:17,268: Train batch 1278: loss: 11.98 gradNorm: 30.45 \n",
      "2025-03-13 21:57:17,637: Train batch 1279: loss: 13.89 gradNorm: 30.71 \n",
      "2025-03-13 21:57:17,718: Train batch 1280: loss: 8.70 gradNorm: 20.17 \n",
      "2025-03-13 21:57:17,817: Train batch 1281: loss: 20.21 gradNorm: 46.45 \n",
      "2025-03-13 21:57:17,945: Train batch 1282: loss: 5.76 gradNorm: 17.80 \n",
      "2025-03-13 21:57:18,138: Train batch 1283: loss: 8.15 gradNorm: 22.42 \n",
      "2025-03-13 21:57:18,288: Train batch 1284: loss: 5.08 gradNorm: 18.94 \n",
      "2025-03-13 21:57:18,403: Train batch 1285: loss: 11.73 gradNorm: 30.39 \n",
      "2025-03-13 21:57:18,696: Train batch 1286: loss: 10.76 gradNorm: 35.21 \n",
      "2025-03-13 21:57:18,768: Train batch 1287: loss: 9.18 gradNorm: 25.48 \n",
      "2025-03-13 21:57:18,873: Train batch 1288: loss: 6.79 gradNorm: 15.73 \n",
      "2025-03-13 21:57:18,953: Train batch 1289: loss: 8.31 gradNorm: 22.18 \n",
      "2025-03-13 21:57:19,228: Train batch 1290: loss: 13.14 gradNorm: 37.38 \n",
      "2025-03-13 21:57:19,505: Train batch 1291: loss: 12.19 gradNorm: 27.93 \n",
      "2025-03-13 21:57:19,609: Train batch 1292: loss: 10.25 gradNorm: 30.56 \n",
      "2025-03-13 21:57:19,692: Train batch 1293: loss: 9.47 gradNorm: 26.90 \n",
      "2025-03-13 21:57:19,821: Train batch 1294: loss: 9.40 gradNorm: 30.30 \n",
      "2025-03-13 21:57:19,895: Train batch 1295: loss: 9.03 gradNorm: 23.74 \n",
      "2025-03-13 21:57:20,194: Train batch 1296: loss: 15.87 gradNorm: 50.40 \n",
      "2025-03-13 21:57:20,274: Train batch 1297: loss: 8.68 gradNorm: 22.78 \n",
      "2025-03-13 21:57:20,352: Train batch 1298: loss: 7.42 gradNorm: 23.06 \n",
      "2025-03-13 21:57:20,451: Train batch 1299: loss: 10.20 gradNorm: 23.37 \n",
      "2025-03-13 21:57:20,567: Train batch 1300: loss: 8.87 gradNorm: 21.66 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:57:20,757: Val batch: CER (t18.2025.01.15): 0.095\n",
      "2025-03-13 21:57:20,759: Val batch 1300: CER (avg): 0.095 \n",
      "2025-03-13 21:57:20,985: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:57:20,989: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:57:20,989: Batches since validation CER improved: 0\n",
      "2025-03-13 21:57:21,069: Train batch 1301: loss: 8.71 gradNorm: 24.13 \n",
      "2025-03-13 21:57:21,218: Train batch 1302: loss: 5.69 gradNorm: 17.98 \n",
      "2025-03-13 21:57:21,292: Train batch 1303: loss: 9.27 gradNorm: 25.64 \n",
      "2025-03-13 21:57:21,441: Train batch 1304: loss: 9.88 gradNorm: 29.63 \n",
      "2025-03-13 21:57:21,717: Train batch 1305: loss: 13.61 gradNorm: 39.70 \n",
      "2025-03-13 21:57:21,815: Train batch 1306: loss: 12.33 gradNorm: 28.55 \n",
      "2025-03-13 21:57:22,041: Train batch 1307: loss: 11.75 gradNorm: 39.21 \n",
      "2025-03-13 21:57:22,293: Train batch 1308: loss: 10.79 gradNorm: 29.64 \n",
      "2025-03-13 21:57:22,644: Train batch 1309: loss: 12.11 gradNorm: 28.36 \n",
      "2025-03-13 21:57:22,910: Train batch 1310: loss: 11.70 gradNorm: 27.74 \n",
      "2025-03-13 21:57:23,013: Train batch 1311: loss: 6.89 gradNorm: 22.05 \n",
      "2025-03-13 21:57:23,094: Train batch 1312: loss: 9.16 gradNorm: 20.56 \n",
      "2025-03-13 21:57:23,192: Train batch 1313: loss: 10.15 gradNorm: 19.71 \n",
      "2025-03-13 21:57:23,292: Train batch 1314: loss: 9.97 gradNorm: 21.11 \n",
      "2025-03-13 21:57:23,463: Train batch 1315: loss: 5.28 gradNorm: 16.74 \n",
      "2025-03-13 21:57:23,644: Train batch 1316: loss: 8.73 gradNorm: 28.35 \n",
      "2025-03-13 21:57:23,770: Train batch 1317: loss: 5.08 gradNorm: 16.20 \n",
      "2025-03-13 21:57:23,846: Train batch 1318: loss: 12.35 gradNorm: 35.24 \n",
      "2025-03-13 21:57:23,932: Train batch 1319: loss: 9.11 gradNorm: 26.24 \n",
      "2025-03-13 21:57:24,018: Train batch 1320: loss: 7.56 gradNorm: 20.33 \n",
      "2025-03-13 21:57:24,105: Train batch 1321: loss: 9.99 gradNorm: 25.79 \n",
      "2025-03-13 21:57:24,194: Train batch 1322: loss: 7.75 gradNorm: 19.85 \n",
      "2025-03-13 21:57:24,570: Train batch 1323: loss: 19.22 gradNorm: 71.47 \n",
      "2025-03-13 21:57:24,630: Train batch 1324: loss: 8.06 gradNorm: 24.23 \n",
      "2025-03-13 21:57:24,717: Train batch 1325: loss: 8.20 gradNorm: 24.85 \n",
      "2025-03-13 21:57:24,814: Train batch 1326: loss: 8.21 gradNorm: 20.36 \n",
      "2025-03-13 21:57:24,909: Train batch 1327: loss: 9.58 gradNorm: 22.06 \n",
      "2025-03-13 21:57:25,079: Train batch 1328: loss: 10.64 gradNorm: 29.54 \n",
      "2025-03-13 21:57:25,162: Train batch 1329: loss: 8.22 gradNorm: 26.45 \n",
      "2025-03-13 21:57:25,251: Train batch 1330: loss: 10.33 gradNorm: 28.78 \n",
      "2025-03-13 21:57:25,414: Train batch 1331: loss: 7.84 gradNorm: 22.59 \n",
      "2025-03-13 21:57:25,586: Train batch 1332: loss: 7.71 gradNorm: 26.68 \n",
      "2025-03-13 21:57:25,759: Train batch 1333: loss: 5.21 gradNorm: 17.70 \n",
      "2025-03-13 21:57:25,907: Train batch 1334: loss: 4.49 gradNorm: 16.45 \n",
      "2025-03-13 21:57:26,020: Train batch 1335: loss: 10.84 gradNorm: 25.52 \n",
      "2025-03-13 21:57:26,130: Train batch 1336: loss: 11.07 gradNorm: 25.96 \n",
      "2025-03-13 21:57:26,475: Train batch 1337: loss: 13.52 gradNorm: 33.58 \n",
      "2025-03-13 21:57:26,543: Train batch 1338: loss: 8.10 gradNorm: 21.01 \n",
      "2025-03-13 21:57:26,620: Train batch 1339: loss: 7.16 gradNorm: 19.96 \n",
      "2025-03-13 21:57:26,815: Train batch 1340: loss: 8.41 gradNorm: 24.33 \n",
      "2025-03-13 21:57:26,965: Train batch 1341: loss: 6.75 gradNorm: 26.89 \n",
      "2025-03-13 21:57:27,125: Train batch 1342: loss: 7.83 gradNorm: 26.01 \n",
      "2025-03-13 21:57:27,241: Train batch 1343: loss: 8.48 gradNorm: 24.44 \n",
      "2025-03-13 21:57:27,322: Train batch 1344: loss: 9.84 gradNorm: 31.90 \n",
      "2025-03-13 21:57:27,410: Train batch 1345: loss: 8.05 gradNorm: 20.61 \n",
      "2025-03-13 21:57:27,700: Train batch 1346: loss: 11.21 gradNorm: 38.94 \n",
      "2025-03-13 21:57:27,835: Train batch 1347: loss: 10.96 gradNorm: 27.23 \n",
      "2025-03-13 21:57:28,130: Train batch 1348: loss: 10.25 gradNorm: 25.26 \n",
      "2025-03-13 21:57:28,257: Train batch 1349: loss: 4.53 gradNorm: 16.66 \n",
      "2025-03-13 21:57:28,339: Train batch 1350: loss: 7.28 gradNorm: 19.73 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:57:28,533: Val batch: CER (t18.2025.01.15): 0.090\n",
      "2025-03-13 21:57:28,535: Val batch 1350: CER (avg): 0.090 \n",
      "2025-03-13 21:57:28,754: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:57:28,757: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:57:28,757: Batches since validation CER improved: 0\n",
      "2025-03-13 21:57:29,114: Train batch 1351: loss: 10.46 gradNorm: 27.62 \n",
      "2025-03-13 21:57:29,396: Train batch 1352: loss: 16.36 gradNorm: 56.68 \n",
      "2025-03-13 21:57:29,668: Train batch 1353: loss: 14.24 gradNorm: 38.57 \n",
      "2025-03-13 21:57:29,738: Train batch 1354: loss: 8.66 gradNorm: 22.73 \n",
      "2025-03-13 21:57:29,951: Train batch 1355: loss: 10.44 gradNorm: 30.80 \n",
      "2025-03-13 21:57:30,121: Train batch 1356: loss: 7.88 gradNorm: 22.95 \n",
      "2025-03-13 21:57:30,290: Train batch 1357: loss: 5.70 gradNorm: 19.70 \n",
      "2025-03-13 21:57:30,356: Train batch 1358: loss: 11.21 gradNorm: 29.05 \n",
      "2025-03-13 21:57:30,649: Train batch 1359: loss: 18.01 gradNorm: 61.86 \n",
      "2025-03-13 21:57:30,806: Train batch 1360: loss: 6.16 gradNorm: 21.84 \n",
      "2025-03-13 21:57:30,937: Train batch 1361: loss: 10.64 gradNorm: 25.56 \n",
      "2025-03-13 21:57:31,214: Train batch 1362: loss: 10.45 gradNorm: 31.44 \n",
      "2025-03-13 21:57:31,295: Train batch 1363: loss: 6.92 gradNorm: 20.12 \n",
      "2025-03-13 21:57:31,393: Train batch 1364: loss: 10.63 gradNorm: 27.90 \n",
      "2025-03-13 21:57:31,661: Train batch 1365: loss: 13.79 gradNorm: 37.79 \n",
      "2025-03-13 21:57:31,749: Train batch 1366: loss: 8.39 gradNorm: 20.72 \n",
      "2025-03-13 21:57:31,867: Train batch 1367: loss: 7.14 gradNorm: 19.71 \n",
      "2025-03-13 21:57:31,965: Train batch 1368: loss: 6.59 gradNorm: 17.95 \n",
      "2025-03-13 21:57:32,213: Train batch 1369: loss: 9.84 gradNorm: 27.56 \n",
      "2025-03-13 21:57:32,483: Train batch 1370: loss: 10.18 gradNorm: 26.68 \n",
      "2025-03-13 21:57:32,556: Train batch 1371: loss: 8.25 gradNorm: 23.98 \n",
      "2025-03-13 21:57:32,759: Train batch 1372: loss: 12.53 gradNorm: 38.96 \n",
      "2025-03-13 21:57:32,866: Train batch 1373: loss: 6.57 gradNorm: 19.42 \n",
      "2025-03-13 21:57:32,987: Train batch 1374: loss: 7.98 gradNorm: 20.20 \n",
      "2025-03-13 21:57:33,068: Train batch 1375: loss: 8.04 gradNorm: 22.76 \n",
      "2025-03-13 21:57:33,136: Train batch 1376: loss: 7.34 gradNorm: 19.55 \n",
      "2025-03-13 21:57:33,211: Train batch 1377: loss: 6.11 gradNorm: 17.62 \n",
      "2025-03-13 21:57:33,598: Train batch 1378: loss: 15.79 gradNorm: 41.23 \n",
      "2025-03-13 21:57:33,726: Train batch 1379: loss: 7.77 gradNorm: 21.40 \n",
      "2025-03-13 21:57:33,825: Train batch 1380: loss: 10.57 gradNorm: 24.82 \n",
      "2025-03-13 21:57:34,030: Train batch 1381: loss: 8.89 gradNorm: 25.61 \n",
      "2025-03-13 21:57:34,176: Train batch 1382: loss: 7.70 gradNorm: 21.27 \n",
      "2025-03-13 21:57:34,279: Train batch 1383: loss: 6.63 gradNorm: 20.92 \n",
      "2025-03-13 21:57:34,627: Train batch 1384: loss: 11.42 gradNorm: 32.70 \n",
      "2025-03-13 21:57:34,742: Train batch 1385: loss: 5.53 gradNorm: 16.94 \n",
      "2025-03-13 21:57:34,833: Train batch 1386: loss: 9.19 gradNorm: 27.49 \n",
      "2025-03-13 21:57:35,228: Train batch 1387: loss: 8.88 gradNorm: 25.47 \n",
      "2025-03-13 21:57:35,505: Train batch 1388: loss: 9.39 gradNorm: 23.75 \n",
      "2025-03-13 21:57:35,905: Train batch 1389: loss: 9.94 gradNorm: 24.82 \n",
      "2025-03-13 21:57:35,990: Train batch 1390: loss: 10.81 gradNorm: 24.76 \n",
      "2025-03-13 21:57:36,135: Train batch 1391: loss: 9.17 gradNorm: 24.84 \n",
      "2025-03-13 21:57:36,242: Train batch 1392: loss: 6.92 gradNorm: 24.72 \n",
      "2025-03-13 21:57:36,404: Train batch 1393: loss: 8.14 gradNorm: 28.56 \n",
      "2025-03-13 21:57:36,711: Train batch 1394: loss: 15.62 gradNorm: 50.64 \n",
      "2025-03-13 21:57:36,813: Train batch 1395: loss: 9.79 gradNorm: 22.78 \n",
      "2025-03-13 21:57:37,077: Train batch 1396: loss: 11.40 gradNorm: 30.39 \n",
      "2025-03-13 21:57:37,227: Train batch 1397: loss: 5.21 gradNorm: 17.72 \n",
      "2025-03-13 21:57:37,307: Train batch 1398: loss: 10.33 gradNorm: 32.00 \n",
      "2025-03-13 21:57:37,398: Train batch 1399: loss: 9.95 gradNorm: 34.49 \n",
      "2025-03-13 21:57:37,480: Train batch 1400: loss: 6.69 gradNorm: 23.16 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:57:37,704: Val batch: CER (t18.2025.01.15): 0.100\n",
      "2025-03-13 21:57:37,705: Val batch 1400: CER (avg): 0.100 \n",
      "2025-03-13 21:57:37,705: Batches since validation CER improved: 50\n",
      "2025-03-13 21:57:37,844: Train batch 1401: loss: 8.89 gradNorm: 24.40 \n",
      "2025-03-13 21:57:37,926: Train batch 1402: loss: 8.19 gradNorm: 24.54 \n",
      "2025-03-13 21:57:38,020: Train batch 1403: loss: 6.88 gradNorm: 19.29 \n",
      "2025-03-13 21:57:38,121: Train batch 1404: loss: 9.52 gradNorm: 22.69 \n",
      "2025-03-13 21:57:38,214: Train batch 1405: loss: 7.26 gradNorm: 18.06 \n",
      "2025-03-13 21:57:38,308: Train batch 1406: loss: 7.29 gradNorm: 19.31 \n",
      "2025-03-13 21:57:38,482: Train batch 1407: loss: 10.09 gradNorm: 26.27 \n",
      "2025-03-13 21:57:38,589: Train batch 1408: loss: 10.57 gradNorm: 23.81 \n",
      "2025-03-13 21:57:38,681: Train batch 1409: loss: 10.81 gradNorm: 33.54 \n",
      "2025-03-13 21:57:38,885: Train batch 1410: loss: 19.05 gradNorm: 62.09 \n",
      "2025-03-13 21:57:38,994: Train batch 1411: loss: 8.32 gradNorm: 23.10 \n",
      "2025-03-13 21:57:39,068: Train batch 1412: loss: 8.14 gradNorm: 28.36 \n",
      "2025-03-13 21:57:39,337: Train batch 1413: loss: 18.41 gradNorm: 66.00 \n",
      "2025-03-13 21:57:39,734: Train batch 1414: loss: 12.81 gradNorm: 34.24 \n",
      "2025-03-13 21:57:40,042: Train batch 1415: loss: 13.57 gradNorm: 40.26 \n",
      "2025-03-13 21:57:40,116: Train batch 1416: loss: 8.76 gradNorm: 23.97 \n",
      "2025-03-13 21:57:40,304: Train batch 1417: loss: 5.40 gradNorm: 19.72 \n",
      "2025-03-13 21:57:40,405: Train batch 1418: loss: 8.24 gradNorm: 21.53 \n",
      "2025-03-13 21:57:40,569: Train batch 1419: loss: 8.46 gradNorm: 23.76 \n",
      "2025-03-13 21:57:40,848: Train batch 1420: loss: 12.99 gradNorm: 37.82 \n",
      "2025-03-13 21:57:40,931: Train batch 1421: loss: 8.73 gradNorm: 26.19 \n",
      "2025-03-13 21:57:41,105: Train batch 1422: loss: 6.32 gradNorm: 20.47 \n",
      "2025-03-13 21:57:41,323: Train batch 1423: loss: 8.40 gradNorm: 26.54 \n",
      "2025-03-13 21:57:41,435: Train batch 1424: loss: 8.89 gradNorm: 22.65 \n",
      "2025-03-13 21:57:41,711: Train batch 1425: loss: 9.80 gradNorm: 29.01 \n",
      "2025-03-13 21:57:41,819: Train batch 1426: loss: 8.04 gradNorm: 24.78 \n",
      "2025-03-13 21:57:41,953: Train batch 1427: loss: 5.12 gradNorm: 18.54 \n",
      "2025-03-13 21:57:42,228: Train batch 1428: loss: 10.83 gradNorm: 33.33 \n",
      "2025-03-13 21:57:42,312: Train batch 1429: loss: 9.34 gradNorm: 24.76 \n",
      "2025-03-13 21:57:42,494: Train batch 1430: loss: 9.54 gradNorm: 31.89 \n",
      "2025-03-13 21:57:42,563: Train batch 1431: loss: 6.51 gradNorm: 19.92 \n",
      "2025-03-13 21:57:42,787: Train batch 1432: loss: 7.15 gradNorm: 22.82 \n",
      "2025-03-13 21:57:42,957: Train batch 1433: loss: 7.02 gradNorm: 20.47 \n",
      "2025-03-13 21:57:43,047: Train batch 1434: loss: 5.82 gradNorm: 16.29 \n",
      "2025-03-13 21:57:43,130: Train batch 1435: loss: 8.77 gradNorm: 25.47 \n",
      "2025-03-13 21:57:43,310: Train batch 1436: loss: 5.95 gradNorm: 22.47 \n",
      "2025-03-13 21:57:43,398: Train batch 1437: loss: 7.84 gradNorm: 22.98 \n",
      "2025-03-13 21:57:43,510: Train batch 1438: loss: 7.49 gradNorm: 24.98 \n",
      "2025-03-13 21:57:43,697: Train batch 1439: loss: 9.68 gradNorm: 33.56 \n",
      "2025-03-13 21:57:43,980: Train batch 1440: loss: 10.90 gradNorm: 38.10 \n",
      "2025-03-13 21:57:44,083: Train batch 1441: loss: 7.65 gradNorm: 27.22 \n",
      "2025-03-13 21:57:44,433: Train batch 1442: loss: 14.91 gradNorm: 41.62 \n",
      "2025-03-13 21:57:44,517: Train batch 1443: loss: 12.76 gradNorm: 39.75 \n",
      "2025-03-13 21:57:44,592: Train batch 1444: loss: 10.25 gradNorm: 31.33 \n",
      "2025-03-13 21:57:44,720: Train batch 1445: loss: 10.31 gradNorm: 26.48 \n",
      "2025-03-13 21:57:44,811: Train batch 1446: loss: 6.54 gradNorm: 21.87 \n",
      "2025-03-13 21:57:44,919: Train batch 1447: loss: 10.39 gradNorm: 31.75 \n",
      "2025-03-13 21:57:45,090: Train batch 1448: loss: 8.94 gradNorm: 35.49 \n",
      "2025-03-13 21:57:45,223: Train batch 1449: loss: 4.00 gradNorm: 15.76 \n",
      "2025-03-13 21:57:45,425: Train batch 1450: loss: 8.00 gradNorm: 26.95 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:57:45,653: Val batch: CER (t18.2025.01.15): 0.097\n",
      "2025-03-13 21:57:45,654: Val batch 1450: CER (avg): 0.097 \n",
      "2025-03-13 21:57:45,654: Batches since validation CER improved: 100\n",
      "2025-03-13 21:57:45,956: Train batch 1451: loss: 11.24 gradNorm: 33.98 \n",
      "2025-03-13 21:57:46,046: Train batch 1452: loss: 12.04 gradNorm: 35.83 \n",
      "2025-03-13 21:57:46,208: Train batch 1453: loss: 7.63 gradNorm: 23.06 \n",
      "2025-03-13 21:57:46,285: Train batch 1454: loss: 9.20 gradNorm: 27.19 \n",
      "2025-03-13 21:57:46,401: Train batch 1455: loss: 9.85 gradNorm: 25.04 \n",
      "2025-03-13 21:57:46,763: Train batch 1456: loss: 11.07 gradNorm: 30.39 \n",
      "2025-03-13 21:57:46,851: Train batch 1457: loss: 7.32 gradNorm: 20.29 \n",
      "2025-03-13 21:57:46,985: Train batch 1458: loss: 8.54 gradNorm: 23.92 \n",
      "2025-03-13 21:57:47,285: Train batch 1459: loss: 10.41 gradNorm: 30.61 \n",
      "2025-03-13 21:57:47,444: Train batch 1460: loss: 5.97 gradNorm: 22.80 \n",
      "2025-03-13 21:57:47,810: Train batch 1461: loss: 8.66 gradNorm: 23.75 \n",
      "2025-03-13 21:57:47,899: Train batch 1462: loss: 8.11 gradNorm: 25.28 \n",
      "2025-03-13 21:57:48,285: Train batch 1463: loss: 8.06 gradNorm: 27.78 \n",
      "2025-03-13 21:57:48,382: Train batch 1464: loss: 7.31 gradNorm: 24.96 \n",
      "2025-03-13 21:57:48,666: Train batch 1465: loss: 11.61 gradNorm: 36.49 \n",
      "2025-03-13 21:57:48,866: Train batch 1466: loss: 8.10 gradNorm: 26.55 \n",
      "2025-03-13 21:57:49,039: Train batch 1467: loss: 7.60 gradNorm: 25.12 \n",
      "2025-03-13 21:57:49,143: Train batch 1468: loss: 9.15 gradNorm: 24.97 \n",
      "2025-03-13 21:57:49,242: Train batch 1469: loss: 8.17 gradNorm: 23.25 \n",
      "2025-03-13 21:57:49,324: Train batch 1470: loss: 7.79 gradNorm: 21.98 \n",
      "2025-03-13 21:57:49,522: Train batch 1471: loss: 10.72 gradNorm: 32.93 \n",
      "2025-03-13 21:57:49,648: Train batch 1472: loss: 6.26 gradNorm: 18.94 \n",
      "2025-03-13 21:57:49,814: Train batch 1473: loss: 4.72 gradNorm: 17.59 \n",
      "2025-03-13 21:57:50,173: Train batch 1474: loss: 8.61 gradNorm: 24.26 \n",
      "2025-03-13 21:57:50,341: Train batch 1475: loss: 6.85 gradNorm: 22.78 \n",
      "2025-03-13 21:57:50,508: Train batch 1476: loss: 4.03 gradNorm: 15.94 \n",
      "2025-03-13 21:57:50,580: Train batch 1477: loss: 8.36 gradNorm: 25.97 \n",
      "2025-03-13 21:57:50,753: Train batch 1478: loss: 4.38 gradNorm: 17.22 \n",
      "2025-03-13 21:57:50,954: Train batch 1479: loss: 6.11 gradNorm: 19.22 \n",
      "2025-03-13 21:57:51,030: Train batch 1480: loss: 7.88 gradNorm: 22.58 \n",
      "2025-03-13 21:57:51,118: Train batch 1481: loss: 7.78 gradNorm: 23.20 \n",
      "2025-03-13 21:57:51,510: Train batch 1482: loss: 7.63 gradNorm: 25.55 \n",
      "2025-03-13 21:57:51,766: Train batch 1483: loss: 14.87 gradNorm: 61.57 \n",
      "2025-03-13 21:57:51,941: Train batch 1484: loss: 4.14 gradNorm: 18.93 \n",
      "2025-03-13 21:57:52,028: Train batch 1485: loss: 7.44 gradNorm: 24.19 \n",
      "2025-03-13 21:57:52,113: Train batch 1486: loss: 10.35 gradNorm: 27.40 \n",
      "2025-03-13 21:57:52,217: Train batch 1487: loss: 6.97 gradNorm: 21.52 \n",
      "2025-03-13 21:57:52,355: Train batch 1488: loss: 8.24 gradNorm: 22.81 \n",
      "2025-03-13 21:57:52,455: Train batch 1489: loss: 6.56 gradNorm: 19.03 \n",
      "2025-03-13 21:57:52,575: Train batch 1490: loss: 6.00 gradNorm: 17.67 \n",
      "2025-03-13 21:57:52,710: Train batch 1491: loss: 6.96 gradNorm: 26.57 \n",
      "2025-03-13 21:57:52,843: Train batch 1492: loss: 9.47 gradNorm: 26.19 \n",
      "2025-03-13 21:57:52,922: Train batch 1493: loss: 6.67 gradNorm: 20.39 \n",
      "2025-03-13 21:57:53,046: Train batch 1494: loss: 5.65 gradNorm: 19.36 \n",
      "2025-03-13 21:57:53,248: Train batch 1495: loss: 7.80 gradNorm: 25.68 \n",
      "2025-03-13 21:57:53,562: Train batch 1496: loss: 10.71 gradNorm: 32.46 \n",
      "2025-03-13 21:57:53,651: Train batch 1497: loss: 6.57 gradNorm: 20.62 \n",
      "2025-03-13 21:57:53,782: Train batch 1498: loss: 6.70 gradNorm: 19.67 \n",
      "2025-03-13 21:57:54,067: Train batch 1499: loss: 7.66 gradNorm: 26.87 \n",
      "2025-03-13 21:57:54,179: Train batch 1500: loss: 6.42 gradNorm: 17.24 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:57:54,384: Val batch: CER (t18.2025.01.15): 0.090\n",
      "2025-03-13 21:57:54,386: Val batch 1500: CER (avg): 0.090 \n",
      "2025-03-13 21:57:54,386: Batches since validation CER improved: 150\n",
      "2025-03-13 21:57:54,454: Train batch 1501: loss: 8.92 gradNorm: 26.18 \n",
      "2025-03-13 21:57:54,598: Train batch 1502: loss: 3.79 gradNorm: 15.85 \n",
      "2025-03-13 21:57:54,678: Train batch 1503: loss: 6.55 gradNorm: 19.45 \n",
      "2025-03-13 21:57:54,745: Train batch 1504: loss: 6.66 gradNorm: 20.55 \n",
      "2025-03-13 21:57:54,881: Train batch 1505: loss: 4.88 gradNorm: 18.61 \n",
      "2025-03-13 21:57:55,182: Train batch 1506: loss: 10.88 gradNorm: 40.93 \n",
      "2025-03-13 21:57:55,263: Train batch 1507: loss: 7.33 gradNorm: 22.54 \n",
      "2025-03-13 21:57:55,344: Train batch 1508: loss: 6.38 gradNorm: 21.30 \n",
      "2025-03-13 21:57:55,592: Train batch 1509: loss: 9.10 gradNorm: 28.68 \n",
      "2025-03-13 21:57:55,746: Train batch 1510: loss: 7.08 gradNorm: 26.42 \n",
      "2025-03-13 21:57:55,868: Train batch 1511: loss: 7.99 gradNorm: 23.67 \n",
      "2025-03-13 21:57:55,939: Train batch 1512: loss: 6.38 gradNorm: 21.46 \n",
      "2025-03-13 21:57:56,023: Train batch 1513: loss: 8.06 gradNorm: 24.78 \n",
      "2025-03-13 21:57:56,414: Train batch 1514: loss: 9.13 gradNorm: 26.37 \n",
      "2025-03-13 21:57:56,694: Train batch 1515: loss: 11.37 gradNorm: 36.49 \n",
      "2025-03-13 21:57:56,775: Train batch 1516: loss: 8.34 gradNorm: 23.26 \n",
      "2025-03-13 21:57:56,851: Train batch 1517: loss: 6.75 gradNorm: 23.05 \n",
      "2025-03-13 21:57:57,161: Train batch 1518: loss: 9.38 gradNorm: 28.16 \n",
      "2025-03-13 21:57:57,433: Train batch 1519: loss: 9.04 gradNorm: 27.32 \n",
      "2025-03-13 21:57:57,628: Train batch 1520: loss: 7.34 gradNorm: 24.11 \n",
      "2025-03-13 21:57:57,698: Train batch 1521: loss: 10.46 gradNorm: 34.27 \n",
      "2025-03-13 21:57:57,888: Train batch 1522: loss: 5.78 gradNorm: 21.83 \n",
      "2025-03-13 21:57:57,980: Train batch 1523: loss: 8.66 gradNorm: 23.40 \n",
      "2025-03-13 21:57:58,070: Train batch 1524: loss: 8.69 gradNorm: 23.45 \n",
      "2025-03-13 21:57:58,176: Train batch 1525: loss: 7.60 gradNorm: 23.81 \n",
      "2025-03-13 21:57:58,379: Train batch 1526: loss: 9.00 gradNorm: 33.35 \n",
      "2025-03-13 21:57:58,578: Train batch 1527: loss: 7.11 gradNorm: 29.68 \n",
      "2025-03-13 21:57:58,736: Train batch 1528: loss: 6.03 gradNorm: 26.27 \n",
      "2025-03-13 21:57:59,105: Train batch 1529: loss: 9.77 gradNorm: 29.32 \n",
      "2025-03-13 21:57:59,193: Train batch 1530: loss: 8.26 gradNorm: 28.60 \n",
      "2025-03-13 21:57:59,280: Train batch 1531: loss: 10.49 gradNorm: 27.72 \n",
      "2025-03-13 21:57:59,533: Train batch 1532: loss: 7.18 gradNorm: 24.64 \n",
      "2025-03-13 21:57:59,623: Train batch 1533: loss: 7.27 gradNorm: 21.96 \n",
      "2025-03-13 21:57:59,728: Train batch 1534: loss: 7.81 gradNorm: 25.92 \n",
      "2025-03-13 21:58:00,014: Train batch 1535: loss: 17.81 gradNorm: 69.97 \n",
      "2025-03-13 21:58:00,186: Train batch 1536: loss: 8.72 gradNorm: 29.62 \n",
      "2025-03-13 21:58:00,310: Train batch 1537: loss: 9.13 gradNorm: 27.09 \n",
      "2025-03-13 21:58:00,421: Train batch 1538: loss: 9.08 gradNorm: 21.07 \n",
      "2025-03-13 21:58:00,507: Train batch 1539: loss: 5.85 gradNorm: 20.84 \n",
      "2025-03-13 21:58:00,677: Train batch 1540: loss: 7.34 gradNorm: 25.52 \n",
      "2025-03-13 21:58:00,783: Train batch 1541: loss: 6.44 gradNorm: 21.60 \n",
      "2025-03-13 21:58:00,871: Train batch 1542: loss: 6.92 gradNorm: 19.68 \n",
      "2025-03-13 21:58:00,974: Train batch 1543: loss: 6.49 gradNorm: 20.10 \n",
      "2025-03-13 21:58:01,068: Train batch 1544: loss: 6.64 gradNorm: 21.62 \n",
      "2025-03-13 21:58:01,194: Train batch 1545: loss: 5.92 gradNorm: 16.73 \n",
      "2025-03-13 21:58:01,499: Train batch 1546: loss: 12.88 gradNorm: 47.90 \n",
      "2025-03-13 21:58:01,605: Train batch 1547: loss: 5.71 gradNorm: 20.49 \n",
      "2025-03-13 21:58:01,687: Train batch 1548: loss: 7.74 gradNorm: 23.48 \n",
      "2025-03-13 21:58:02,048: Train batch 1549: loss: 10.00 gradNorm: 28.72 \n",
      "2025-03-13 21:58:02,435: Train batch 1550: loss: 7.66 gradNorm: 26.98 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:58:02,642: Val batch: CER (t18.2025.01.15): 0.150\n",
      "2025-03-13 21:58:02,643: Val batch 1550: CER (avg): 0.150 \n",
      "2025-03-13 21:58:02,643: Batches since validation CER improved: 200\n",
      "2025-03-13 21:58:02,802: Train batch 1551: loss: 10.96 gradNorm: 39.65 \n",
      "2025-03-13 21:58:02,911: Train batch 1552: loss: 6.87 gradNorm: 21.23 \n",
      "2025-03-13 21:58:03,022: Train batch 1553: loss: 8.17 gradNorm: 28.03 \n",
      "2025-03-13 21:58:03,100: Train batch 1554: loss: 6.17 gradNorm: 20.05 \n",
      "2025-03-13 21:58:03,191: Train batch 1555: loss: 8.36 gradNorm: 23.04 \n",
      "2025-03-13 21:58:03,299: Train batch 1556: loss: 6.76 gradNorm: 24.76 \n",
      "2025-03-13 21:58:03,660: Train batch 1557: loss: 9.49 gradNorm: 30.33 \n",
      "2025-03-13 21:58:03,768: Train batch 1558: loss: 5.48 gradNorm: 18.89 \n",
      "2025-03-13 21:58:03,935: Train batch 1559: loss: 6.38 gradNorm: 24.43 \n",
      "2025-03-13 21:58:04,220: Train batch 1560: loss: 10.56 gradNorm: 36.65 \n",
      "2025-03-13 21:58:04,490: Train batch 1561: loss: 9.62 gradNorm: 29.10 \n",
      "2025-03-13 21:58:04,565: Train batch 1562: loss: 6.20 gradNorm: 22.37 \n",
      "2025-03-13 21:58:04,707: Train batch 1563: loss: 6.72 gradNorm: 20.43 \n",
      "2025-03-13 21:58:04,999: Train batch 1564: loss: 7.35 gradNorm: 29.83 \n",
      "2025-03-13 21:58:05,097: Train batch 1565: loss: 7.39 gradNorm: 24.54 \n",
      "2025-03-13 21:58:05,198: Train batch 1566: loss: 7.91 gradNorm: 25.98 \n",
      "2025-03-13 21:58:05,310: Train batch 1567: loss: 6.95 gradNorm: 22.66 \n",
      "2025-03-13 21:58:05,413: Train batch 1568: loss: 6.61 gradNorm: 21.87 \n",
      "2025-03-13 21:58:05,493: Train batch 1569: loss: 8.37 gradNorm: 25.98 \n",
      "2025-03-13 21:58:05,590: Train batch 1570: loss: 7.03 gradNorm: 22.99 \n",
      "2025-03-13 21:58:05,862: Train batch 1571: loss: 20.88 gradNorm: 71.09 \n",
      "2025-03-13 21:58:06,134: Train batch 1572: loss: 14.01 gradNorm: 49.56 \n",
      "2025-03-13 21:58:06,200: Train batch 1573: loss: 6.84 gradNorm: 22.46 \n",
      "2025-03-13 21:58:06,375: Train batch 1574: loss: 5.71 gradNorm: 24.40 \n",
      "2025-03-13 21:58:06,458: Train batch 1575: loss: 6.53 gradNorm: 22.73 \n",
      "2025-03-13 21:58:06,546: Train batch 1576: loss: 7.84 gradNorm: 26.36 \n",
      "2025-03-13 21:58:06,621: Train batch 1577: loss: 5.83 gradNorm: 18.02 \n",
      "2025-03-13 21:58:06,888: Train batch 1578: loss: 9.60 gradNorm: 34.29 \n",
      "2025-03-13 21:58:07,083: Train batch 1579: loss: 6.21 gradNorm: 22.54 \n",
      "2025-03-13 21:58:07,261: Train batch 1580: loss: 4.90 gradNorm: 18.39 \n",
      "2025-03-13 21:58:07,542: Train batch 1581: loss: 8.89 gradNorm: 25.34 \n",
      "2025-03-13 21:58:07,679: Train batch 1582: loss: 8.79 gradNorm: 24.65 \n",
      "2025-03-13 21:58:07,803: Train batch 1583: loss: 4.89 gradNorm: 18.92 \n",
      "2025-03-13 21:58:07,903: Train batch 1584: loss: 7.42 gradNorm: 24.86 \n",
      "2025-03-13 21:58:07,982: Train batch 1585: loss: 7.92 gradNorm: 28.41 \n",
      "2025-03-13 21:58:08,053: Train batch 1586: loss: 7.85 gradNorm: 24.04 \n",
      "2025-03-13 21:58:08,414: Train batch 1587: loss: 9.00 gradNorm: 37.17 \n",
      "2025-03-13 21:58:08,662: Train batch 1588: loss: 8.92 gradNorm: 31.77 \n",
      "2025-03-13 21:58:08,749: Train batch 1589: loss: 6.42 gradNorm: 21.03 \n",
      "2025-03-13 21:58:09,051: Train batch 1590: loss: 8.74 gradNorm: 29.09 \n",
      "2025-03-13 21:58:09,322: Train batch 1591: loss: 7.27 gradNorm: 23.43 \n",
      "2025-03-13 21:58:09,481: Train batch 1592: loss: 5.13 gradNorm: 23.69 \n",
      "2025-03-13 21:58:09,703: Train batch 1593: loss: 7.68 gradNorm: 25.43 \n",
      "2025-03-13 21:58:09,959: Train batch 1594: loss: 8.53 gradNorm: 28.29 \n",
      "2025-03-13 21:58:10,040: Train batch 1595: loss: 6.78 gradNorm: 24.62 \n",
      "2025-03-13 21:58:10,170: Train batch 1596: loss: 3.57 gradNorm: 15.21 \n",
      "2025-03-13 21:58:10,273: Train batch 1597: loss: 8.05 gradNorm: 23.31 \n",
      "2025-03-13 21:58:10,346: Train batch 1598: loss: 4.65 gradNorm: 15.85 \n",
      "2025-03-13 21:58:10,496: Train batch 1599: loss: 7.26 gradNorm: 23.43 \n",
      "2025-03-13 21:58:10,678: Train batch 1600: loss: 4.22 gradNorm: 16.86 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:58:10,881: Val batch: CER (t18.2025.01.15): 0.090\n",
      "2025-03-13 21:58:10,882: Val batch 1600: CER (avg): 0.090 \n",
      "2025-03-13 21:58:11,088: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:58:11,092: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:58:11,092: Batches since validation CER improved: 0\n",
      "2025-03-13 21:58:11,255: Train batch 1601: loss: 3.65 gradNorm: 15.92 \n",
      "2025-03-13 21:58:11,523: Train batch 1602: loss: 12.03 gradNorm: 39.41 \n",
      "2025-03-13 21:58:11,876: Train batch 1603: loss: 7.87 gradNorm: 27.79 \n",
      "2025-03-13 21:58:11,973: Train batch 1604: loss: 9.25 gradNorm: 26.53 \n",
      "2025-03-13 21:58:12,168: Train batch 1605: loss: 7.12 gradNorm: 24.47 \n",
      "2025-03-13 21:58:12,265: Train batch 1606: loss: 6.39 gradNorm: 22.98 \n",
      "2025-03-13 21:58:12,538: Train batch 1607: loss: 7.08 gradNorm: 28.69 \n",
      "2025-03-13 21:58:12,610: Train batch 1608: loss: 7.51 gradNorm: 25.89 \n",
      "2025-03-13 21:58:12,782: Train batch 1609: loss: 4.57 gradNorm: 18.96 \n",
      "2025-03-13 21:58:12,946: Train batch 1610: loss: 4.08 gradNorm: 17.63 \n",
      "2025-03-13 21:58:13,028: Train batch 1611: loss: 6.98 gradNorm: 18.94 \n",
      "2025-03-13 21:58:13,125: Train batch 1612: loss: 7.24 gradNorm: 24.36 \n",
      "2025-03-13 21:58:13,290: Train batch 1613: loss: 6.98 gradNorm: 24.90 \n",
      "2025-03-13 21:58:13,452: Train batch 1614: loss: 4.25 gradNorm: 18.46 \n",
      "2025-03-13 21:58:13,536: Train batch 1615: loss: 6.35 gradNorm: 21.78 \n",
      "2025-03-13 21:58:13,651: Train batch 1616: loss: 6.46 gradNorm: 24.79 \n",
      "2025-03-13 21:58:13,726: Train batch 1617: loss: 5.03 gradNorm: 18.81 \n",
      "2025-03-13 21:58:13,817: Train batch 1618: loss: 5.87 gradNorm: 20.62 \n",
      "2025-03-13 21:58:13,932: Train batch 1619: loss: 7.21 gradNorm: 21.53 \n",
      "2025-03-13 21:58:14,168: Train batch 1620: loss: 9.10 gradNorm: 29.63 \n",
      "2025-03-13 21:58:14,333: Train batch 1621: loss: 4.07 gradNorm: 19.83 \n",
      "2025-03-13 21:58:14,420: Train batch 1622: loss: 8.05 gradNorm: 26.41 \n",
      "2025-03-13 21:58:14,600: Train batch 1623: loss: 6.09 gradNorm: 21.69 \n",
      "2025-03-13 21:58:14,672: Train batch 1624: loss: 5.85 gradNorm: 21.99 \n",
      "2025-03-13 21:58:15,067: Train batch 1625: loss: 10.34 gradNorm: 30.01 \n",
      "2025-03-13 21:58:15,417: Train batch 1626: loss: 9.64 gradNorm: 28.73 \n",
      "2025-03-13 21:58:15,728: Train batch 1627: loss: 6.12 gradNorm: 22.62 \n",
      "2025-03-13 21:58:16,018: Train batch 1628: loss: 8.58 gradNorm: 29.43 \n",
      "2025-03-13 21:58:16,102: Train batch 1629: loss: 5.97 gradNorm: 20.01 \n",
      "2025-03-13 21:58:16,395: Train batch 1630: loss: 7.22 gradNorm: 26.80 \n",
      "2025-03-13 21:58:16,527: Train batch 1631: loss: 7.55 gradNorm: 21.03 \n",
      "2025-03-13 21:58:16,618: Train batch 1632: loss: 6.38 gradNorm: 23.08 \n",
      "2025-03-13 21:58:16,694: Train batch 1633: loss: 5.90 gradNorm: 23.28 \n",
      "2025-03-13 21:58:16,872: Train batch 1634: loss: 4.20 gradNorm: 18.57 \n",
      "2025-03-13 21:58:17,147: Train batch 1635: loss: 10.25 gradNorm: 37.80 \n",
      "2025-03-13 21:58:17,261: Train batch 1636: loss: 6.04 gradNorm: 21.36 \n",
      "2025-03-13 21:58:17,337: Train batch 1637: loss: 5.44 gradNorm: 21.47 \n",
      "2025-03-13 21:58:17,449: Train batch 1638: loss: 5.41 gradNorm: 18.63 \n",
      "2025-03-13 21:58:17,549: Train batch 1639: loss: 5.90 gradNorm: 18.64 \n",
      "2025-03-13 21:58:17,703: Train batch 1640: loss: 5.24 gradNorm: 21.22 \n",
      "2025-03-13 21:58:17,902: Train batch 1641: loss: 8.01 gradNorm: 24.90 \n",
      "2025-03-13 21:58:18,041: Train batch 1642: loss: 6.07 gradNorm: 23.64 \n",
      "2025-03-13 21:58:18,284: Train batch 1643: loss: 14.40 gradNorm: 50.00 \n",
      "2025-03-13 21:58:18,670: Train batch 1644: loss: 12.42 gradNorm: 42.18 \n",
      "2025-03-13 21:58:18,868: Train batch 1645: loss: 5.30 gradNorm: 19.93 \n",
      "2025-03-13 21:58:18,951: Train batch 1646: loss: 6.99 gradNorm: 23.53 \n",
      "2025-03-13 21:58:19,040: Train batch 1647: loss: 7.95 gradNorm: 28.38 \n",
      "2025-03-13 21:58:19,125: Train batch 1648: loss: 8.49 gradNorm: 24.64 \n",
      "2025-03-13 21:58:19,288: Train batch 1649: loss: 4.04 gradNorm: 19.55 \n",
      "2025-03-13 21:58:19,519: Train batch 1650: loss: 8.33 gradNorm: 28.47 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:58:19,719: Val batch: CER (t18.2025.01.15): 0.084\n",
      "2025-03-13 21:58:19,721: Val batch 1650: CER (avg): 0.084 \n",
      "2025-03-13 21:58:19,938: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:58:19,942: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:58:19,943: Batches since validation CER improved: 0\n",
      "2025-03-13 21:58:20,014: Train batch 1651: loss: 7.61 gradNorm: 26.95 \n",
      "2025-03-13 21:58:20,115: Train batch 1652: loss: 9.10 gradNorm: 24.42 \n",
      "2025-03-13 21:58:20,492: Train batch 1653: loss: 6.35 gradNorm: 22.97 \n",
      "2025-03-13 21:58:20,588: Train batch 1654: loss: 6.40 gradNorm: 20.22 \n",
      "2025-03-13 21:58:20,669: Train batch 1655: loss: 6.81 gradNorm: 22.33 \n",
      "2025-03-13 21:58:20,744: Train batch 1656: loss: 4.65 gradNorm: 16.03 \n",
      "2025-03-13 21:58:20,827: Train batch 1657: loss: 5.98 gradNorm: 20.82 \n",
      "2025-03-13 21:58:20,997: Train batch 1658: loss: 3.79 gradNorm: 17.48 \n",
      "2025-03-13 21:58:21,105: Train batch 1659: loss: 9.51 gradNorm: 24.96 \n",
      "2025-03-13 21:58:21,503: Train batch 1660: loss: 10.19 gradNorm: 47.13 \n",
      "2025-03-13 21:58:21,615: Train batch 1661: loss: 6.43 gradNorm: 19.78 \n",
      "2025-03-13 21:58:21,897: Train batch 1662: loss: 11.43 gradNorm: 40.35 \n",
      "2025-03-13 21:58:22,056: Train batch 1663: loss: 3.40 gradNorm: 14.88 \n",
      "2025-03-13 21:58:22,237: Train batch 1664: loss: 7.26 gradNorm: 28.72 \n",
      "2025-03-13 21:58:22,417: Train batch 1665: loss: 3.89 gradNorm: 14.66 \n",
      "2025-03-13 21:58:22,672: Train batch 1666: loss: 6.21 gradNorm: 20.91 \n",
      "2025-03-13 21:58:22,982: Train batch 1667: loss: 6.73 gradNorm: 23.84 \n",
      "2025-03-13 21:58:23,087: Train batch 1668: loss: 5.61 gradNorm: 17.58 \n",
      "2025-03-13 21:58:23,366: Train batch 1669: loss: 5.86 gradNorm: 22.35 \n",
      "2025-03-13 21:58:23,728: Train batch 1670: loss: 7.03 gradNorm: 22.04 \n",
      "2025-03-13 21:58:23,958: Train batch 1671: loss: 5.88 gradNorm: 18.85 \n",
      "2025-03-13 21:58:24,036: Train batch 1672: loss: 6.64 gradNorm: 24.58 \n",
      "2025-03-13 21:58:24,240: Train batch 1673: loss: 5.78 gradNorm: 24.22 \n",
      "2025-03-13 21:58:24,345: Train batch 1674: loss: 6.42 gradNorm: 20.96 \n",
      "2025-03-13 21:58:24,699: Train batch 1675: loss: 8.16 gradNorm: 31.12 \n",
      "2025-03-13 21:58:24,828: Train batch 1676: loss: 3.54 gradNorm: 15.46 \n",
      "2025-03-13 21:58:24,942: Train batch 1677: loss: 4.58 gradNorm: 16.41 \n",
      "2025-03-13 21:58:25,034: Train batch 1678: loss: 7.90 gradNorm: 23.84 \n",
      "2025-03-13 21:58:25,117: Train batch 1679: loss: 8.93 gradNorm: 29.34 \n",
      "2025-03-13 21:58:25,195: Train batch 1680: loss: 9.08 gradNorm: 32.20 \n",
      "2025-03-13 21:58:25,373: Train batch 1681: loss: 4.77 gradNorm: 23.88 \n",
      "2025-03-13 21:58:25,477: Train batch 1682: loss: 5.99 gradNorm: 21.10 \n",
      "2025-03-13 21:58:25,610: Train batch 1683: loss: 4.46 gradNorm: 21.58 \n",
      "2025-03-13 21:58:25,690: Train batch 1684: loss: 4.27 gradNorm: 16.56 \n",
      "2025-03-13 21:58:25,942: Train batch 1685: loss: 10.86 gradNorm: 40.83 \n",
      "2025-03-13 21:58:26,017: Train batch 1686: loss: 5.73 gradNorm: 23.83 \n",
      "2025-03-13 21:58:26,177: Train batch 1687: loss: 3.30 gradNorm: 16.68 \n",
      "2025-03-13 21:58:26,346: Train batch 1688: loss: 5.19 gradNorm: 22.62 \n",
      "2025-03-13 21:58:26,453: Train batch 1689: loss: 5.46 gradNorm: 20.50 \n",
      "2025-03-13 21:58:26,533: Train batch 1690: loss: 6.20 gradNorm: 21.72 \n",
      "2025-03-13 21:58:26,922: Train batch 1691: loss: 8.77 gradNorm: 27.16 \n",
      "2025-03-13 21:58:27,086: Train batch 1692: loss: 7.52 gradNorm: 25.29 \n",
      "2025-03-13 21:58:27,183: Train batch 1693: loss: 3.88 gradNorm: 16.82 \n",
      "2025-03-13 21:58:27,309: Train batch 1694: loss: 5.00 gradNorm: 18.28 \n",
      "2025-03-13 21:58:27,384: Train batch 1695: loss: 5.20 gradNorm: 17.48 \n",
      "2025-03-13 21:58:27,658: Train batch 1696: loss: 6.38 gradNorm: 29.65 \n",
      "2025-03-13 21:58:27,766: Train batch 1697: loss: 5.22 gradNorm: 19.18 \n",
      "2025-03-13 21:58:27,867: Train batch 1698: loss: 9.26 gradNorm: 25.23 \n",
      "2025-03-13 21:58:27,952: Train batch 1699: loss: 7.99 gradNorm: 26.15 \n",
      "2025-03-13 21:58:28,105: Train batch 1700: loss: 4.79 gradNorm: 26.01 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:58:28,298: Val batch: CER (t18.2025.01.15): 0.107\n",
      "2025-03-13 21:58:28,299: Val batch 1700: CER (avg): 0.107 \n",
      "2025-03-13 21:58:28,299: Batches since validation CER improved: 50\n",
      "2025-03-13 21:58:28,387: Train batch 1701: loss: 4.43 gradNorm: 17.64 \n",
      "2025-03-13 21:58:28,475: Train batch 1702: loss: 8.06 gradNorm: 29.36 \n",
      "2025-03-13 21:58:28,582: Train batch 1703: loss: 6.49 gradNorm: 19.67 \n",
      "2025-03-13 21:58:28,657: Train batch 1704: loss: 6.66 gradNorm: 22.42 \n",
      "2025-03-13 21:58:29,064: Train batch 1705: loss: 7.82 gradNorm: 28.61 \n",
      "2025-03-13 21:58:29,330: Train batch 1706: loss: 5.33 gradNorm: 19.82 \n",
      "2025-03-13 21:58:29,677: Train batch 1707: loss: 6.14 gradNorm: 21.61 \n",
      "2025-03-13 21:58:29,759: Train batch 1708: loss: 6.22 gradNorm: 24.66 \n",
      "2025-03-13 21:58:29,852: Train batch 1709: loss: 7.25 gradNorm: 23.93 \n",
      "2025-03-13 21:58:30,123: Train batch 1710: loss: 10.84 gradNorm: 41.97 \n",
      "2025-03-13 21:58:30,280: Train batch 1711: loss: 4.40 gradNorm: 21.33 \n",
      "2025-03-13 21:58:30,409: Train batch 1712: loss: 4.40 gradNorm: 19.08 \n",
      "2025-03-13 21:58:30,547: Train batch 1713: loss: 7.10 gradNorm: 24.97 \n",
      "2025-03-13 21:58:30,691: Train batch 1714: loss: 5.64 gradNorm: 19.90 \n",
      "2025-03-13 21:58:30,927: Train batch 1715: loss: 10.85 gradNorm: 49.61 \n",
      "2025-03-13 21:58:31,057: Train batch 1716: loss: 5.52 gradNorm: 26.15 \n",
      "2025-03-13 21:58:31,155: Train batch 1717: loss: 6.38 gradNorm: 21.61 \n",
      "2025-03-13 21:58:31,303: Train batch 1718: loss: 3.03 gradNorm: 16.14 \n",
      "2025-03-13 21:58:31,385: Train batch 1719: loss: 7.81 gradNorm: 25.98 \n",
      "2025-03-13 21:58:31,470: Train batch 1720: loss: 5.81 gradNorm: 22.71 \n",
      "2025-03-13 21:58:31,572: Train batch 1721: loss: 7.88 gradNorm: 25.69 \n",
      "2025-03-13 21:58:31,856: Train batch 1722: loss: 9.59 gradNorm: 36.26 \n",
      "2025-03-13 21:58:32,017: Train batch 1723: loss: 4.64 gradNorm: 24.48 \n",
      "2025-03-13 21:58:32,104: Train batch 1724: loss: 6.02 gradNorm: 22.10 \n",
      "2025-03-13 21:58:32,204: Train batch 1725: loss: 7.27 gradNorm: 24.58 \n",
      "2025-03-13 21:58:32,303: Train batch 1726: loss: 6.45 gradNorm: 23.79 \n",
      "2025-03-13 21:58:32,656: Train batch 1727: loss: 9.38 gradNorm: 32.48 \n",
      "2025-03-13 21:58:32,731: Train batch 1728: loss: 5.56 gradNorm: 23.32 \n",
      "2025-03-13 21:58:32,814: Train batch 1729: loss: 5.86 gradNorm: 22.93 \n",
      "2025-03-13 21:58:32,891: Train batch 1730: loss: 5.45 gradNorm: 18.94 \n",
      "2025-03-13 21:58:33,210: Train batch 1731: loss: 8.42 gradNorm: 31.84 \n",
      "2025-03-13 21:58:33,308: Train batch 1732: loss: 6.26 gradNorm: 21.24 \n",
      "2025-03-13 21:58:33,606: Train batch 1733: loss: 8.41 gradNorm: 34.35 \n",
      "2025-03-13 21:58:33,685: Train batch 1734: loss: 5.94 gradNorm: 22.17 \n",
      "2025-03-13 21:58:34,034: Train batch 1735: loss: 5.56 gradNorm: 23.42 \n",
      "2025-03-13 21:58:34,332: Train batch 1736: loss: 5.41 gradNorm: 21.13 \n",
      "2025-03-13 21:58:34,512: Train batch 1737: loss: 5.88 gradNorm: 21.59 \n",
      "2025-03-13 21:58:34,687: Train batch 1738: loss: 5.06 gradNorm: 17.47 \n",
      "2025-03-13 21:58:34,789: Train batch 1739: loss: 8.46 gradNorm: 25.39 \n",
      "2025-03-13 21:58:34,927: Train batch 1740: loss: 4.30 gradNorm: 19.15 \n",
      "2025-03-13 21:58:35,104: Train batch 1741: loss: 4.06 gradNorm: 18.31 \n",
      "2025-03-13 21:58:35,384: Train batch 1742: loss: 7.50 gradNorm: 26.25 \n",
      "2025-03-13 21:58:35,643: Train batch 1743: loss: 6.89 gradNorm: 34.45 \n",
      "2025-03-13 21:58:35,807: Train batch 1744: loss: 6.12 gradNorm: 22.54 \n",
      "2025-03-13 21:58:36,160: Train batch 1745: loss: 6.02 gradNorm: 21.51 \n",
      "2025-03-13 21:58:36,429: Train batch 1746: loss: 6.94 gradNorm: 38.06 \n",
      "2025-03-13 21:58:36,557: Train batch 1747: loss: 7.23 gradNorm: 28.30 \n",
      "2025-03-13 21:58:36,631: Train batch 1748: loss: 5.57 gradNorm: 21.51 \n",
      "2025-03-13 21:58:36,714: Train batch 1749: loss: 6.78 gradNorm: 23.39 \n",
      "2025-03-13 21:58:36,874: Train batch 1750: loss: 2.91 gradNorm: 14.64 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:58:37,058: Val batch: CER (t18.2025.01.15): 0.084\n",
      "2025-03-13 21:58:37,060: Val batch 1750: CER (avg): 0.084 \n",
      "2025-03-13 21:58:37,060: Batches since validation CER improved: 100\n",
      "2025-03-13 21:58:37,346: Train batch 1751: loss: 9.54 gradNorm: 37.40 \n",
      "2025-03-13 21:58:37,448: Train batch 1752: loss: 6.39 gradNorm: 21.20 \n",
      "2025-03-13 21:58:37,743: Train batch 1753: loss: 6.37 gradNorm: 29.23 \n",
      "2025-03-13 21:58:37,822: Train batch 1754: loss: 5.44 gradNorm: 17.98 \n",
      "2025-03-13 21:58:37,933: Train batch 1755: loss: 4.71 gradNorm: 18.25 \n",
      "2025-03-13 21:58:38,204: Train batch 1756: loss: 6.30 gradNorm: 24.88 \n",
      "2025-03-13 21:58:38,276: Train batch 1757: loss: 5.46 gradNorm: 22.61 \n",
      "2025-03-13 21:58:38,354: Train batch 1758: loss: 5.85 gradNorm: 21.77 \n",
      "2025-03-13 21:58:38,528: Train batch 1759: loss: 6.39 gradNorm: 31.01 \n",
      "2025-03-13 21:58:38,641: Train batch 1760: loss: 8.31 gradNorm: 28.21 \n",
      "2025-03-13 21:58:38,912: Train batch 1761: loss: 7.33 gradNorm: 27.35 \n",
      "2025-03-13 21:58:39,012: Train batch 1762: loss: 6.45 gradNorm: 18.99 \n",
      "2025-03-13 21:58:39,199: Train batch 1763: loss: 5.87 gradNorm: 23.46 \n",
      "2025-03-13 21:58:39,287: Train batch 1764: loss: 6.37 gradNorm: 24.00 \n",
      "2025-03-13 21:58:39,378: Train batch 1765: loss: 5.27 gradNorm: 20.76 \n",
      "2025-03-13 21:58:39,475: Train batch 1766: loss: 7.64 gradNorm: 25.35 \n",
      "2025-03-13 21:58:39,554: Train batch 1767: loss: 4.35 gradNorm: 17.76 \n",
      "2025-03-13 21:58:39,725: Train batch 1768: loss: 4.26 gradNorm: 19.70 \n",
      "2025-03-13 21:58:39,867: Train batch 1769: loss: 6.60 gradNorm: 28.95 \n",
      "2025-03-13 21:58:39,969: Train batch 1770: loss: 4.88 gradNorm: 18.28 \n",
      "2025-03-13 21:58:40,130: Train batch 1771: loss: 3.88 gradNorm: 19.50 \n",
      "2025-03-13 21:58:40,415: Train batch 1772: loss: 6.44 gradNorm: 25.74 \n",
      "2025-03-13 21:58:40,546: Train batch 1773: loss: 2.98 gradNorm: 15.24 \n",
      "2025-03-13 21:58:40,816: Train batch 1774: loss: 7.71 gradNorm: 29.68 \n",
      "2025-03-13 21:58:40,919: Train batch 1775: loss: 6.51 gradNorm: 20.82 \n",
      "2025-03-13 21:58:41,021: Train batch 1776: loss: 5.52 gradNorm: 20.22 \n",
      "2025-03-13 21:58:41,180: Train batch 1777: loss: 3.76 gradNorm: 17.63 \n",
      "2025-03-13 21:58:41,267: Train batch 1778: loss: 6.08 gradNorm: 23.45 \n",
      "2025-03-13 21:58:41,436: Train batch 1779: loss: 2.20 gradNorm: 13.95 \n",
      "2025-03-13 21:58:41,518: Train batch 1780: loss: 7.13 gradNorm: 24.38 \n",
      "2025-03-13 21:58:41,687: Train batch 1781: loss: 6.02 gradNorm: 24.25 \n",
      "2025-03-13 21:58:42,033: Train batch 1782: loss: 10.29 gradNorm: 39.81 \n",
      "2025-03-13 21:58:42,193: Train batch 1783: loss: 3.27 gradNorm: 15.63 \n",
      "2025-03-13 21:58:42,349: Train batch 1784: loss: 3.38 gradNorm: 18.16 \n",
      "2025-03-13 21:58:42,616: Train batch 1785: loss: 8.77 gradNorm: 38.61 \n",
      "2025-03-13 21:58:42,747: Train batch 1786: loss: 3.13 gradNorm: 18.41 \n",
      "2025-03-13 21:58:42,877: Train batch 1787: loss: 5.04 gradNorm: 18.61 \n",
      "2025-03-13 21:58:43,033: Train batch 1788: loss: 3.32 gradNorm: 18.10 \n",
      "2025-03-13 21:58:43,122: Train batch 1789: loss: 6.01 gradNorm: 19.94 \n",
      "2025-03-13 21:58:43,541: Train batch 1790: loss: 5.99 gradNorm: 26.70 \n",
      "2025-03-13 21:58:43,668: Train batch 1791: loss: 4.18 gradNorm: 17.22 \n",
      "2025-03-13 21:58:43,747: Train batch 1792: loss: 4.34 gradNorm: 17.45 \n",
      "2025-03-13 21:58:43,842: Train batch 1793: loss: 6.76 gradNorm: 22.53 \n",
      "2025-03-13 21:58:44,039: Train batch 1794: loss: 5.20 gradNorm: 19.61 \n",
      "2025-03-13 21:58:44,179: Train batch 1795: loss: 5.87 gradNorm: 20.71 \n",
      "2025-03-13 21:58:44,336: Train batch 1796: loss: 2.42 gradNorm: 13.87 \n",
      "2025-03-13 21:58:44,426: Train batch 1797: loss: 4.57 gradNorm: 19.32 \n",
      "2025-03-13 21:58:44,535: Train batch 1798: loss: 5.21 gradNorm: 21.00 \n",
      "2025-03-13 21:58:44,618: Train batch 1799: loss: 5.83 gradNorm: 19.62 \n",
      "2025-03-13 21:58:44,711: Train batch 1800: loss: 6.56 gradNorm: 22.75 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:58:44,924: Val batch: CER (t18.2025.01.15): 0.083\n",
      "2025-03-13 21:58:44,926: Val batch 1800: CER (avg): 0.083 \n",
      "2025-03-13 21:58:45,149: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:58:45,152: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:58:45,152: Batches since validation CER improved: 0\n",
      "2025-03-13 21:58:45,320: Train batch 1801: loss: 5.89 gradNorm: 24.45 \n",
      "2025-03-13 21:58:45,525: Train batch 1802: loss: 5.09 gradNorm: 24.46 \n",
      "2025-03-13 21:58:45,662: Train batch 1803: loss: 2.90 gradNorm: 14.71 \n",
      "2025-03-13 21:58:45,736: Train batch 1804: loss: 5.17 gradNorm: 18.89 \n",
      "2025-03-13 21:58:45,808: Train batch 1805: loss: 6.11 gradNorm: 22.12 \n",
      "2025-03-13 21:58:45,893: Train batch 1806: loss: 4.03 gradNorm: 15.63 \n",
      "2025-03-13 21:58:46,146: Train batch 1807: loss: 12.52 gradNorm: 60.38 \n",
      "2025-03-13 21:58:46,499: Train batch 1808: loss: 7.35 gradNorm: 26.51 \n",
      "2025-03-13 21:58:46,640: Train batch 1809: loss: 7.10 gradNorm: 42.17 \n",
      "2025-03-13 21:58:46,728: Train batch 1810: loss: 5.97 gradNorm: 22.77 \n",
      "2025-03-13 21:58:46,833: Train batch 1811: loss: 6.61 gradNorm: 24.06 \n",
      "2025-03-13 21:58:47,222: Train batch 1812: loss: 6.77 gradNorm: 25.86 \n",
      "2025-03-13 21:58:47,532: Train batch 1813: loss: 6.87 gradNorm: 32.54 \n",
      "2025-03-13 21:58:47,660: Train batch 1814: loss: 5.14 gradNorm: 24.87 \n",
      "2025-03-13 21:58:47,849: Train batch 1815: loss: 6.35 gradNorm: 24.80 \n",
      "2025-03-13 21:58:47,999: Train batch 1816: loss: 2.96 gradNorm: 15.21 \n",
      "2025-03-13 21:58:48,081: Train batch 1817: loss: 6.29 gradNorm: 23.42 \n",
      "2025-03-13 21:58:48,366: Train batch 1818: loss: 5.66 gradNorm: 19.65 \n",
      "2025-03-13 21:58:48,518: Train batch 1819: loss: 2.43 gradNorm: 13.78 \n",
      "2025-03-13 21:58:48,632: Train batch 1820: loss: 4.32 gradNorm: 20.58 \n",
      "2025-03-13 21:58:48,783: Train batch 1821: loss: 2.19 gradNorm: 13.16 \n",
      "2025-03-13 21:58:48,886: Train batch 1822: loss: 6.44 gradNorm: 24.41 \n",
      "2025-03-13 21:58:49,161: Train batch 1823: loss: 5.38 gradNorm: 20.40 \n",
      "2025-03-13 21:58:49,244: Train batch 1824: loss: 4.39 gradNorm: 17.53 \n",
      "2025-03-13 21:58:49,331: Train batch 1825: loss: 5.39 gradNorm: 18.27 \n",
      "2025-03-13 21:58:49,507: Train batch 1826: loss: 4.29 gradNorm: 18.09 \n",
      "2025-03-13 21:58:49,610: Train batch 1827: loss: 4.51 gradNorm: 18.96 \n",
      "2025-03-13 21:58:49,765: Train batch 1828: loss: 3.35 gradNorm: 18.63 \n",
      "2025-03-13 21:58:49,891: Train batch 1829: loss: 4.01 gradNorm: 15.70 \n",
      "2025-03-13 21:58:50,018: Train batch 1830: loss: 4.16 gradNorm: 17.77 \n",
      "2025-03-13 21:58:50,256: Train batch 1831: loss: 4.97 gradNorm: 21.25 \n",
      "2025-03-13 21:58:50,357: Train batch 1832: loss: 5.39 gradNorm: 18.82 \n",
      "2025-03-13 21:58:50,446: Train batch 1833: loss: 5.66 gradNorm: 22.56 \n",
      "2025-03-13 21:58:50,841: Train batch 1834: loss: 5.55 gradNorm: 23.98 \n",
      "2025-03-13 21:58:51,140: Train batch 1835: loss: 7.15 gradNorm: 27.67 \n",
      "2025-03-13 21:58:51,239: Train batch 1836: loss: 5.40 gradNorm: 20.41 \n",
      "2025-03-13 21:58:51,591: Train batch 1837: loss: 5.24 gradNorm: 22.97 \n",
      "2025-03-13 21:58:51,681: Train batch 1838: loss: 4.28 gradNorm: 20.42 \n",
      "2025-03-13 21:58:51,770: Train batch 1839: loss: 7.64 gradNorm: 27.04 \n",
      "2025-03-13 21:58:51,843: Train batch 1840: loss: 5.19 gradNorm: 21.66 \n",
      "2025-03-13 21:58:51,916: Train batch 1841: loss: 5.81 gradNorm: 24.17 \n",
      "2025-03-13 21:58:52,220: Train batch 1842: loss: 6.48 gradNorm: 24.28 \n",
      "2025-03-13 21:58:52,299: Train batch 1843: loss: 4.45 gradNorm: 18.02 \n",
      "2025-03-13 21:58:52,386: Train batch 1844: loss: 4.07 gradNorm: 17.87 \n",
      "2025-03-13 21:58:52,626: Train batch 1845: loss: 5.96 gradNorm: 27.33 \n",
      "2025-03-13 21:58:52,702: Train batch 1846: loss: 3.90 gradNorm: 16.68 \n",
      "2025-03-13 21:58:52,833: Train batch 1847: loss: 3.31 gradNorm: 15.44 \n",
      "2025-03-13 21:58:53,107: Train batch 1848: loss: 4.33 gradNorm: 17.32 \n",
      "2025-03-13 21:58:53,207: Train batch 1849: loss: 4.87 gradNorm: 19.50 \n",
      "2025-03-13 21:58:53,310: Train batch 1850: loss: 5.25 gradNorm: 22.36 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:58:53,503: Val batch: CER (t18.2025.01.15): 0.078\n",
      "2025-03-13 21:58:53,505: Val batch 1850: CER (avg): 0.078 \n",
      "2025-03-13 21:58:53,708: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:58:53,711: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:58:53,712: Batches since validation CER improved: 0\n",
      "2025-03-13 21:58:53,986: Train batch 1851: loss: 5.34 gradNorm: 22.59 \n",
      "2025-03-13 21:58:54,123: Train batch 1852: loss: 6.05 gradNorm: 24.58 \n",
      "2025-03-13 21:58:54,236: Train batch 1853: loss: 4.92 gradNorm: 20.30 \n",
      "2025-03-13 21:58:54,411: Train batch 1854: loss: 3.06 gradNorm: 15.17 \n",
      "2025-03-13 21:58:54,492: Train batch 1855: loss: 7.33 gradNorm: 26.23 \n",
      "2025-03-13 21:58:54,661: Train batch 1856: loss: 2.27 gradNorm: 11.68 \n",
      "2025-03-13 21:58:54,764: Train batch 1857: loss: 6.25 gradNorm: 21.38 \n",
      "2025-03-13 21:58:54,894: Train batch 1858: loss: 4.58 gradNorm: 18.53 \n",
      "2025-03-13 21:58:54,999: Train batch 1859: loss: 4.55 gradNorm: 17.49 \n",
      "2025-03-13 21:58:55,111: Train batch 1860: loss: 5.32 gradNorm: 19.18 \n",
      "2025-03-13 21:58:55,297: Train batch 1861: loss: 2.84 gradNorm: 16.42 \n",
      "2025-03-13 21:58:55,386: Train batch 1862: loss: 4.59 gradNorm: 19.15 \n",
      "2025-03-13 21:58:55,491: Train batch 1863: loss: 3.30 gradNorm: 13.84 \n",
      "2025-03-13 21:58:55,572: Train batch 1864: loss: 6.09 gradNorm: 28.24 \n",
      "2025-03-13 21:58:55,727: Train batch 1865: loss: 1.73 gradNorm: 13.61 \n",
      "2025-03-13 21:58:55,922: Train batch 1866: loss: 4.99 gradNorm: 23.21 \n",
      "2025-03-13 21:58:55,996: Train batch 1867: loss: 4.71 gradNorm: 19.80 \n",
      "2025-03-13 21:58:56,086: Train batch 1868: loss: 5.05 gradNorm: 19.63 \n",
      "2025-03-13 21:58:56,228: Train batch 1869: loss: 5.56 gradNorm: 20.55 \n",
      "2025-03-13 21:58:56,465: Train batch 1870: loss: 4.97 gradNorm: 21.76 \n",
      "2025-03-13 21:58:56,802: Train batch 1871: loss: 8.11 gradNorm: 36.51 \n",
      "2025-03-13 21:58:56,883: Train batch 1872: loss: 4.41 gradNorm: 18.44 \n",
      "2025-03-13 21:58:56,966: Train batch 1873: loss: 6.52 gradNorm: 23.89 \n",
      "2025-03-13 21:58:57,124: Train batch 1874: loss: 3.05 gradNorm: 16.33 \n",
      "2025-03-13 21:58:57,257: Train batch 1875: loss: 2.50 gradNorm: 12.93 \n",
      "2025-03-13 21:58:57,356: Train batch 1876: loss: 7.19 gradNorm: 24.31 \n",
      "2025-03-13 21:58:57,457: Train batch 1877: loss: 5.53 gradNorm: 20.34 \n",
      "2025-03-13 21:58:57,649: Train batch 1878: loss: 5.35 gradNorm: 24.27 \n",
      "2025-03-13 21:58:57,929: Train batch 1879: loss: 8.01 gradNorm: 38.04 \n",
      "2025-03-13 21:58:58,094: Train batch 1880: loss: 5.67 gradNorm: 23.42 \n",
      "2025-03-13 21:58:58,453: Train batch 1881: loss: 7.11 gradNorm: 26.15 \n",
      "2025-03-13 21:58:58,720: Train batch 1882: loss: 5.73 gradNorm: 24.08 \n",
      "2025-03-13 21:58:58,883: Train batch 1883: loss: 4.79 gradNorm: 22.03 \n",
      "2025-03-13 21:58:58,969: Train batch 1884: loss: 4.56 gradNorm: 19.83 \n",
      "2025-03-13 21:58:59,167: Train batch 1885: loss: 2.59 gradNorm: 14.69 \n",
      "2025-03-13 21:58:59,270: Train batch 1886: loss: 4.64 gradNorm: 19.79 \n",
      "2025-03-13 21:58:59,337: Train batch 1887: loss: 5.11 gradNorm: 20.41 \n",
      "2025-03-13 21:58:59,471: Train batch 1888: loss: 4.03 gradNorm: 17.53 \n",
      "2025-03-13 21:58:59,559: Train batch 1889: loss: 5.71 gradNorm: 21.54 \n",
      "2025-03-13 21:58:59,734: Train batch 1890: loss: 2.46 gradNorm: 13.20 \n",
      "2025-03-13 21:58:59,816: Train batch 1891: loss: 4.12 gradNorm: 20.21 \n",
      "2025-03-13 21:58:59,974: Train batch 1892: loss: 2.81 gradNorm: 14.81 \n",
      "2025-03-13 21:59:00,132: Train batch 1893: loss: 1.85 gradNorm: 13.01 \n",
      "2025-03-13 21:59:00,432: Train batch 1894: loss: 6.66 gradNorm: 28.83 \n",
      "2025-03-13 21:59:00,583: Train batch 1895: loss: 2.86 gradNorm: 16.93 \n",
      "2025-03-13 21:59:00,972: Train batch 1896: loss: 4.48 gradNorm: 21.31 \n",
      "2025-03-13 21:59:01,138: Train batch 1897: loss: 5.41 gradNorm: 23.71 \n",
      "2025-03-13 21:59:01,217: Train batch 1898: loss: 7.16 gradNorm: 27.85 \n",
      "2025-03-13 21:59:01,302: Train batch 1899: loss: 4.90 gradNorm: 22.36 \n",
      "2025-03-13 21:59:01,430: Train batch 1900: loss: 2.73 gradNorm: 17.30 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:59:01,625: Val batch: CER (t18.2025.01.15): 0.097\n",
      "2025-03-13 21:59:01,626: Val batch 1900: CER (avg): 0.097 \n",
      "2025-03-13 21:59:01,626: Batches since validation CER improved: 50\n",
      "2025-03-13 21:59:01,896: Train batch 1901: loss: 4.75 gradNorm: 20.12 \n",
      "2025-03-13 21:59:02,302: Train batch 1902: loss: 5.22 gradNorm: 21.68 \n",
      "2025-03-13 21:59:02,471: Train batch 1903: loss: 1.99 gradNorm: 14.14 \n",
      "2025-03-13 21:59:02,631: Train batch 1904: loss: 1.90 gradNorm: 11.14 \n",
      "2025-03-13 21:59:02,979: Train batch 1905: loss: 3.85 gradNorm: 19.56 \n",
      "2025-03-13 21:59:03,172: Train batch 1906: loss: 4.60 gradNorm: 20.12 \n",
      "2025-03-13 21:59:03,431: Train batch 1907: loss: 5.49 gradNorm: 26.80 \n",
      "2025-03-13 21:59:03,706: Train batch 1908: loss: 4.88 gradNorm: 18.50 \n",
      "2025-03-13 21:59:03,876: Train batch 1909: loss: 4.92 gradNorm: 20.99 \n",
      "2025-03-13 21:59:03,974: Train batch 1910: loss: 6.08 gradNorm: 23.21 \n",
      "2025-03-13 21:59:04,247: Train batch 1911: loss: 5.44 gradNorm: 23.59 \n",
      "2025-03-13 21:59:04,398: Train batch 1912: loss: 2.51 gradNorm: 12.92 \n",
      "2025-03-13 21:59:04,586: Train batch 1913: loss: 5.67 gradNorm: 22.95 \n",
      "2025-03-13 21:59:04,829: Train batch 1914: loss: 7.58 gradNorm: 32.76 \n",
      "2025-03-13 21:59:04,906: Train batch 1915: loss: 4.22 gradNorm: 18.89 \n",
      "2025-03-13 21:59:05,008: Train batch 1916: loss: 5.62 gradNorm: 22.92 \n",
      "2025-03-13 21:59:05,339: Train batch 1917: loss: 4.82 gradNorm: 24.60 \n",
      "2025-03-13 21:59:05,435: Train batch 1918: loss: 6.09 gradNorm: 21.10 \n",
      "2025-03-13 21:59:05,538: Train batch 1919: loss: 3.39 gradNorm: 16.39 \n",
      "2025-03-13 21:59:05,667: Train batch 1920: loss: 3.44 gradNorm: 16.30 \n",
      "2025-03-13 21:59:06,036: Train batch 1921: loss: 4.98 gradNorm: 22.47 \n",
      "2025-03-13 21:59:06,303: Train batch 1922: loss: 6.31 gradNorm: 27.39 \n",
      "2025-03-13 21:59:06,578: Train batch 1923: loss: 3.70 gradNorm: 20.32 \n",
      "2025-03-13 21:59:06,871: Train batch 1924: loss: 4.60 gradNorm: 21.81 \n",
      "2025-03-13 21:59:07,108: Train batch 1925: loss: 3.58 gradNorm: 18.80 \n",
      "2025-03-13 21:59:07,196: Train batch 1926: loss: 5.83 gradNorm: 20.44 \n",
      "2025-03-13 21:59:07,357: Train batch 1927: loss: 2.94 gradNorm: 18.77 \n",
      "2025-03-13 21:59:07,526: Train batch 1928: loss: 4.95 gradNorm: 23.50 \n",
      "2025-03-13 21:59:07,610: Train batch 1929: loss: 7.98 gradNorm: 28.14 \n",
      "2025-03-13 21:59:07,688: Train batch 1930: loss: 4.63 gradNorm: 20.95 \n",
      "2025-03-13 21:59:07,782: Train batch 1931: loss: 5.92 gradNorm: 20.39 \n",
      "2025-03-13 21:59:08,082: Train batch 1932: loss: 9.72 gradNorm: 50.93 \n",
      "2025-03-13 21:59:08,233: Train batch 1933: loss: 3.05 gradNorm: 17.81 \n",
      "2025-03-13 21:59:08,347: Train batch 1934: loss: 3.77 gradNorm: 18.42 \n",
      "2025-03-13 21:59:08,517: Train batch 1935: loss: 6.29 gradNorm: 29.12 \n",
      "2025-03-13 21:59:08,650: Train batch 1936: loss: 4.71 gradNorm: 21.43 \n",
      "2025-03-13 21:59:08,898: Train batch 1937: loss: 5.42 gradNorm: 24.72 \n",
      "2025-03-13 21:59:08,996: Train batch 1938: loss: 5.55 gradNorm: 21.52 \n",
      "2025-03-13 21:59:09,266: Train batch 1939: loss: 4.62 gradNorm: 21.32 \n",
      "2025-03-13 21:59:09,393: Train batch 1940: loss: 2.53 gradNorm: 16.42 \n",
      "2025-03-13 21:59:09,563: Train batch 1941: loss: 1.63 gradNorm: 11.35 \n",
      "2025-03-13 21:59:09,663: Train batch 1942: loss: 3.40 gradNorm: 17.57 \n",
      "2025-03-13 21:59:09,746: Train batch 1943: loss: 5.02 gradNorm: 22.17 \n",
      "2025-03-13 21:59:10,003: Train batch 1944: loss: 5.99 gradNorm: 29.35 \n",
      "2025-03-13 21:59:10,208: Train batch 1945: loss: 4.83 gradNorm: 24.08 \n",
      "2025-03-13 21:59:10,321: Train batch 1946: loss: 3.52 gradNorm: 15.04 \n",
      "2025-03-13 21:59:10,648: Train batch 1947: loss: 4.00 gradNorm: 21.18 \n",
      "2025-03-13 21:59:10,904: Train batch 1948: loss: 4.04 gradNorm: 22.94 \n",
      "2025-03-13 21:59:10,997: Train batch 1949: loss: 5.69 gradNorm: 20.66 \n",
      "2025-03-13 21:59:11,073: Train batch 1950: loss: 6.70 gradNorm: 27.43 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:59:11,273: Val batch: CER (t18.2025.01.15): 0.082\n",
      "2025-03-13 21:59:11,274: Val batch 1950: CER (avg): 0.082 \n",
      "2025-03-13 21:59:11,274: Batches since validation CER improved: 100\n",
      "2025-03-13 21:59:11,357: Train batch 1951: loss: 5.94 gradNorm: 24.28 \n",
      "2025-03-13 21:59:11,460: Train batch 1952: loss: 4.32 gradNorm: 17.95 \n",
      "2025-03-13 21:59:11,588: Train batch 1953: loss: 3.96 gradNorm: 17.99 \n",
      "2025-03-13 21:59:11,748: Train batch 1954: loss: 3.77 gradNorm: 25.20 \n",
      "2025-03-13 21:59:11,813: Train batch 1955: loss: 5.19 gradNorm: 24.06 \n",
      "2025-03-13 21:59:11,897: Train batch 1956: loss: 6.88 gradNorm: 27.79 \n",
      "2025-03-13 21:59:12,031: Train batch 1957: loss: 4.37 gradNorm: 20.34 \n",
      "2025-03-13 21:59:12,201: Train batch 1958: loss: 3.78 gradNorm: 18.28 \n",
      "2025-03-13 21:59:12,371: Train batch 1959: loss: 3.82 gradNorm: 20.94 \n",
      "2025-03-13 21:59:12,728: Train batch 1960: loss: 9.00 gradNorm: 37.94 \n",
      "2025-03-13 21:59:12,804: Train batch 1961: loss: 6.57 gradNorm: 28.80 \n",
      "2025-03-13 21:59:13,095: Train batch 1962: loss: 5.11 gradNorm: 28.42 \n",
      "2025-03-13 21:59:13,190: Train batch 1963: loss: 4.76 gradNorm: 22.42 \n",
      "2025-03-13 21:59:13,440: Train batch 1964: loss: 4.45 gradNorm: 20.61 \n",
      "2025-03-13 21:59:13,791: Train batch 1965: loss: 5.58 gradNorm: 21.72 \n",
      "2025-03-13 21:59:14,094: Train batch 1966: loss: 5.21 gradNorm: 25.18 \n",
      "2025-03-13 21:59:14,249: Train batch 1967: loss: 4.74 gradNorm: 22.91 \n",
      "2025-03-13 21:59:14,402: Train batch 1968: loss: 3.13 gradNorm: 20.70 \n",
      "2025-03-13 21:59:14,484: Train batch 1969: loss: 7.36 gradNorm: 27.13 \n",
      "2025-03-13 21:59:14,653: Train batch 1970: loss: 3.74 gradNorm: 19.05 \n",
      "2025-03-13 21:59:14,736: Train batch 1971: loss: 4.83 gradNorm: 19.63 \n",
      "2025-03-13 21:59:14,809: Train batch 1972: loss: 4.46 gradNorm: 19.69 \n",
      "2025-03-13 21:59:15,006: Train batch 1973: loss: 4.87 gradNorm: 22.18 \n",
      "2025-03-13 21:59:15,079: Train batch 1974: loss: 5.48 gradNorm: 22.55 \n",
      "2025-03-13 21:59:15,207: Train batch 1975: loss: 4.00 gradNorm: 21.57 \n",
      "2025-03-13 21:59:15,309: Train batch 1976: loss: 5.10 gradNorm: 19.87 \n",
      "2025-03-13 21:59:15,459: Train batch 1977: loss: 3.04 gradNorm: 17.05 \n",
      "2025-03-13 21:59:15,558: Train batch 1978: loss: 4.90 gradNorm: 20.05 \n",
      "2025-03-13 21:59:15,829: Train batch 1979: loss: 6.79 gradNorm: 34.96 \n",
      "2025-03-13 21:59:16,099: Train batch 1980: loss: 4.93 gradNorm: 25.25 \n",
      "2025-03-13 21:59:16,180: Train batch 1981: loss: 6.60 gradNorm: 26.85 \n",
      "2025-03-13 21:59:16,418: Train batch 1982: loss: 4.12 gradNorm: 21.89 \n",
      "2025-03-13 21:59:16,559: Train batch 1983: loss: 7.01 gradNorm: 30.13 \n",
      "2025-03-13 21:59:16,664: Train batch 1984: loss: 6.66 gradNorm: 26.39 \n",
      "2025-03-13 21:59:16,737: Train batch 1985: loss: 3.71 gradNorm: 17.22 \n",
      "2025-03-13 21:59:16,936: Train batch 1986: loss: 4.25 gradNorm: 19.42 \n",
      "2025-03-13 21:59:17,238: Train batch 1987: loss: 7.97 gradNorm: 37.33 \n",
      "2025-03-13 21:59:17,510: Train batch 1988: loss: 8.15 gradNorm: 40.80 \n",
      "2025-03-13 21:59:17,599: Train batch 1989: loss: 4.01 gradNorm: 19.89 \n",
      "2025-03-13 21:59:17,884: Train batch 1990: loss: 6.38 gradNorm: 31.12 \n",
      "2025-03-13 21:59:17,996: Train batch 1991: loss: 3.76 gradNorm: 16.05 \n",
      "2025-03-13 21:59:18,164: Train batch 1992: loss: 5.76 gradNorm: 27.63 \n",
      "2025-03-13 21:59:18,254: Train batch 1993: loss: 6.78 gradNorm: 31.67 \n",
      "2025-03-13 21:59:18,324: Train batch 1994: loss: 6.24 gradNorm: 24.93 \n",
      "2025-03-13 21:59:18,529: Train batch 1995: loss: 5.03 gradNorm: 21.37 \n",
      "2025-03-13 21:59:18,707: Train batch 1996: loss: 3.59 gradNorm: 18.22 \n",
      "2025-03-13 21:59:18,795: Train batch 1997: loss: 5.32 gradNorm: 22.88 \n",
      "2025-03-13 21:59:19,066: Train batch 1998: loss: 5.87 gradNorm: 30.18 \n",
      "2025-03-13 21:59:19,196: Train batch 1999: loss: 5.69 gradNorm: 26.06 \n",
      "2025-03-13 21:59:19,350: Train batch 2000: loss: 2.83 gradNorm: 17.52 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:59:19,546: Val batch: CER (t18.2025.01.15): 0.077\n",
      "2025-03-13 21:59:19,547: Val batch 2000: CER (avg): 0.077 \n",
      "2025-03-13 21:59:19,770: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:59:19,773: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:59:19,774: Batches since validation CER improved: 0\n",
      "2025-03-13 21:59:19,933: Train batch 2001: loss: 5.29 gradNorm: 27.42 \n",
      "2025-03-13 21:59:20,016: Train batch 2002: loss: 7.80 gradNorm: 29.51 \n",
      "2025-03-13 21:59:20,084: Train batch 2003: loss: 4.07 gradNorm: 19.18 \n",
      "2025-03-13 21:59:20,398: Train batch 2004: loss: 5.71 gradNorm: 26.33 \n",
      "2025-03-13 21:59:20,475: Train batch 2005: loss: 5.42 gradNorm: 19.75 \n",
      "2025-03-13 21:59:20,567: Train batch 2006: loss: 3.97 gradNorm: 18.39 \n",
      "2025-03-13 21:59:20,822: Train batch 2007: loss: 3.55 gradNorm: 18.46 \n",
      "2025-03-13 21:59:21,132: Train batch 2008: loss: 4.86 gradNorm: 22.13 \n",
      "2025-03-13 21:59:21,214: Train batch 2009: loss: 6.02 gradNorm: 23.82 \n",
      "2025-03-13 21:59:21,321: Train batch 2010: loss: 5.63 gradNorm: 22.63 \n",
      "2025-03-13 21:59:21,403: Train batch 2011: loss: 4.05 gradNorm: 18.72 \n",
      "2025-03-13 21:59:21,672: Train batch 2012: loss: 4.45 gradNorm: 21.49 \n",
      "2025-03-13 21:59:21,799: Train batch 2013: loss: 2.06 gradNorm: 12.15 \n",
      "2025-03-13 21:59:21,880: Train batch 2014: loss: 5.05 gradNorm: 19.96 \n",
      "2025-03-13 21:59:22,062: Train batch 2015: loss: 6.44 gradNorm: 28.90 \n",
      "2025-03-13 21:59:22,188: Train batch 2016: loss: 3.71 gradNorm: 17.16 \n",
      "2025-03-13 21:59:22,258: Train batch 2017: loss: 5.92 gradNorm: 22.56 \n",
      "2025-03-13 21:59:22,348: Train batch 2018: loss: 5.61 gradNorm: 25.18 \n",
      "2025-03-13 21:59:22,446: Train batch 2019: loss: 6.45 gradNorm: 24.45 \n",
      "2025-03-13 21:59:22,551: Train batch 2020: loss: 3.90 gradNorm: 18.74 \n",
      "2025-03-13 21:59:22,899: Train batch 2021: loss: 6.63 gradNorm: 35.01 \n",
      "2025-03-13 21:59:22,984: Train batch 2022: loss: 5.68 gradNorm: 23.70 \n",
      "2025-03-13 21:59:23,156: Train batch 2023: loss: 5.08 gradNorm: 26.31 \n",
      "2025-03-13 21:59:23,415: Train batch 2024: loss: 4.04 gradNorm: 20.56 \n",
      "2025-03-13 21:59:23,502: Train batch 2025: loss: 5.03 gradNorm: 24.17 \n",
      "2025-03-13 21:59:23,686: Train batch 2026: loss: 3.89 gradNorm: 20.72 \n",
      "2025-03-13 21:59:24,056: Train batch 2027: loss: 4.88 gradNorm: 20.76 \n",
      "2025-03-13 21:59:24,136: Train batch 2028: loss: 7.35 gradNorm: 29.38 \n",
      "2025-03-13 21:59:24,435: Train batch 2029: loss: 4.13 gradNorm: 20.98 \n",
      "2025-03-13 21:59:24,531: Train batch 2030: loss: 5.13 gradNorm: 23.11 \n",
      "2025-03-13 21:59:24,617: Train batch 2031: loss: 4.38 gradNorm: 18.53 \n",
      "2025-03-13 21:59:24,815: Train batch 2032: loss: 3.34 gradNorm: 16.92 \n",
      "2025-03-13 21:59:24,982: Train batch 2033: loss: 2.85 gradNorm: 17.78 \n",
      "2025-03-13 21:59:25,107: Train batch 2034: loss: 3.68 gradNorm: 18.73 \n",
      "2025-03-13 21:59:25,189: Train batch 2035: loss: 5.15 gradNorm: 23.82 \n",
      "2025-03-13 21:59:25,274: Train batch 2036: loss: 5.19 gradNorm: 18.90 \n",
      "2025-03-13 21:59:25,401: Train batch 2037: loss: 2.84 gradNorm: 14.88 \n",
      "2025-03-13 21:59:25,470: Train batch 2038: loss: 4.05 gradNorm: 20.15 \n",
      "2025-03-13 21:59:25,571: Train batch 2039: loss: 5.18 gradNorm: 22.98 \n",
      "2025-03-13 21:59:25,668: Train batch 2040: loss: 5.50 gradNorm: 19.16 \n",
      "2025-03-13 21:59:25,874: Train batch 2041: loss: 7.08 gradNorm: 36.47 \n",
      "2025-03-13 21:59:26,134: Train batch 2042: loss: 4.82 gradNorm: 23.00 \n",
      "2025-03-13 21:59:26,237: Train batch 2043: loss: 5.98 gradNorm: 24.04 \n",
      "2025-03-13 21:59:26,478: Train batch 2044: loss: 4.43 gradNorm: 24.93 \n",
      "2025-03-13 21:59:26,560: Train batch 2045: loss: 4.58 gradNorm: 22.38 \n",
      "2025-03-13 21:59:26,661: Train batch 2046: loss: 7.30 gradNorm: 26.32 \n",
      "2025-03-13 21:59:27,043: Train batch 2047: loss: 7.79 gradNorm: 38.10 \n",
      "2025-03-13 21:59:27,125: Train batch 2048: loss: 5.62 gradNorm: 21.07 \n",
      "2025-03-13 21:59:27,398: Train batch 2049: loss: 4.60 gradNorm: 22.11 \n",
      "2025-03-13 21:59:27,700: Train batch 2050: loss: 3.74 gradNorm: 20.20 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:59:27,890: Val batch: CER (t18.2025.01.15): 0.086\n",
      "2025-03-13 21:59:27,891: Val batch 2050: CER (avg): 0.086 \n",
      "2025-03-13 21:59:27,891: Batches since validation CER improved: 50\n",
      "2025-03-13 21:59:28,161: Train batch 2051: loss: 3.52 gradNorm: 14.75 \n",
      "2025-03-13 21:59:28,257: Train batch 2052: loss: 5.62 gradNorm: 23.10 \n",
      "2025-03-13 21:59:28,462: Train batch 2053: loss: 5.54 gradNorm: 22.89 \n",
      "2025-03-13 21:59:28,664: Train batch 2054: loss: 2.85 gradNorm: 16.55 \n",
      "2025-03-13 21:59:28,735: Train batch 2055: loss: 6.00 gradNorm: 27.62 \n",
      "2025-03-13 21:59:28,839: Train batch 2056: loss: 4.66 gradNorm: 18.14 \n",
      "2025-03-13 21:59:28,919: Train batch 2057: loss: 4.20 gradNorm: 16.68 \n",
      "2025-03-13 21:59:29,041: Train batch 2058: loss: 4.03 gradNorm: 18.64 \n",
      "2025-03-13 21:59:29,132: Train batch 2059: loss: 3.83 gradNorm: 19.43 \n",
      "2025-03-13 21:59:29,233: Train batch 2060: loss: 2.93 gradNorm: 15.12 \n",
      "2025-03-13 21:59:29,417: Train batch 2061: loss: 4.09 gradNorm: 21.18 \n",
      "2025-03-13 21:59:29,560: Train batch 2062: loss: 4.15 gradNorm: 20.84 \n",
      "2025-03-13 21:59:29,659: Train batch 2063: loss: 5.33 gradNorm: 24.43 \n",
      "2025-03-13 21:59:29,731: Train batch 2064: loss: 5.03 gradNorm: 22.47 \n",
      "2025-03-13 21:59:29,842: Train batch 2065: loss: 6.13 gradNorm: 24.60 \n",
      "2025-03-13 21:59:29,928: Train batch 2066: loss: 3.20 gradNorm: 16.21 \n",
      "2025-03-13 21:59:30,038: Train batch 2067: loss: 4.51 gradNorm: 18.14 \n",
      "2025-03-13 21:59:30,150: Train batch 2068: loss: 3.24 gradNorm: 16.19 \n",
      "2025-03-13 21:59:30,233: Train batch 2069: loss: 5.78 gradNorm: 28.87 \n",
      "2025-03-13 21:59:30,341: Train batch 2070: loss: 2.99 gradNorm: 18.05 \n",
      "2025-03-13 21:59:30,730: Train batch 2071: loss: 5.41 gradNorm: 23.11 \n",
      "2025-03-13 21:59:30,813: Train batch 2072: loss: 6.05 gradNorm: 27.17 \n",
      "2025-03-13 21:59:30,969: Train batch 2073: loss: 3.39 gradNorm: 17.54 \n",
      "2025-03-13 21:59:31,048: Train batch 2074: loss: 4.57 gradNorm: 20.25 \n",
      "2025-03-13 21:59:31,144: Train batch 2075: loss: 5.06 gradNorm: 21.78 \n",
      "2025-03-13 21:59:31,247: Train batch 2076: loss: 2.16 gradNorm: 14.73 \n",
      "2025-03-13 21:59:31,336: Train batch 2077: loss: 4.83 gradNorm: 20.88 \n",
      "2025-03-13 21:59:31,417: Train batch 2078: loss: 5.92 gradNorm: 24.91 \n",
      "2025-03-13 21:59:31,485: Train batch 2079: loss: 4.71 gradNorm: 20.61 \n",
      "2025-03-13 21:59:31,708: Train batch 2080: loss: 5.43 gradNorm: 26.51 \n",
      "2025-03-13 21:59:32,134: Train batch 2081: loss: 4.29 gradNorm: 20.06 \n",
      "2025-03-13 21:59:32,208: Train batch 2082: loss: 5.09 gradNorm: 22.39 \n",
      "2025-03-13 21:59:32,326: Train batch 2083: loss: 3.82 gradNorm: 17.48 \n",
      "2025-03-13 21:59:32,571: Train batch 2084: loss: 5.44 gradNorm: 27.63 \n",
      "2025-03-13 21:59:32,665: Train batch 2085: loss: 7.43 gradNorm: 29.66 \n",
      "2025-03-13 21:59:32,767: Train batch 2086: loss: 3.22 gradNorm: 15.10 \n",
      "2025-03-13 21:59:33,042: Train batch 2087: loss: 7.31 gradNorm: 31.41 \n",
      "2025-03-13 21:59:33,275: Train batch 2088: loss: 7.80 gradNorm: 36.29 \n",
      "2025-03-13 21:59:33,356: Train batch 2089: loss: 4.07 gradNorm: 18.51 \n",
      "2025-03-13 21:59:33,524: Train batch 2090: loss: 5.31 gradNorm: 22.64 \n",
      "2025-03-13 21:59:33,708: Train batch 2091: loss: 2.61 gradNorm: 16.36 \n",
      "2025-03-13 21:59:33,834: Train batch 2092: loss: 5.33 gradNorm: 25.12 \n",
      "2025-03-13 21:59:33,935: Train batch 2093: loss: 5.90 gradNorm: 25.46 \n",
      "2025-03-13 21:59:34,035: Train batch 2094: loss: 4.25 gradNorm: 19.81 \n",
      "2025-03-13 21:59:34,137: Train batch 2095: loss: 4.03 gradNorm: 20.27 \n",
      "2025-03-13 21:59:34,207: Train batch 2096: loss: 4.11 gradNorm: 20.65 \n",
      "2025-03-13 21:59:34,294: Train batch 2097: loss: 3.51 gradNorm: 18.52 \n",
      "2025-03-13 21:59:34,374: Train batch 2098: loss: 5.02 gradNorm: 18.46 \n",
      "2025-03-13 21:59:34,473: Train batch 2099: loss: 4.67 gradNorm: 20.26 \n",
      "2025-03-13 21:59:34,571: Train batch 2100: loss: 5.25 gradNorm: 20.02 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:59:34,769: Val batch: CER (t18.2025.01.15): 0.075\n",
      "2025-03-13 21:59:34,770: Val batch 2100: CER (avg): 0.075 \n",
      "2025-03-13 21:59:34,991: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:59:34,994: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:59:34,995: Batches since validation CER improved: 0\n",
      "2025-03-13 21:59:35,156: Train batch 2101: loss: 4.26 gradNorm: 21.37 \n",
      "2025-03-13 21:59:35,337: Train batch 2102: loss: 4.65 gradNorm: 22.88 \n",
      "2025-03-13 21:59:35,422: Train batch 2103: loss: 4.16 gradNorm: 19.85 \n",
      "2025-03-13 21:59:35,507: Train batch 2104: loss: 4.65 gradNorm: 19.76 \n",
      "2025-03-13 21:59:35,576: Train batch 2105: loss: 3.62 gradNorm: 18.17 \n",
      "2025-03-13 21:59:35,658: Train batch 2106: loss: 3.49 gradNorm: 16.45 \n",
      "2025-03-13 21:59:35,761: Train batch 2107: loss: 5.14 gradNorm: 23.44 \n",
      "2025-03-13 21:59:35,940: Train batch 2108: loss: 4.85 gradNorm: 26.17 \n",
      "2025-03-13 21:59:36,214: Train batch 2109: loss: 6.28 gradNorm: 31.00 \n",
      "2025-03-13 21:59:36,312: Train batch 2110: loss: 6.73 gradNorm: 32.28 \n",
      "2025-03-13 21:59:36,494: Train batch 2111: loss: 4.10 gradNorm: 19.39 \n",
      "2025-03-13 21:59:36,817: Train batch 2112: loss: 4.82 gradNorm: 25.66 \n",
      "2025-03-13 21:59:36,952: Train batch 2113: loss: 3.24 gradNorm: 17.35 \n",
      "2025-03-13 21:59:37,163: Train batch 2114: loss: 2.87 gradNorm: 20.37 \n",
      "2025-03-13 21:59:37,521: Train batch 2115: loss: 4.74 gradNorm: 22.38 \n",
      "2025-03-13 21:59:37,590: Train batch 2116: loss: 4.92 gradNorm: 22.88 \n",
      "2025-03-13 21:59:37,672: Train batch 2117: loss: 6.11 gradNorm: 33.99 \n",
      "2025-03-13 21:59:37,764: Train batch 2118: loss: 6.05 gradNorm: 25.12 \n",
      "2025-03-13 21:59:37,856: Train batch 2119: loss: 3.75 gradNorm: 16.37 \n",
      "2025-03-13 21:59:38,126: Train batch 2120: loss: 5.23 gradNorm: 29.64 \n",
      "2025-03-13 21:59:38,228: Train batch 2121: loss: 3.87 gradNorm: 16.13 \n",
      "2025-03-13 21:59:38,297: Train batch 2122: loss: 4.34 gradNorm: 20.94 \n",
      "2025-03-13 21:59:38,692: Train batch 2123: loss: 5.35 gradNorm: 27.23 \n",
      "2025-03-13 21:59:38,791: Train batch 2124: loss: 4.46 gradNorm: 20.12 \n",
      "2025-03-13 21:59:38,940: Train batch 2125: loss: 3.65 gradNorm: 23.34 \n",
      "2025-03-13 21:59:39,146: Train batch 2126: loss: 4.87 gradNorm: 22.52 \n",
      "2025-03-13 21:59:39,246: Train batch 2127: loss: 5.32 gradNorm: 23.14 \n",
      "2025-03-13 21:59:39,429: Train batch 2128: loss: 4.50 gradNorm: 23.16 \n",
      "2025-03-13 21:59:39,594: Train batch 2129: loss: 4.06 gradNorm: 23.30 \n",
      "2025-03-13 21:59:39,844: Train batch 2130: loss: 6.28 gradNorm: 30.34 \n",
      "2025-03-13 21:59:40,038: Train batch 2131: loss: 2.41 gradNorm: 15.88 \n",
      "2025-03-13 21:59:40,125: Train batch 2132: loss: 5.48 gradNorm: 22.96 \n",
      "2025-03-13 21:59:40,284: Train batch 2133: loss: 3.27 gradNorm: 21.50 \n",
      "2025-03-13 21:59:40,587: Train batch 2134: loss: 3.68 gradNorm: 21.02 \n",
      "2025-03-13 21:59:40,690: Train batch 2135: loss: 3.57 gradNorm: 17.01 \n",
      "2025-03-13 21:59:40,802: Train batch 2136: loss: 3.80 gradNorm: 19.03 \n",
      "2025-03-13 21:59:40,889: Train batch 2137: loss: 3.68 gradNorm: 18.45 \n",
      "2025-03-13 21:59:41,057: Train batch 2138: loss: 1.99 gradNorm: 13.50 \n",
      "2025-03-13 21:59:41,229: Train batch 2139: loss: 7.13 gradNorm: 32.64 \n",
      "2025-03-13 21:59:41,381: Train batch 2140: loss: 2.35 gradNorm: 14.40 \n",
      "2025-03-13 21:59:41,512: Train batch 2141: loss: 1.73 gradNorm: 11.60 \n",
      "2025-03-13 21:59:41,593: Train batch 2142: loss: 5.15 gradNorm: 23.38 \n",
      "2025-03-13 21:59:41,673: Train batch 2143: loss: 4.63 gradNorm: 22.79 \n",
      "2025-03-13 21:59:41,777: Train batch 2144: loss: 3.72 gradNorm: 19.37 \n",
      "2025-03-13 21:59:41,887: Train batch 2145: loss: 5.13 gradNorm: 21.28 \n",
      "2025-03-13 21:59:41,960: Train batch 2146: loss: 4.36 gradNorm: 21.01 \n",
      "2025-03-13 21:59:42,055: Train batch 2147: loss: 5.27 gradNorm: 21.68 \n",
      "2025-03-13 21:59:42,153: Train batch 2148: loss: 4.26 gradNorm: 18.21 \n",
      "2025-03-13 21:59:42,451: Train batch 2149: loss: 5.42 gradNorm: 33.46 \n",
      "2025-03-13 21:59:42,564: Train batch 2150: loss: 2.99 gradNorm: 13.78 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:59:42,786: Val batch: CER (t18.2025.01.15): 0.083\n",
      "2025-03-13 21:59:42,788: Val batch 2150: CER (avg): 0.083 \n",
      "2025-03-13 21:59:42,788: Batches since validation CER improved: 50\n",
      "2025-03-13 21:59:43,146: Train batch 2151: loss: 6.10 gradNorm: 27.32 \n",
      "2025-03-13 21:59:43,216: Train batch 2152: loss: 4.49 gradNorm: 22.93 \n",
      "2025-03-13 21:59:43,307: Train batch 2153: loss: 3.37 gradNorm: 17.33 \n",
      "2025-03-13 21:59:43,419: Train batch 2154: loss: 3.17 gradNorm: 15.22 \n",
      "2025-03-13 21:59:43,502: Train batch 2155: loss: 4.45 gradNorm: 21.06 \n",
      "2025-03-13 21:59:43,795: Train batch 2156: loss: 5.00 gradNorm: 28.81 \n",
      "2025-03-13 21:59:44,069: Train batch 2157: loss: 9.52 gradNorm: 53.41 \n",
      "2025-03-13 21:59:44,237: Train batch 2158: loss: 2.68 gradNorm: 15.94 \n",
      "2025-03-13 21:59:44,346: Train batch 2159: loss: 2.84 gradNorm: 14.72 \n",
      "2025-03-13 21:59:44,427: Train batch 2160: loss: 3.92 gradNorm: 17.78 \n",
      "2025-03-13 21:59:44,621: Train batch 2161: loss: 4.36 gradNorm: 20.76 \n",
      "2025-03-13 21:59:44,701: Train batch 2162: loss: 5.11 gradNorm: 24.14 \n",
      "2025-03-13 21:59:44,853: Train batch 2163: loss: 2.54 gradNorm: 17.65 \n",
      "2025-03-13 21:59:45,052: Train batch 2164: loss: 6.69 gradNorm: 34.73 \n",
      "2025-03-13 21:59:45,383: Train batch 2165: loss: 6.47 gradNorm: 34.18 \n",
      "2025-03-13 21:59:45,482: Train batch 2166: loss: 4.66 gradNorm: 19.36 \n",
      "2025-03-13 21:59:45,758: Train batch 2167: loss: 4.02 gradNorm: 21.66 \n",
      "2025-03-13 21:59:45,907: Train batch 2168: loss: 2.22 gradNorm: 15.88 \n",
      "2025-03-13 21:59:46,039: Train batch 2169: loss: 4.16 gradNorm: 23.35 \n",
      "2025-03-13 21:59:46,222: Train batch 2170: loss: 2.33 gradNorm: 14.48 \n",
      "2025-03-13 21:59:46,381: Train batch 2171: loss: 1.72 gradNorm: 11.80 \n",
      "2025-03-13 21:59:46,687: Train batch 2172: loss: 5.82 gradNorm: 28.95 \n",
      "2025-03-13 21:59:47,041: Train batch 2173: loss: 4.67 gradNorm: 26.22 \n",
      "2025-03-13 21:59:47,139: Train batch 2174: loss: 3.71 gradNorm: 20.70 \n",
      "2025-03-13 21:59:47,230: Train batch 2175: loss: 4.31 gradNorm: 21.91 \n",
      "2025-03-13 21:59:47,361: Train batch 2176: loss: 3.12 gradNorm: 16.23 \n",
      "2025-03-13 21:59:47,747: Train batch 2177: loss: 5.66 gradNorm: 28.12 \n",
      "2025-03-13 21:59:47,874: Train batch 2178: loss: 3.50 gradNorm: 18.04 \n",
      "2025-03-13 21:59:47,971: Train batch 2179: loss: 3.43 gradNorm: 15.75 \n",
      "2025-03-13 21:59:48,099: Train batch 2180: loss: 2.99 gradNorm: 17.34 \n",
      "2025-03-13 21:59:48,365: Train batch 2181: loss: 4.19 gradNorm: 22.42 \n",
      "2025-03-13 21:59:48,545: Train batch 2182: loss: 1.45 gradNorm: 10.28 \n",
      "2025-03-13 21:59:48,644: Train batch 2183: loss: 5.41 gradNorm: 21.70 \n",
      "2025-03-13 21:59:48,829: Train batch 2184: loss: 3.71 gradNorm: 20.13 \n",
      "2025-03-13 21:59:48,904: Train batch 2185: loss: 5.22 gradNorm: 22.63 \n",
      "2025-03-13 21:59:49,058: Train batch 2186: loss: 1.57 gradNorm: 13.26 \n",
      "2025-03-13 21:59:49,302: Train batch 2187: loss: 4.23 gradNorm: 24.74 \n",
      "2025-03-13 21:59:49,411: Train batch 2188: loss: 4.44 gradNorm: 19.84 \n",
      "2025-03-13 21:59:49,493: Train batch 2189: loss: 4.89 gradNorm: 24.07 \n",
      "2025-03-13 21:59:49,581: Train batch 2190: loss: 3.22 gradNorm: 17.08 \n",
      "2025-03-13 21:59:49,854: Train batch 2191: loss: 4.15 gradNorm: 20.56 \n",
      "2025-03-13 21:59:49,957: Train batch 2192: loss: 2.43 gradNorm: 14.61 \n",
      "2025-03-13 21:59:50,357: Train batch 2193: loss: 4.97 gradNorm: 27.31 \n",
      "2025-03-13 21:59:50,510: Train batch 2194: loss: 1.60 gradNorm: 12.29 \n",
      "2025-03-13 21:59:50,607: Train batch 2195: loss: 4.88 gradNorm: 20.35 \n",
      "2025-03-13 21:59:50,780: Train batch 2196: loss: 3.34 gradNorm: 20.84 \n",
      "2025-03-13 21:59:51,131: Train batch 2197: loss: 3.50 gradNorm: 19.42 \n",
      "2025-03-13 21:59:51,232: Train batch 2198: loss: 4.42 gradNorm: 18.83 \n",
      "2025-03-13 21:59:51,417: Train batch 2199: loss: 2.76 gradNorm: 16.39 \n",
      "2025-03-13 21:59:51,486: Train batch 2200: loss: 3.91 gradNorm: 17.58 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:59:51,690: Val batch: CER (t18.2025.01.15): 0.080\n",
      "2025-03-13 21:59:51,691: Val batch 2200: CER (avg): 0.080 \n",
      "2025-03-13 21:59:51,691: Batches since validation CER improved: 100\n",
      "2025-03-13 21:59:51,821: Train batch 2201: loss: 5.38 gradNorm: 23.40 \n",
      "2025-03-13 21:59:51,899: Train batch 2202: loss: 5.05 gradNorm: 20.42 \n",
      "2025-03-13 21:59:52,068: Train batch 2203: loss: 4.61 gradNorm: 22.94 \n",
      "2025-03-13 21:59:52,244: Train batch 2204: loss: 3.96 gradNorm: 20.22 \n",
      "2025-03-13 21:59:52,440: Train batch 2205: loss: 2.96 gradNorm: 15.35 \n",
      "2025-03-13 21:59:52,584: Train batch 2206: loss: 3.36 gradNorm: 19.91 \n",
      "2025-03-13 21:59:52,778: Train batch 2207: loss: 1.69 gradNorm: 10.26 \n",
      "2025-03-13 21:59:52,905: Train batch 2208: loss: 3.33 gradNorm: 21.44 \n",
      "2025-03-13 21:59:52,980: Train batch 2209: loss: 4.44 gradNorm: 21.69 \n",
      "2025-03-13 21:59:53,054: Train batch 2210: loss: 5.31 gradNorm: 25.44 \n",
      "2025-03-13 21:59:53,248: Train batch 2211: loss: 3.57 gradNorm: 19.81 \n",
      "2025-03-13 21:59:53,346: Train batch 2212: loss: 2.68 gradNorm: 18.57 \n",
      "2025-03-13 21:59:53,446: Train batch 2213: loss: 3.70 gradNorm: 19.61 \n",
      "2025-03-13 21:59:53,527: Train batch 2214: loss: 4.12 gradNorm: 22.40 \n",
      "2025-03-13 21:59:53,631: Train batch 2215: loss: 2.47 gradNorm: 12.76 \n",
      "2025-03-13 21:59:53,759: Train batch 2216: loss: 2.15 gradNorm: 15.13 \n",
      "2025-03-13 21:59:53,984: Train batch 2217: loss: 6.02 gradNorm: 30.13 \n",
      "2025-03-13 21:59:54,068: Train batch 2218: loss: 4.65 gradNorm: 25.53 \n",
      "2025-03-13 21:59:54,247: Train batch 2219: loss: 2.19 gradNorm: 15.49 \n",
      "2025-03-13 21:59:54,352: Train batch 2220: loss: 4.03 gradNorm: 17.87 \n",
      "2025-03-13 21:59:54,628: Train batch 2221: loss: 2.82 gradNorm: 16.26 \n",
      "2025-03-13 21:59:54,737: Train batch 2222: loss: 4.45 gradNorm: 20.59 \n",
      "2025-03-13 21:59:54,834: Train batch 2223: loss: 4.65 gradNorm: 18.55 \n",
      "2025-03-13 21:59:55,005: Train batch 2224: loss: 4.07 gradNorm: 21.83 \n",
      "2025-03-13 21:59:55,145: Train batch 2225: loss: 2.91 gradNorm: 15.74 \n",
      "2025-03-13 21:59:55,244: Train batch 2226: loss: 3.82 gradNorm: 18.52 \n",
      "2025-03-13 21:59:55,329: Train batch 2227: loss: 5.28 gradNorm: 22.94 \n",
      "2025-03-13 21:59:55,656: Train batch 2228: loss: 5.58 gradNorm: 29.20 \n",
      "2025-03-13 21:59:55,730: Train batch 2229: loss: 4.75 gradNorm: 20.67 \n",
      "2025-03-13 21:59:55,805: Train batch 2230: loss: 3.89 gradNorm: 22.02 \n",
      "2025-03-13 21:59:55,884: Train batch 2231: loss: 3.53 gradNorm: 17.95 \n",
      "2025-03-13 21:59:56,054: Train batch 2232: loss: 1.82 gradNorm: 15.96 \n",
      "2025-03-13 21:59:56,151: Train batch 2233: loss: 4.99 gradNorm: 21.03 \n",
      "2025-03-13 21:59:56,497: Train batch 2234: loss: 5.89 gradNorm: 30.29 \n",
      "2025-03-13 21:59:56,886: Train batch 2235: loss: 5.38 gradNorm: 23.92 \n",
      "2025-03-13 21:59:57,165: Train batch 2236: loss: 4.45 gradNorm: 22.20 \n",
      "2025-03-13 21:59:57,242: Train batch 2237: loss: 3.61 gradNorm: 21.22 \n",
      "2025-03-13 21:59:57,330: Train batch 2238: loss: 4.71 gradNorm: 24.77 \n",
      "2025-03-13 21:59:57,404: Train batch 2239: loss: 4.33 gradNorm: 19.99 \n",
      "2025-03-13 21:59:57,556: Train batch 2240: loss: 2.05 gradNorm: 14.42 \n",
      "2025-03-13 21:59:57,638: Train batch 2241: loss: 4.10 gradNorm: 21.40 \n",
      "2025-03-13 21:59:57,718: Train batch 2242: loss: 3.82 gradNorm: 19.98 \n",
      "2025-03-13 21:59:57,909: Train batch 2243: loss: 3.98 gradNorm: 18.98 \n",
      "2025-03-13 21:59:58,066: Train batch 2244: loss: 2.40 gradNorm: 14.84 \n",
      "2025-03-13 21:59:58,339: Train batch 2245: loss: 4.38 gradNorm: 24.92 \n",
      "2025-03-13 21:59:58,416: Train batch 2246: loss: 4.53 gradNorm: 21.30 \n",
      "2025-03-13 21:59:58,651: Train batch 2247: loss: 3.63 gradNorm: 18.98 \n",
      "2025-03-13 21:59:58,722: Train batch 2248: loss: 4.80 gradNorm: 23.70 \n",
      "2025-03-13 21:59:58,841: Train batch 2249: loss: 5.52 gradNorm: 20.52 \n",
      "2025-03-13 21:59:58,922: Train batch 2250: loss: 4.36 gradNorm: 23.07 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 21:59:59,122: Val batch: CER (t18.2025.01.15): 0.074\n",
      "2025-03-13 21:59:59,123: Val batch 2250: CER (avg): 0.074 \n",
      "2025-03-13 21:59:59,341: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:59:59,345: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 21:59:59,345: Batches since validation CER improved: 0\n",
      "2025-03-13 21:59:59,427: Train batch 2251: loss: 3.66 gradNorm: 21.43 \n",
      "2025-03-13 21:59:59,542: Train batch 2252: loss: 3.40 gradNorm: 16.63 \n",
      "2025-03-13 21:59:59,701: Train batch 2253: loss: 3.88 gradNorm: 21.63 \n",
      "2025-03-13 22:00:00,010: Train batch 2254: loss: 5.59 gradNorm: 34.66 \n",
      "2025-03-13 22:00:00,207: Train batch 2255: loss: 3.85 gradNorm: 21.93 \n",
      "2025-03-13 22:00:00,293: Train batch 2256: loss: 2.53 gradNorm: 15.98 \n",
      "2025-03-13 22:00:00,375: Train batch 2257: loss: 3.63 gradNorm: 19.16 \n",
      "2025-03-13 22:00:00,496: Train batch 2258: loss: 2.28 gradNorm: 13.02 \n",
      "2025-03-13 22:00:00,584: Train batch 2259: loss: 2.58 gradNorm: 14.69 \n",
      "2025-03-13 22:00:00,958: Train batch 2260: loss: 5.48 gradNorm: 31.12 \n",
      "2025-03-13 22:00:01,085: Train batch 2261: loss: 4.08 gradNorm: 23.16 \n",
      "2025-03-13 22:00:01,167: Train batch 2262: loss: 2.91 gradNorm: 14.42 \n",
      "2025-03-13 22:00:01,521: Train batch 2263: loss: 3.50 gradNorm: 18.73 \n",
      "2025-03-13 22:00:01,610: Train batch 2264: loss: 2.86 gradNorm: 14.79 \n",
      "2025-03-13 22:00:01,710: Train batch 2265: loss: 2.19 gradNorm: 15.22 \n",
      "2025-03-13 22:00:01,914: Train batch 2266: loss: 3.41 gradNorm: 19.29 \n",
      "2025-03-13 22:00:02,110: Train batch 2267: loss: 1.69 gradNorm: 12.46 \n",
      "2025-03-13 22:00:02,190: Train batch 2268: loss: 4.88 gradNorm: 20.16 \n",
      "2025-03-13 22:00:02,321: Train batch 2269: loss: 4.00 gradNorm: 17.28 \n",
      "2025-03-13 22:00:02,619: Train batch 2270: loss: 3.32 gradNorm: 19.07 \n",
      "2025-03-13 22:00:02,717: Train batch 2271: loss: 2.97 gradNorm: 15.27 \n",
      "2025-03-13 22:00:02,796: Train batch 2272: loss: 3.14 gradNorm: 18.04 \n",
      "2025-03-13 22:00:02,884: Train batch 2273: loss: 3.38 gradNorm: 18.72 \n",
      "2025-03-13 22:00:02,966: Train batch 2274: loss: 3.50 gradNorm: 19.60 \n",
      "2025-03-13 22:00:03,271: Train batch 2275: loss: 5.69 gradNorm: 32.69 \n",
      "2025-03-13 22:00:03,429: Train batch 2276: loss: 1.91 gradNorm: 15.25 \n",
      "2025-03-13 22:00:03,510: Train batch 2277: loss: 3.94 gradNorm: 18.02 \n",
      "2025-03-13 22:00:03,637: Train batch 2278: loss: 3.14 gradNorm: 16.75 \n",
      "2025-03-13 22:00:03,713: Train batch 2279: loss: 3.40 gradNorm: 18.95 \n",
      "2025-03-13 22:00:04,000: Train batch 2280: loss: 3.79 gradNorm: 19.21 \n",
      "2025-03-13 22:00:04,094: Train batch 2281: loss: 4.79 gradNorm: 22.11 \n",
      "2025-03-13 22:00:04,172: Train batch 2282: loss: 2.70 gradNorm: 15.96 \n",
      "2025-03-13 22:00:04,344: Train batch 2283: loss: 5.72 gradNorm: 27.68 \n",
      "2025-03-13 22:00:04,432: Train batch 2284: loss: 3.49 gradNorm: 18.17 \n",
      "2025-03-13 22:00:04,565: Train batch 2285: loss: 3.51 gradNorm: 18.17 \n",
      "2025-03-13 22:00:04,747: Train batch 2286: loss: 2.87 gradNorm: 21.13 \n",
      "2025-03-13 22:00:04,843: Train batch 2287: loss: 4.29 gradNorm: 16.75 \n",
      "2025-03-13 22:00:04,917: Train batch 2288: loss: 4.07 gradNorm: 21.46 \n",
      "2025-03-13 22:00:04,997: Train batch 2289: loss: 4.09 gradNorm: 19.05 \n",
      "2025-03-13 22:00:05,100: Train batch 2290: loss: 3.61 gradNorm: 18.00 \n",
      "2025-03-13 22:00:05,278: Train batch 2291: loss: 3.52 gradNorm: 19.08 \n",
      "2025-03-13 22:00:05,361: Train batch 2292: loss: 4.90 gradNorm: 25.76 \n",
      "2025-03-13 22:00:05,446: Train batch 2293: loss: 3.22 gradNorm: 16.67 \n",
      "2025-03-13 22:00:05,639: Train batch 2294: loss: 3.46 gradNorm: 22.76 \n",
      "2025-03-13 22:00:05,908: Train batch 2295: loss: 5.56 gradNorm: 26.56 \n",
      "2025-03-13 22:00:06,256: Train batch 2296: loss: 4.62 gradNorm: 23.29 \n",
      "2025-03-13 22:00:06,356: Train batch 2297: loss: 3.19 gradNorm: 17.53 \n",
      "2025-03-13 22:00:06,499: Train batch 2298: loss: 2.89 gradNorm: 15.54 \n",
      "2025-03-13 22:00:06,590: Train batch 2299: loss: 3.78 gradNorm: 20.47 \n",
      "2025-03-13 22:00:06,677: Train batch 2300: loss: 3.17 gradNorm: 18.98 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:00:06,887: Val batch: CER (t18.2025.01.15): 0.084\n",
      "2025-03-13 22:00:06,888: Val batch 2300: CER (avg): 0.084 \n",
      "2025-03-13 22:00:06,888: Batches since validation CER improved: 50\n",
      "2025-03-13 22:00:06,968: Train batch 2301: loss: 4.39 gradNorm: 21.15 \n",
      "2025-03-13 22:00:07,345: Train batch 2302: loss: 4.40 gradNorm: 22.87 \n",
      "2025-03-13 22:00:07,457: Train batch 2303: loss: 3.00 gradNorm: 16.76 \n",
      "2025-03-13 22:00:07,681: Train batch 2304: loss: 3.57 gradNorm: 20.37 \n",
      "2025-03-13 22:00:07,810: Train batch 2305: loss: 3.93 gradNorm: 18.41 \n",
      "2025-03-13 22:00:08,121: Train batch 2306: loss: 3.26 gradNorm: 20.74 \n",
      "2025-03-13 22:00:08,220: Train batch 2307: loss: 3.94 gradNorm: 18.82 \n",
      "2025-03-13 22:00:08,472: Train batch 2308: loss: 3.40 gradNorm: 19.97 \n",
      "2025-03-13 22:00:08,573: Train batch 2309: loss: 2.67 gradNorm: 18.31 \n",
      "2025-03-13 22:00:08,671: Train batch 2310: loss: 4.24 gradNorm: 20.37 \n",
      "2025-03-13 22:00:08,741: Train batch 2311: loss: 3.91 gradNorm: 22.49 \n",
      "2025-03-13 22:00:08,816: Train batch 2312: loss: 3.27 gradNorm: 19.56 \n",
      "2025-03-13 22:00:08,898: Train batch 2313: loss: 4.27 gradNorm: 19.27 \n",
      "2025-03-13 22:00:09,186: Train batch 2314: loss: 4.19 gradNorm: 22.32 \n",
      "2025-03-13 22:00:09,274: Train batch 2315: loss: 4.66 gradNorm: 23.43 \n",
      "2025-03-13 22:00:09,344: Train batch 2316: loss: 2.90 gradNorm: 15.30 \n",
      "2025-03-13 22:00:09,473: Train batch 2317: loss: 2.55 gradNorm: 14.70 \n",
      "2025-03-13 22:00:09,727: Train batch 2318: loss: 4.07 gradNorm: 24.94 \n",
      "2025-03-13 22:00:09,882: Train batch 2319: loss: 2.87 gradNorm: 18.10 \n",
      "2025-03-13 22:00:10,184: Train batch 2320: loss: 3.10 gradNorm: 18.81 \n",
      "2025-03-13 22:00:10,475: Train batch 2321: loss: 5.80 gradNorm: 30.51 \n",
      "2025-03-13 22:00:10,545: Train batch 2322: loss: 3.76 gradNorm: 20.11 \n",
      "2025-03-13 22:00:10,733: Train batch 2323: loss: 3.13 gradNorm: 22.13 \n",
      "2025-03-13 22:00:10,868: Train batch 2324: loss: 3.03 gradNorm: 16.75 \n",
      "2025-03-13 22:00:10,966: Train batch 2325: loss: 2.85 gradNorm: 18.80 \n",
      "2025-03-13 22:00:11,158: Train batch 2326: loss: 3.57 gradNorm: 22.81 \n",
      "2025-03-13 22:00:11,432: Train batch 2327: loss: 5.47 gradNorm: 31.72 \n",
      "2025-03-13 22:00:11,601: Train batch 2328: loss: 1.83 gradNorm: 13.01 \n",
      "2025-03-13 22:00:11,954: Train batch 2329: loss: 3.77 gradNorm: 22.54 \n",
      "2025-03-13 22:00:12,040: Train batch 2330: loss: 3.17 gradNorm: 18.87 \n",
      "2025-03-13 22:00:12,181: Train batch 2331: loss: 3.50 gradNorm: 20.16 \n",
      "2025-03-13 22:00:12,306: Train batch 2332: loss: 4.87 gradNorm: 22.05 \n",
      "2025-03-13 22:00:12,434: Train batch 2333: loss: 1.52 gradNorm: 14.05 \n",
      "2025-03-13 22:00:12,601: Train batch 2334: loss: 2.31 gradNorm: 14.53 \n",
      "2025-03-13 22:00:12,772: Train batch 2335: loss: 2.18 gradNorm: 15.15 \n",
      "2025-03-13 22:00:12,935: Train batch 2336: loss: 1.80 gradNorm: 14.32 \n",
      "2025-03-13 22:00:13,032: Train batch 2337: loss: 3.44 gradNorm: 18.14 \n",
      "2025-03-13 22:00:13,216: Train batch 2338: loss: 2.39 gradNorm: 15.27 \n",
      "2025-03-13 22:00:13,490: Train batch 2339: loss: 4.41 gradNorm: 31.47 \n",
      "2025-03-13 22:00:13,638: Train batch 2340: loss: 2.12 gradNorm: 13.62 \n",
      "2025-03-13 22:00:13,929: Train batch 2341: loss: 3.13 gradNorm: 19.22 \n",
      "2025-03-13 22:00:14,087: Train batch 2342: loss: 1.74 gradNorm: 11.94 \n",
      "2025-03-13 22:00:14,166: Train batch 2343: loss: 3.70 gradNorm: 17.81 \n",
      "2025-03-13 22:00:14,294: Train batch 2344: loss: 3.18 gradNorm: 19.01 \n",
      "2025-03-13 22:00:14,360: Train batch 2345: loss: 3.50 gradNorm: 22.55 \n",
      "2025-03-13 22:00:14,536: Train batch 2346: loss: 3.92 gradNorm: 23.51 \n",
      "2025-03-13 22:00:14,630: Train batch 2347: loss: 4.59 gradNorm: 22.08 \n",
      "2025-03-13 22:00:14,734: Train batch 2348: loss: 3.26 gradNorm: 17.92 \n",
      "2025-03-13 22:00:15,101: Train batch 2349: loss: 5.05 gradNorm: 26.87 \n",
      "2025-03-13 22:00:15,227: Train batch 2350: loss: 2.46 gradNorm: 17.50 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:00:15,434: Val batch: CER (t18.2025.01.15): 0.088\n",
      "2025-03-13 22:00:15,436: Val batch 2350: CER (avg): 0.088 \n",
      "2025-03-13 22:00:15,436: Batches since validation CER improved: 100\n",
      "2025-03-13 22:00:15,630: Train batch 2351: loss: 2.43 gradNorm: 17.19 \n",
      "2025-03-13 22:00:15,801: Train batch 2352: loss: 1.43 gradNorm: 10.00 \n",
      "2025-03-13 22:00:16,075: Train batch 2353: loss: 6.25 gradNorm: 32.39 \n",
      "2025-03-13 22:00:16,182: Train batch 2354: loss: 3.02 gradNorm: 19.09 \n",
      "2025-03-13 22:00:16,262: Train batch 2355: loss: 3.61 gradNorm: 20.16 \n",
      "2025-03-13 22:00:16,648: Train batch 2356: loss: 5.42 gradNorm: 27.07 \n",
      "2025-03-13 22:00:16,852: Train batch 2357: loss: 2.66 gradNorm: 17.78 \n",
      "2025-03-13 22:00:17,161: Train batch 2358: loss: 3.86 gradNorm: 27.57 \n",
      "2025-03-13 22:00:17,243: Train batch 2359: loss: 5.47 gradNorm: 25.23 \n",
      "2025-03-13 22:00:17,374: Train batch 2360: loss: 2.26 gradNorm: 15.49 \n",
      "2025-03-13 22:00:17,643: Train batch 2361: loss: 4.22 gradNorm: 28.69 \n",
      "2025-03-13 22:00:17,756: Train batch 2362: loss: 2.16 gradNorm: 14.51 \n",
      "2025-03-13 22:00:17,836: Train batch 2363: loss: 3.47 gradNorm: 16.89 \n",
      "2025-03-13 22:00:17,948: Train batch 2364: loss: 2.09 gradNorm: 14.05 \n",
      "2025-03-13 22:00:18,050: Train batch 2365: loss: 4.58 gradNorm: 21.09 \n",
      "2025-03-13 22:00:18,362: Train batch 2366: loss: 3.80 gradNorm: 21.57 \n",
      "2025-03-13 22:00:18,497: Train batch 2367: loss: 4.10 gradNorm: 22.65 \n",
      "2025-03-13 22:00:18,607: Train batch 2368: loss: 3.61 gradNorm: 22.11 \n",
      "2025-03-13 22:00:18,786: Train batch 2369: loss: 3.61 gradNorm: 25.11 \n",
      "2025-03-13 22:00:18,886: Train batch 2370: loss: 2.21 gradNorm: 11.65 \n",
      "2025-03-13 22:00:19,158: Train batch 2371: loss: 3.90 gradNorm: 19.39 \n",
      "2025-03-13 22:00:19,247: Train batch 2372: loss: 4.67 gradNorm: 20.51 \n",
      "2025-03-13 22:00:19,333: Train batch 2373: loss: 4.50 gradNorm: 20.98 \n",
      "2025-03-13 22:00:19,502: Train batch 2374: loss: 3.53 gradNorm: 20.36 \n",
      "2025-03-13 22:00:19,806: Train batch 2375: loss: 4.15 gradNorm: 20.47 \n",
      "2025-03-13 22:00:19,902: Train batch 2376: loss: 2.80 gradNorm: 14.25 \n",
      "2025-03-13 22:00:20,014: Train batch 2377: loss: 2.33 gradNorm: 15.20 \n",
      "2025-03-13 22:00:20,193: Train batch 2378: loss: 1.94 gradNorm: 15.93 \n",
      "2025-03-13 22:00:20,354: Train batch 2379: loss: 2.45 gradNorm: 16.56 \n",
      "2025-03-13 22:00:20,453: Train batch 2380: loss: 2.52 gradNorm: 13.83 \n",
      "2025-03-13 22:00:20,620: Train batch 2381: loss: 2.94 gradNorm: 17.09 \n",
      "2025-03-13 22:00:20,870: Train batch 2382: loss: 3.13 gradNorm: 21.01 \n",
      "2025-03-13 22:00:20,953: Train batch 2383: loss: 4.76 gradNorm: 21.48 \n",
      "2025-03-13 22:00:21,033: Train batch 2384: loss: 2.62 gradNorm: 18.77 \n",
      "2025-03-13 22:00:21,339: Train batch 2385: loss: 4.05 gradNorm: 22.99 \n",
      "2025-03-13 22:00:21,706: Train batch 2386: loss: 6.04 gradNorm: 29.86 \n",
      "2025-03-13 22:00:21,806: Train batch 2387: loss: 2.30 gradNorm: 13.05 \n",
      "2025-03-13 22:00:21,941: Train batch 2388: loss: 3.02 gradNorm: 17.70 \n",
      "2025-03-13 22:00:22,040: Train batch 2389: loss: 3.37 gradNorm: 17.84 \n",
      "2025-03-13 22:00:22,108: Train batch 2390: loss: 3.60 gradNorm: 19.90 \n",
      "2025-03-13 22:00:22,208: Train batch 2391: loss: 4.29 gradNorm: 21.60 \n",
      "2025-03-13 22:00:22,315: Train batch 2392: loss: 4.10 gradNorm: 20.91 \n",
      "2025-03-13 22:00:22,474: Train batch 2393: loss: 1.57 gradNorm: 11.15 \n",
      "2025-03-13 22:00:22,571: Train batch 2394: loss: 2.98 gradNorm: 16.47 \n",
      "2025-03-13 22:00:22,809: Train batch 2395: loss: 3.60 gradNorm: 23.86 \n",
      "2025-03-13 22:00:22,926: Train batch 2396: loss: 3.16 gradNorm: 17.88 \n",
      "2025-03-13 22:00:23,160: Train batch 2397: loss: 3.72 gradNorm: 18.08 \n",
      "2025-03-13 22:00:23,483: Train batch 2398: loss: 3.35 gradNorm: 21.39 \n",
      "2025-03-13 22:00:23,642: Train batch 2399: loss: 2.98 gradNorm: 20.07 \n",
      "2025-03-13 22:00:23,859: Train batch 2400: loss: 3.73 gradNorm: 18.55 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:00:24,060: Val batch: CER (t18.2025.01.15): 0.092\n",
      "2025-03-13 22:00:24,061: Val batch 2400: CER (avg): 0.092 \n",
      "2025-03-13 22:00:24,061: Batches since validation CER improved: 150\n",
      "2025-03-13 22:00:24,141: Train batch 2401: loss: 4.18 gradNorm: 22.96 \n",
      "2025-03-13 22:00:24,211: Train batch 2402: loss: 3.50 gradNorm: 21.27 \n",
      "2025-03-13 22:00:24,419: Train batch 2403: loss: 2.96 gradNorm: 17.70 \n",
      "2025-03-13 22:00:24,722: Train batch 2404: loss: 3.65 gradNorm: 23.05 \n",
      "2025-03-13 22:00:24,824: Train batch 2405: loss: 3.76 gradNorm: 20.48 \n",
      "2025-03-13 22:00:24,906: Train batch 2406: loss: 3.76 gradNorm: 18.50 \n",
      "2025-03-13 22:00:25,033: Train batch 2407: loss: 2.18 gradNorm: 18.87 \n",
      "2025-03-13 22:00:25,113: Train batch 2408: loss: 4.55 gradNorm: 20.60 \n",
      "2025-03-13 22:00:25,279: Train batch 2409: loss: 4.26 gradNorm: 25.70 \n",
      "2025-03-13 22:00:25,581: Train batch 2410: loss: 4.36 gradNorm: 24.63 \n",
      "2025-03-13 22:00:25,774: Train batch 2411: loss: 2.74 gradNorm: 17.15 \n",
      "2025-03-13 22:00:25,945: Train batch 2412: loss: 2.31 gradNorm: 17.79 \n",
      "2025-03-13 22:00:26,020: Train batch 2413: loss: 4.76 gradNorm: 24.78 \n",
      "2025-03-13 22:00:26,136: Train batch 2414: loss: 2.88 gradNorm: 16.93 \n",
      "2025-03-13 22:00:26,215: Train batch 2415: loss: 4.89 gradNorm: 23.35 \n",
      "2025-03-13 22:00:26,367: Train batch 2416: loss: 2.16 gradNorm: 14.25 \n",
      "2025-03-13 22:00:26,435: Train batch 2417: loss: 4.51 gradNorm: 21.22 \n",
      "2025-03-13 22:00:26,607: Train batch 2418: loss: 1.58 gradNorm: 10.61 \n",
      "2025-03-13 22:00:26,766: Train batch 2419: loss: 1.59 gradNorm: 10.85 \n",
      "2025-03-13 22:00:27,114: Train batch 2420: loss: 5.97 gradNorm: 32.77 \n",
      "2025-03-13 22:00:27,222: Train batch 2421: loss: 1.22 gradNorm: 12.15 \n",
      "2025-03-13 22:00:27,575: Train batch 2422: loss: 3.36 gradNorm: 19.75 \n",
      "2025-03-13 22:00:27,964: Train batch 2423: loss: 2.67 gradNorm: 16.20 \n",
      "2025-03-13 22:00:28,060: Train batch 2424: loss: 3.97 gradNorm: 20.81 \n",
      "2025-03-13 22:00:28,151: Train batch 2425: loss: 3.80 gradNorm: 19.14 \n",
      "2025-03-13 22:00:28,250: Train batch 2426: loss: 2.63 gradNorm: 14.34 \n",
      "2025-03-13 22:00:28,334: Train batch 2427: loss: 3.72 gradNorm: 23.29 \n",
      "2025-03-13 22:00:28,437: Train batch 2428: loss: 2.77 gradNorm: 14.48 \n",
      "2025-03-13 22:00:28,520: Train batch 2429: loss: 4.59 gradNorm: 21.99 \n",
      "2025-03-13 22:00:28,632: Train batch 2430: loss: 2.39 gradNorm: 14.29 \n",
      "2025-03-13 22:00:28,816: Train batch 2431: loss: 3.12 gradNorm: 20.47 \n",
      "2025-03-13 22:00:28,924: Train batch 2432: loss: 2.80 gradNorm: 15.80 \n",
      "2025-03-13 22:00:29,003: Train batch 2433: loss: 3.37 gradNorm: 18.61 \n",
      "2025-03-13 22:00:29,116: Train batch 2434: loss: 1.95 gradNorm: 14.54 \n",
      "2025-03-13 22:00:29,310: Train batch 2435: loss: 1.91 gradNorm: 13.75 \n",
      "2025-03-13 22:00:29,475: Train batch 2436: loss: 2.58 gradNorm: 16.75 \n",
      "2025-03-13 22:00:29,682: Train batch 2437: loss: 2.40 gradNorm: 16.04 \n",
      "2025-03-13 22:00:29,764: Train batch 2438: loss: 3.79 gradNorm: 21.07 \n",
      "2025-03-13 22:00:29,854: Train batch 2439: loss: 3.30 gradNorm: 17.21 \n",
      "2025-03-13 22:00:30,109: Train batch 2440: loss: 5.04 gradNorm: 26.83 \n",
      "2025-03-13 22:00:30,192: Train batch 2441: loss: 2.70 gradNorm: 18.19 \n",
      "2025-03-13 22:00:30,274: Train batch 2442: loss: 3.28 gradNorm: 16.52 \n",
      "2025-03-13 22:00:30,382: Train batch 2443: loss: 2.62 gradNorm: 19.04 \n",
      "2025-03-13 22:00:30,540: Train batch 2444: loss: 3.35 gradNorm: 21.61 \n",
      "2025-03-13 22:00:30,622: Train batch 2445: loss: 2.68 gradNorm: 13.79 \n",
      "2025-03-13 22:00:30,702: Train batch 2446: loss: 3.56 gradNorm: 21.02 \n",
      "2025-03-13 22:00:30,772: Train batch 2447: loss: 3.23 gradNorm: 22.77 \n",
      "2025-03-13 22:00:30,849: Train batch 2448: loss: 2.35 gradNorm: 14.96 \n",
      "2025-03-13 22:00:31,116: Train batch 2449: loss: 8.08 gradNorm: 38.27 \n",
      "2025-03-13 22:00:31,516: Train batch 2450: loss: 5.08 gradNorm: 27.10 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:00:31,709: Val batch: CER (t18.2025.01.15): 0.083\n",
      "2025-03-13 22:00:31,710: Val batch 2450: CER (avg): 0.083 \n",
      "2025-03-13 22:00:31,711: Batches since validation CER improved: 200\n",
      "2025-03-13 22:00:31,837: Train batch 2451: loss: 3.14 gradNorm: 18.71 \n",
      "2025-03-13 22:00:31,982: Train batch 2452: loss: 3.22 gradNorm: 21.75 \n",
      "2025-03-13 22:00:32,280: Train batch 2453: loss: 3.20 gradNorm: 18.73 \n",
      "2025-03-13 22:00:32,398: Train batch 2454: loss: 2.53 gradNorm: 15.00 \n",
      "2025-03-13 22:00:32,605: Train batch 2455: loss: 4.46 gradNorm: 24.33 \n",
      "2025-03-13 22:00:32,960: Train batch 2456: loss: 3.99 gradNorm: 23.48 \n",
      "2025-03-13 22:00:33,103: Train batch 2457: loss: 2.78 gradNorm: 18.95 \n",
      "2025-03-13 22:00:33,213: Train batch 2458: loss: 3.80 gradNorm: 20.82 \n",
      "2025-03-13 22:00:33,282: Train batch 2459: loss: 3.41 gradNorm: 19.29 \n",
      "2025-03-13 22:00:33,476: Train batch 2460: loss: 2.56 gradNorm: 17.64 \n",
      "2025-03-13 22:00:33,579: Train batch 2461: loss: 2.81 gradNorm: 17.27 \n",
      "2025-03-13 22:00:33,879: Train batch 2462: loss: 4.81 gradNorm: 30.17 \n",
      "2025-03-13 22:00:33,956: Train batch 2463: loss: 4.12 gradNorm: 21.53 \n",
      "2025-03-13 22:00:34,208: Train batch 2464: loss: 3.09 gradNorm: 18.09 \n",
      "2025-03-13 22:00:34,405: Train batch 2465: loss: 2.86 gradNorm: 18.48 \n",
      "2025-03-13 22:00:34,701: Train batch 2466: loss: 3.82 gradNorm: 20.43 \n",
      "2025-03-13 22:00:34,871: Train batch 2467: loss: 1.79 gradNorm: 13.39 \n",
      "2025-03-13 22:00:35,226: Train batch 2468: loss: 3.41 gradNorm: 20.52 \n",
      "2025-03-13 22:00:35,382: Train batch 2469: loss: 2.15 gradNorm: 14.58 \n",
      "2025-03-13 22:00:35,658: Train batch 2470: loss: 4.62 gradNorm: 25.94 \n",
      "2025-03-13 22:00:35,880: Train batch 2471: loss: 3.39 gradNorm: 21.00 \n",
      "2025-03-13 22:00:36,044: Train batch 2472: loss: 1.69 gradNorm: 12.37 \n",
      "2025-03-13 22:00:36,192: Train batch 2473: loss: 1.45 gradNorm: 14.71 \n",
      "2025-03-13 22:00:36,339: Train batch 2474: loss: 1.10 gradNorm: 13.28 \n",
      "2025-03-13 22:00:36,438: Train batch 2475: loss: 3.67 gradNorm: 21.17 \n",
      "2025-03-13 22:00:36,572: Train batch 2476: loss: 2.92 gradNorm: 18.21 \n",
      "2025-03-13 22:00:36,748: Train batch 2477: loss: 1.40 gradNorm: 9.63 \n",
      "2025-03-13 22:00:36,842: Train batch 2478: loss: 4.13 gradNorm: 18.56 \n",
      "2025-03-13 22:00:37,150: Train batch 2479: loss: 6.01 gradNorm: 38.75 \n",
      "2025-03-13 22:00:37,290: Train batch 2480: loss: 1.08 gradNorm: 10.22 \n",
      "2025-03-13 22:00:37,445: Train batch 2481: loss: 1.06 gradNorm: 8.76 \n",
      "2025-03-13 22:00:37,596: Train batch 2482: loss: 1.21 gradNorm: 11.61 \n",
      "2025-03-13 22:00:37,674: Train batch 2483: loss: 4.46 gradNorm: 23.53 \n",
      "2025-03-13 22:00:38,070: Train batch 2484: loss: 6.22 gradNorm: 43.61 \n",
      "2025-03-13 22:00:38,422: Train batch 2485: loss: 4.16 gradNorm: 30.44 \n",
      "2025-03-13 22:00:38,493: Train batch 2486: loss: 4.65 gradNorm: 25.28 \n",
      "2025-03-13 22:00:38,789: Train batch 2487: loss: 3.42 gradNorm: 21.50 \n",
      "2025-03-13 22:00:38,876: Train batch 2488: loss: 4.81 gradNorm: 22.34 \n",
      "2025-03-13 22:00:38,958: Train batch 2489: loss: 3.75 gradNorm: 21.25 \n",
      "2025-03-13 22:00:39,154: Train batch 2490: loss: 8.10 gradNorm: 63.50 \n",
      "2025-03-13 22:00:39,351: Train batch 2491: loss: 1.74 gradNorm: 15.12 \n",
      "2025-03-13 22:00:39,700: Train batch 2492: loss: 5.32 gradNorm: 41.06 \n",
      "2025-03-13 22:00:39,780: Train batch 2493: loss: 4.02 gradNorm: 19.31 \n",
      "2025-03-13 22:00:39,883: Train batch 2494: loss: 4.81 gradNorm: 25.42 \n",
      "2025-03-13 22:00:40,156: Train batch 2495: loss: 5.12 gradNorm: 33.26 \n",
      "2025-03-13 22:00:40,353: Train batch 2496: loss: 2.64 gradNorm: 18.44 \n",
      "2025-03-13 22:00:40,497: Train batch 2497: loss: 3.17 gradNorm: 20.15 \n",
      "2025-03-13 22:00:40,814: Train batch 2498: loss: 4.78 gradNorm: 24.64 \n",
      "2025-03-13 22:00:40,917: Train batch 2499: loss: 3.58 gradNorm: 16.87 \n",
      "2025-03-13 22:00:41,047: Train batch 2500: loss: 1.58 gradNorm: 16.36 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:00:41,241: Val batch: CER (t18.2025.01.15): 0.073\n",
      "2025-03-13 22:00:41,243: Val batch 2500: CER (avg): 0.073 \n",
      "2025-03-13 22:00:41,462: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 22:00:41,466: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 22:00:41,466: Batches since validation CER improved: 0\n",
      "2025-03-13 22:00:41,737: Train batch 2501: loss: 2.36 gradNorm: 15.86 \n",
      "2025-03-13 22:00:41,811: Train batch 2502: loss: 4.25 gradNorm: 25.31 \n",
      "2025-03-13 22:00:41,900: Train batch 2503: loss: 5.21 gradNorm: 24.47 \n",
      "2025-03-13 22:00:42,249: Train batch 2504: loss: 3.22 gradNorm: 21.75 \n",
      "2025-03-13 22:00:42,432: Train batch 2505: loss: 2.44 gradNorm: 18.88 \n",
      "2025-03-13 22:00:42,517: Train batch 2506: loss: 4.24 gradNorm: 25.36 \n",
      "2025-03-13 22:00:42,591: Train batch 2507: loss: 4.26 gradNorm: 21.66 \n",
      "2025-03-13 22:00:42,689: Train batch 2508: loss: 3.48 gradNorm: 18.92 \n",
      "2025-03-13 22:00:42,785: Train batch 2509: loss: 3.54 gradNorm: 21.07 \n",
      "2025-03-13 22:00:42,970: Train batch 2510: loss: 2.94 gradNorm: 18.99 \n",
      "2025-03-13 22:00:43,051: Train batch 2511: loss: 2.71 gradNorm: 15.71 \n",
      "2025-03-13 22:00:43,136: Train batch 2512: loss: 3.33 gradNorm: 18.54 \n",
      "2025-03-13 22:00:43,246: Train batch 2513: loss: 2.60 gradNorm: 15.61 \n",
      "2025-03-13 22:00:43,402: Train batch 2514: loss: 1.38 gradNorm: 11.68 \n",
      "2025-03-13 22:00:43,477: Train batch 2515: loss: 2.53 gradNorm: 14.87 \n",
      "2025-03-13 22:00:43,729: Train batch 2516: loss: 3.89 gradNorm: 25.64 \n",
      "2025-03-13 22:00:43,820: Train batch 2517: loss: 4.07 gradNorm: 19.44 \n",
      "2025-03-13 22:00:43,916: Train batch 2518: loss: 1.74 gradNorm: 11.28 \n",
      "2025-03-13 22:00:44,226: Train batch 2519: loss: 3.52 gradNorm: 23.03 \n",
      "2025-03-13 22:00:44,318: Train batch 2520: loss: 3.37 gradNorm: 18.70 \n",
      "2025-03-13 22:00:44,504: Train batch 2521: loss: 2.53 gradNorm: 19.28 \n",
      "2025-03-13 22:00:44,592: Train batch 2522: loss: 3.00 gradNorm: 17.93 \n",
      "2025-03-13 22:00:44,770: Train batch 2523: loss: 1.54 gradNorm: 11.56 \n",
      "2025-03-13 22:00:44,897: Train batch 2524: loss: 2.88 gradNorm: 18.91 \n",
      "2025-03-13 22:00:45,066: Train batch 2525: loss: 1.76 gradNorm: 17.53 \n",
      "2025-03-13 22:00:45,214: Train batch 2526: loss: 1.83 gradNorm: 14.23 \n",
      "2025-03-13 22:00:45,409: Train batch 2527: loss: 2.87 gradNorm: 18.06 \n",
      "2025-03-13 22:00:45,494: Train batch 2528: loss: 3.03 gradNorm: 16.60 \n",
      "2025-03-13 22:00:45,883: Train batch 2529: loss: 2.81 gradNorm: 18.88 \n",
      "2025-03-13 22:00:46,023: Train batch 2530: loss: 2.42 gradNorm: 19.14 \n",
      "2025-03-13 22:00:46,190: Train batch 2531: loss: 2.78 gradNorm: 19.87 \n",
      "2025-03-13 22:00:46,288: Train batch 2532: loss: 4.67 gradNorm: 23.57 \n",
      "2025-03-13 22:00:46,387: Train batch 2533: loss: 3.62 gradNorm: 18.03 \n",
      "2025-03-13 22:00:46,457: Train batch 2534: loss: 4.28 gradNorm: 22.88 \n",
      "2025-03-13 22:00:46,672: Train batch 2535: loss: 2.14 gradNorm: 16.02 \n",
      "2025-03-13 22:00:46,807: Train batch 2536: loss: 2.42 gradNorm: 15.44 \n",
      "2025-03-13 22:00:46,928: Train batch 2537: loss: 3.93 gradNorm: 18.53 \n",
      "2025-03-13 22:00:47,201: Train batch 2538: loss: 2.56 gradNorm: 21.63 \n",
      "2025-03-13 22:00:47,281: Train batch 2539: loss: 2.45 gradNorm: 14.67 \n",
      "2025-03-13 22:00:47,441: Train batch 2540: loss: 2.48 gradNorm: 17.39 \n",
      "2025-03-13 22:00:47,659: Train batch 2541: loss: 1.86 gradNorm: 14.64 \n",
      "2025-03-13 22:00:47,936: Train batch 2542: loss: 2.96 gradNorm: 17.97 \n",
      "2025-03-13 22:00:48,024: Train batch 2543: loss: 3.33 gradNorm: 16.04 \n",
      "2025-03-13 22:00:48,099: Train batch 2544: loss: 3.10 gradNorm: 17.41 \n",
      "2025-03-13 22:00:48,413: Train batch 2545: loss: 2.78 gradNorm: 16.15 \n",
      "2025-03-13 22:00:48,613: Train batch 2546: loss: 3.20 gradNorm: 19.13 \n",
      "2025-03-13 22:00:48,740: Train batch 2547: loss: 3.19 gradNorm: 16.69 \n",
      "2025-03-13 22:00:49,010: Train batch 2548: loss: 2.66 gradNorm: 20.58 \n",
      "2025-03-13 22:00:49,181: Train batch 2549: loss: 2.66 gradNorm: 22.76 \n",
      "2025-03-13 22:00:49,270: Train batch 2550: loss: 2.42 gradNorm: 16.76 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:00:49,473: Val batch: CER (t18.2025.01.15): 0.079\n",
      "2025-03-13 22:00:49,474: Val batch 2550: CER (avg): 0.079 \n",
      "2025-03-13 22:00:49,474: Batches since validation CER improved: 50\n",
      "2025-03-13 22:00:49,569: Train batch 2551: loss: 2.63 gradNorm: 14.58 \n",
      "2025-03-13 22:00:49,765: Train batch 2552: loss: 2.00 gradNorm: 15.35 \n",
      "2025-03-13 22:00:49,925: Train batch 2553: loss: 2.15 gradNorm: 15.16 \n",
      "2025-03-13 22:00:50,078: Train batch 2554: loss: 2.29 gradNorm: 18.13 \n",
      "2025-03-13 22:00:50,177: Train batch 2555: loss: 2.02 gradNorm: 13.99 \n",
      "2025-03-13 22:00:50,532: Train batch 2556: loss: 4.06 gradNorm: 23.84 \n",
      "2025-03-13 22:00:50,618: Train batch 2557: loss: 3.27 gradNorm: 20.08 \n",
      "2025-03-13 22:00:50,914: Train batch 2558: loss: 3.30 gradNorm: 17.42 \n",
      "2025-03-13 22:00:50,997: Train batch 2559: loss: 2.92 gradNorm: 16.67 \n",
      "2025-03-13 22:00:51,108: Train batch 2560: loss: 2.58 gradNorm: 14.21 \n",
      "2025-03-13 22:00:51,203: Train batch 2561: loss: 2.69 gradNorm: 15.69 \n",
      "2025-03-13 22:00:51,298: Train batch 2562: loss: 3.38 gradNorm: 18.37 \n",
      "2025-03-13 22:00:51,366: Train batch 2563: loss: 4.08 gradNorm: 21.59 \n",
      "2025-03-13 22:00:51,460: Train batch 2564: loss: 3.21 gradNorm: 16.01 \n",
      "2025-03-13 22:00:51,745: Train batch 2565: loss: 4.60 gradNorm: 30.78 \n",
      "2025-03-13 22:00:51,933: Train batch 2566: loss: 3.08 gradNorm: 20.61 \n",
      "2025-03-13 22:00:52,216: Train batch 2567: loss: 3.12 gradNorm: 21.38 \n",
      "2025-03-13 22:00:52,571: Train batch 2568: loss: 3.03 gradNorm: 23.32 \n",
      "2025-03-13 22:00:52,731: Train batch 2569: loss: 1.86 gradNorm: 12.03 \n",
      "2025-03-13 22:00:52,888: Train batch 2570: loss: 0.96 gradNorm: 8.71 \n",
      "2025-03-13 22:00:52,968: Train batch 2571: loss: 3.18 gradNorm: 16.73 \n",
      "2025-03-13 22:00:53,342: Train batch 2572: loss: 2.57 gradNorm: 18.60 \n",
      "2025-03-13 22:00:53,423: Train batch 2573: loss: 3.17 gradNorm: 16.72 \n",
      "2025-03-13 22:00:53,692: Train batch 2574: loss: 3.57 gradNorm: 23.97 \n",
      "2025-03-13 22:00:53,789: Train batch 2575: loss: 2.65 gradNorm: 15.43 \n",
      "2025-03-13 22:00:53,955: Train batch 2576: loss: 1.70 gradNorm: 18.59 \n",
      "2025-03-13 22:00:54,125: Train batch 2577: loss: 3.66 gradNorm: 27.10 \n",
      "2025-03-13 22:00:54,331: Train batch 2578: loss: 2.65 gradNorm: 18.34 \n",
      "2025-03-13 22:00:54,465: Train batch 2579: loss: 3.27 gradNorm: 19.41 \n",
      "2025-03-13 22:00:54,787: Train batch 2580: loss: 4.08 gradNorm: 24.41 \n",
      "2025-03-13 22:00:54,978: Train batch 2581: loss: 2.33 gradNorm: 19.15 \n",
      "2025-03-13 22:00:55,081: Train batch 2582: loss: 3.45 gradNorm: 20.02 \n",
      "2025-03-13 22:00:55,340: Train batch 2583: loss: 3.27 gradNorm: 19.95 \n",
      "2025-03-13 22:00:55,472: Train batch 2584: loss: 2.98 gradNorm: 19.25 \n",
      "2025-03-13 22:00:55,569: Train batch 2585: loss: 4.75 gradNorm: 25.64 \n",
      "2025-03-13 22:00:55,642: Train batch 2586: loss: 3.17 gradNorm: 20.19 \n",
      "2025-03-13 22:00:55,720: Train batch 2587: loss: 3.28 gradNorm: 21.35 \n",
      "2025-03-13 22:00:55,908: Train batch 2588: loss: 2.47 gradNorm: 16.83 \n",
      "2025-03-13 22:00:56,200: Train batch 2589: loss: 3.60 gradNorm: 21.19 \n",
      "2025-03-13 22:00:56,453: Train batch 2590: loss: 3.49 gradNorm: 22.79 \n",
      "2025-03-13 22:00:56,756: Train batch 2591: loss: 4.09 gradNorm: 26.48 \n",
      "2025-03-13 22:00:56,912: Train batch 2592: loss: 2.09 gradNorm: 14.15 \n",
      "2025-03-13 22:00:57,065: Train batch 2593: loss: 1.89 gradNorm: 21.83 \n",
      "2025-03-13 22:00:57,147: Train batch 2594: loss: 3.85 gradNorm: 19.36 \n",
      "2025-03-13 22:00:57,427: Train batch 2595: loss: 3.49 gradNorm: 21.73 \n",
      "2025-03-13 22:00:57,679: Train batch 2596: loss: 2.64 gradNorm: 19.83 \n",
      "2025-03-13 22:00:57,782: Train batch 2597: loss: 4.41 gradNorm: 22.68 \n",
      "2025-03-13 22:00:58,083: Train batch 2598: loss: 2.41 gradNorm: 16.45 \n",
      "2025-03-13 22:00:58,171: Train batch 2599: loss: 5.27 gradNorm: 26.89 \n",
      "2025-03-13 22:00:58,298: Train batch 2600: loss: 2.98 gradNorm: 16.10 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:00:58,488: Val batch: CER (t18.2025.01.15): 0.068\n",
      "2025-03-13 22:00:58,490: Val batch 2600: CER (avg): 0.068 \n",
      "2025-03-13 22:00:58,709: Model checkpoint saved to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 22:00:58,713: Saved training args.yaml to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/\n",
      "2025-03-13 22:00:58,713: Batches since validation CER improved: 0\n",
      "2025-03-13 22:00:59,074: Train batch 2601: loss: 2.64 gradNorm: 17.77 \n",
      "2025-03-13 22:00:59,177: Train batch 2602: loss: 2.14 gradNorm: 15.40 \n",
      "2025-03-13 22:00:59,263: Train batch 2603: loss: 4.17 gradNorm: 23.58 \n",
      "2025-03-13 22:00:59,337: Train batch 2604: loss: 3.50 gradNorm: 23.45 \n",
      "2025-03-13 22:00:59,605: Train batch 2605: loss: 3.26 gradNorm: 19.23 \n",
      "2025-03-13 22:00:59,707: Train batch 2606: loss: 3.88 gradNorm: 20.26 \n",
      "2025-03-13 22:01:00,082: Train batch 2607: loss: 3.03 gradNorm: 22.71 \n",
      "2025-03-13 22:01:00,227: Train batch 2608: loss: 3.19 gradNorm: 22.91 \n",
      "2025-03-13 22:01:00,420: Train batch 2609: loss: 2.36 gradNorm: 18.25 \n",
      "2025-03-13 22:01:00,504: Train batch 2610: loss: 2.72 gradNorm: 15.28 \n",
      "2025-03-13 22:01:00,587: Train batch 2611: loss: 3.96 gradNorm: 22.44 \n",
      "2025-03-13 22:01:00,859: Train batch 2612: loss: 3.79 gradNorm: 24.49 \n",
      "2025-03-13 22:01:00,931: Train batch 2613: loss: 3.23 gradNorm: 20.52 \n",
      "2025-03-13 22:01:01,016: Train batch 2614: loss: 2.79 gradNorm: 18.05 \n",
      "2025-03-13 22:01:01,232: Train batch 2615: loss: 1.99 gradNorm: 14.03 \n",
      "2025-03-13 22:01:01,335: Train batch 2616: loss: 3.94 gradNorm: 16.49 \n",
      "2025-03-13 22:01:01,452: Train batch 2617: loss: 2.23 gradNorm: 16.94 \n",
      "2025-03-13 22:01:01,520: Train batch 2618: loss: 2.43 gradNorm: 15.77 \n",
      "2025-03-13 22:01:01,625: Train batch 2619: loss: 3.04 gradNorm: 19.29 \n",
      "2025-03-13 22:01:01,885: Train batch 2620: loss: 2.99 gradNorm: 19.39 \n",
      "2025-03-13 22:01:02,154: Train batch 2621: loss: 2.95 gradNorm: 20.47 \n",
      "2025-03-13 22:01:02,298: Train batch 2622: loss: 2.24 gradNorm: 16.29 \n",
      "2025-03-13 22:01:02,481: Train batch 2623: loss: 3.59 gradNorm: 21.37 \n",
      "2025-03-13 22:01:02,763: Train batch 2624: loss: 4.77 gradNorm: 27.01 \n",
      "2025-03-13 22:01:02,845: Train batch 2625: loss: 2.59 gradNorm: 19.39 \n",
      "2025-03-13 22:01:02,915: Train batch 2626: loss: 3.18 gradNorm: 20.51 \n",
      "2025-03-13 22:01:03,117: Train batch 2627: loss: 2.08 gradNorm: 17.92 \n",
      "2025-03-13 22:01:03,277: Train batch 2628: loss: 1.83 gradNorm: 14.85 \n",
      "2025-03-13 22:01:03,349: Train batch 2629: loss: 3.49 gradNorm: 20.65 \n",
      "2025-03-13 22:01:03,442: Train batch 2630: loss: 3.54 gradNorm: 18.32 \n",
      "2025-03-13 22:01:03,557: Train batch 2631: loss: 2.39 gradNorm: 12.32 \n",
      "2025-03-13 22:01:03,873: Train batch 2632: loss: 4.62 gradNorm: 35.06 \n",
      "2025-03-13 22:01:04,028: Train batch 2633: loss: 1.46 gradNorm: 10.51 \n",
      "2025-03-13 22:01:04,157: Train batch 2634: loss: 2.79 gradNorm: 16.13 \n",
      "2025-03-13 22:01:04,450: Train batch 2635: loss: 2.81 gradNorm: 19.72 \n",
      "2025-03-13 22:01:04,725: Train batch 2636: loss: 3.01 gradNorm: 22.43 \n",
      "2025-03-13 22:01:04,908: Train batch 2637: loss: 2.21 gradNorm: 17.24 \n",
      "2025-03-13 22:01:05,001: Train batch 2638: loss: 2.24 gradNorm: 16.73 \n",
      "2025-03-13 22:01:05,272: Train batch 2639: loss: 2.88 gradNorm: 21.16 \n",
      "2025-03-13 22:01:05,615: Train batch 2640: loss: 2.18 gradNorm: 17.65 \n",
      "2025-03-13 22:01:05,766: Train batch 2641: loss: 1.73 gradNorm: 15.40 \n",
      "2025-03-13 22:01:05,931: Train batch 2642: loss: 2.19 gradNorm: 18.21 \n",
      "2025-03-13 22:01:06,072: Train batch 2643: loss: 2.63 gradNorm: 17.62 \n",
      "2025-03-13 22:01:06,165: Train batch 2644: loss: 3.98 gradNorm: 24.83 \n",
      "2025-03-13 22:01:06,240: Train batch 2645: loss: 2.85 gradNorm: 17.94 \n",
      "2025-03-13 22:01:06,469: Train batch 2646: loss: 3.98 gradNorm: 22.90 \n",
      "2025-03-13 22:01:06,622: Train batch 2647: loss: 1.55 gradNorm: 11.02 \n",
      "2025-03-13 22:01:06,886: Train batch 2648: loss: 2.11 gradNorm: 16.93 \n",
      "2025-03-13 22:01:07,080: Train batch 2649: loss: 1.88 gradNorm: 15.45 \n",
      "2025-03-13 22:01:07,191: Train batch 2650: loss: 2.59 gradNorm: 17.48 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:01:07,433: Val batch: CER (t18.2025.01.15): 0.078\n",
      "2025-03-13 22:01:07,434: Val batch 2650: CER (avg): 0.078 \n",
      "2025-03-13 22:01:07,435: Batches since validation CER improved: 50\n",
      "2025-03-13 22:01:07,572: Train batch 2651: loss: 2.67 gradNorm: 19.29 \n",
      "2025-03-13 22:01:07,683: Train batch 2652: loss: 3.66 gradNorm: 20.13 \n",
      "2025-03-13 22:01:07,898: Train batch 2653: loss: 3.21 gradNorm: 20.08 \n",
      "2025-03-13 22:01:08,018: Train batch 2654: loss: 3.15 gradNorm: 17.79 \n",
      "2025-03-13 22:01:08,102: Train batch 2655: loss: 3.42 gradNorm: 16.91 \n",
      "2025-03-13 22:01:08,174: Train batch 2656: loss: 3.43 gradNorm: 21.23 \n",
      "2025-03-13 22:01:08,268: Train batch 2657: loss: 2.49 gradNorm: 15.49 \n",
      "2025-03-13 22:01:08,338: Train batch 2658: loss: 2.21 gradNorm: 15.56 \n",
      "2025-03-13 22:01:08,416: Train batch 2659: loss: 2.92 gradNorm: 17.57 \n",
      "2025-03-13 22:01:08,606: Train batch 2660: loss: 2.13 gradNorm: 16.27 \n",
      "2025-03-13 22:01:08,795: Train batch 2661: loss: 1.90 gradNorm: 14.10 \n",
      "2025-03-13 22:01:08,896: Train batch 2662: loss: 3.81 gradNorm: 20.73 \n",
      "2025-03-13 22:01:08,976: Train batch 2663: loss: 2.88 gradNorm: 17.31 \n",
      "2025-03-13 22:01:09,065: Train batch 2664: loss: 3.50 gradNorm: 19.23 \n",
      "2025-03-13 22:01:09,357: Train batch 2665: loss: 2.32 gradNorm: 15.12 \n",
      "2025-03-13 22:01:09,627: Train batch 2666: loss: 2.98 gradNorm: 21.11 \n",
      "2025-03-13 22:01:09,719: Train batch 2667: loss: 2.81 gradNorm: 18.24 \n",
      "2025-03-13 22:01:09,888: Train batch 2668: loss: 1.51 gradNorm: 16.59 \n",
      "2025-03-13 22:01:10,156: Train batch 2669: loss: 1.83 gradNorm: 14.38 \n",
      "2025-03-13 22:01:10,242: Train batch 2670: loss: 3.11 gradNorm: 18.55 \n",
      "2025-03-13 22:01:10,344: Train batch 2671: loss: 2.87 gradNorm: 17.14 \n",
      "2025-03-13 22:01:10,696: Train batch 2672: loss: 3.79 gradNorm: 21.88 \n",
      "2025-03-13 22:01:10,779: Train batch 2673: loss: 3.37 gradNorm: 16.41 \n",
      "2025-03-13 22:01:10,967: Train batch 2674: loss: 2.87 gradNorm: 25.88 \n",
      "2025-03-13 22:01:11,052: Train batch 2675: loss: 3.39 gradNorm: 19.73 \n",
      "2025-03-13 22:01:11,176: Train batch 2676: loss: 1.87 gradNorm: 20.19 \n",
      "2025-03-13 22:01:11,272: Train batch 2677: loss: 2.96 gradNorm: 17.93 \n",
      "2025-03-13 22:01:11,571: Train batch 2678: loss: 2.95 gradNorm: 19.17 \n",
      "2025-03-13 22:01:11,727: Train batch 2679: loss: 1.56 gradNorm: 12.56 \n",
      "2025-03-13 22:01:11,886: Train batch 2680: loss: 1.50 gradNorm: 11.18 \n",
      "2025-03-13 22:01:12,079: Train batch 2681: loss: 3.04 gradNorm: 18.91 \n",
      "2025-03-13 22:01:12,238: Train batch 2682: loss: 1.82 gradNorm: 13.25 \n",
      "2025-03-13 22:01:12,494: Train batch 2683: loss: 2.46 gradNorm: 17.74 \n",
      "2025-03-13 22:01:12,767: Train batch 2684: loss: 2.51 gradNorm: 21.23 \n",
      "2025-03-13 22:01:12,920: Train batch 2685: loss: 1.63 gradNorm: 15.73 \n",
      "2025-03-13 22:01:13,217: Train batch 2686: loss: 3.64 gradNorm: 19.30 \n",
      "2025-03-13 22:01:13,570: Train batch 2687: loss: 2.27 gradNorm: 16.73 \n",
      "2025-03-13 22:01:13,639: Train batch 2688: loss: 4.37 gradNorm: 24.08 \n",
      "2025-03-13 22:01:13,958: Train batch 2689: loss: 2.09 gradNorm: 14.88 \n",
      "2025-03-13 22:01:14,144: Train batch 2690: loss: 2.10 gradNorm: 18.37 \n",
      "2025-03-13 22:01:14,241: Train batch 2691: loss: 4.08 gradNorm: 21.11 \n",
      "2025-03-13 22:01:14,422: Train batch 2692: loss: 1.58 gradNorm: 15.28 \n",
      "2025-03-13 22:01:14,506: Train batch 2693: loss: 2.30 gradNorm: 16.02 \n",
      "2025-03-13 22:01:14,582: Train batch 2694: loss: 2.68 gradNorm: 18.15 \n",
      "2025-03-13 22:01:14,870: Train batch 2695: loss: 3.04 gradNorm: 18.66 \n",
      "2025-03-13 22:01:15,030: Train batch 2696: loss: 1.58 gradNorm: 19.56 \n",
      "2025-03-13 22:01:15,298: Train batch 2697: loss: 3.33 gradNorm: 21.50 \n",
      "2025-03-13 22:01:15,385: Train batch 2698: loss: 3.43 gradNorm: 21.03 \n",
      "2025-03-13 22:01:15,477: Train batch 2699: loss: 2.50 gradNorm: 19.75 \n",
      "2025-03-13 22:01:15,642: Train batch 2700: loss: 1.45 gradNorm: 12.58 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:01:15,835: Val batch: CER (t18.2025.01.15): 0.074\n",
      "2025-03-13 22:01:15,836: Val batch 2700: CER (avg): 0.074 \n",
      "2025-03-13 22:01:15,837: Batches since validation CER improved: 100\n",
      "2025-03-13 22:01:15,908: Train batch 2701: loss: 2.46 gradNorm: 16.96 \n",
      "2025-03-13 22:01:16,003: Train batch 2702: loss: 3.33 gradNorm: 17.44 \n",
      "2025-03-13 22:01:16,091: Train batch 2703: loss: 3.30 gradNorm: 20.17 \n",
      "2025-03-13 22:01:16,470: Train batch 2704: loss: 4.51 gradNorm: 26.16 \n",
      "2025-03-13 22:01:16,661: Train batch 2705: loss: 2.32 gradNorm: 15.87 \n",
      "2025-03-13 22:01:16,813: Train batch 2706: loss: 1.91 gradNorm: 14.14 \n",
      "2025-03-13 22:01:16,941: Train batch 2707: loss: 1.40 gradNorm: 13.50 \n",
      "2025-03-13 22:01:17,038: Train batch 2708: loss: 4.60 gradNorm: 21.65 \n",
      "2025-03-13 22:01:17,174: Train batch 2709: loss: 1.16 gradNorm: 14.05 \n",
      "2025-03-13 22:01:17,301: Train batch 2710: loss: 3.30 gradNorm: 22.89 \n",
      "2025-03-13 22:01:17,383: Train batch 2711: loss: 2.61 gradNorm: 17.36 \n",
      "2025-03-13 22:01:17,481: Train batch 2712: loss: 2.20 gradNorm: 16.06 \n",
      "2025-03-13 22:01:17,834: Train batch 2713: loss: 3.69 gradNorm: 24.39 \n",
      "2025-03-13 22:01:17,961: Train batch 2714: loss: 2.71 gradNorm: 15.51 \n",
      "2025-03-13 22:01:18,047: Train batch 2715: loss: 3.31 gradNorm: 17.78 \n",
      "2025-03-13 22:01:18,320: Train batch 2716: loss: 4.84 gradNorm: 27.44 \n",
      "2025-03-13 22:01:18,490: Train batch 2717: loss: 2.48 gradNorm: 18.47 \n",
      "2025-03-13 22:01:18,645: Train batch 2718: loss: 1.49 gradNorm: 12.86 \n",
      "2025-03-13 22:01:18,803: Train batch 2719: loss: 1.25 gradNorm: 11.10 \n",
      "2025-03-13 22:01:18,874: Train batch 2720: loss: 2.77 gradNorm: 18.38 \n",
      "2025-03-13 22:01:18,972: Train batch 2721: loss: 3.65 gradNorm: 21.10 \n",
      "2025-03-13 22:01:19,062: Train batch 2722: loss: 2.28 gradNorm: 14.62 \n",
      "2025-03-13 22:01:19,224: Train batch 2723: loss: 1.25 gradNorm: 14.11 \n",
      "2025-03-13 22:01:19,394: Train batch 2724: loss: 1.68 gradNorm: 15.37 \n",
      "2025-03-13 22:01:19,472: Train batch 2725: loss: 2.91 gradNorm: 15.95 \n",
      "2025-03-13 22:01:19,859: Train batch 2726: loss: 3.31 gradNorm: 25.89 \n",
      "2025-03-13 22:01:19,935: Train batch 2727: loss: 2.96 gradNorm: 17.25 \n",
      "2025-03-13 22:01:20,224: Train batch 2728: loss: 2.35 gradNorm: 18.81 \n",
      "2025-03-13 22:01:20,418: Train batch 2729: loss: 1.36 gradNorm: 12.13 \n",
      "2025-03-13 22:01:20,502: Train batch 2730: loss: 2.27 gradNorm: 15.15 \n",
      "2025-03-13 22:01:20,685: Train batch 2731: loss: 1.71 gradNorm: 15.11 \n",
      "2025-03-13 22:01:20,759: Train batch 2732: loss: 2.61 gradNorm: 16.73 \n",
      "2025-03-13 22:01:21,137: Train batch 2733: loss: 2.57 gradNorm: 17.85 \n",
      "2025-03-13 22:01:21,220: Train batch 2734: loss: 3.38 gradNorm: 19.38 \n",
      "2025-03-13 22:01:21,303: Train batch 2735: loss: 2.60 gradNorm: 17.16 \n",
      "2025-03-13 22:01:21,427: Train batch 2736: loss: 1.32 gradNorm: 13.69 \n",
      "2025-03-13 22:01:21,514: Train batch 2737: loss: 2.17 gradNorm: 17.75 \n",
      "2025-03-13 22:01:21,693: Train batch 2738: loss: 1.88 gradNorm: 18.29 \n",
      "2025-03-13 22:01:21,783: Train batch 2739: loss: 1.81 gradNorm: 13.31 \n",
      "2025-03-13 22:01:21,888: Train batch 2740: loss: 2.87 gradNorm: 18.20 \n",
      "2025-03-13 22:01:22,138: Train batch 2741: loss: 3.10 gradNorm: 21.82 \n",
      "2025-03-13 22:01:22,209: Train batch 2742: loss: 2.73 gradNorm: 17.19 \n",
      "2025-03-13 22:01:22,307: Train batch 2743: loss: 4.34 gradNorm: 19.21 \n",
      "2025-03-13 22:01:22,471: Train batch 2744: loss: 1.77 gradNorm: 14.78 \n",
      "2025-03-13 22:01:22,644: Train batch 2745: loss: 1.30 gradNorm: 14.30 \n",
      "2025-03-13 22:01:22,757: Train batch 2746: loss: 2.09 gradNorm: 12.99 \n",
      "2025-03-13 22:01:22,838: Train batch 2747: loss: 3.33 gradNorm: 18.89 \n",
      "2025-03-13 22:01:22,937: Train batch 2748: loss: 2.77 gradNorm: 15.04 \n",
      "2025-03-13 22:01:23,092: Train batch 2749: loss: 2.66 gradNorm: 18.20 \n",
      "2025-03-13 22:01:23,396: Train batch 2750: loss: 2.30 gradNorm: 17.88 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:01:23,588: Val batch: CER (t18.2025.01.15): 0.074\n",
      "2025-03-13 22:01:23,589: Val batch 2750: CER (avg): 0.074 \n",
      "2025-03-13 22:01:23,590: Batches since validation CER improved: 150\n",
      "2025-03-13 22:01:23,700: Train batch 2751: loss: 3.10 gradNorm: 18.30 \n",
      "2025-03-13 22:01:23,781: Train batch 2752: loss: 2.24 gradNorm: 18.31 \n",
      "2025-03-13 22:01:24,051: Train batch 2753: loss: 2.56 gradNorm: 17.86 \n",
      "2025-03-13 22:01:24,126: Train batch 2754: loss: 2.63 gradNorm: 16.71 \n",
      "2025-03-13 22:01:24,257: Train batch 2755: loss: 2.26 gradNorm: 13.42 \n",
      "2025-03-13 22:01:24,430: Train batch 2756: loss: 2.17 gradNorm: 20.68 \n",
      "2025-03-13 22:01:24,504: Train batch 2757: loss: 2.56 gradNorm: 18.42 \n",
      "2025-03-13 22:01:24,858: Train batch 2758: loss: 2.68 gradNorm: 19.25 \n",
      "2025-03-13 22:01:24,961: Train batch 2759: loss: 3.07 gradNorm: 19.23 \n",
      "2025-03-13 22:01:25,260: Train batch 2760: loss: 2.60 gradNorm: 19.64 \n",
      "2025-03-13 22:01:25,458: Train batch 2761: loss: 2.20 gradNorm: 16.70 \n",
      "2025-03-13 22:01:25,591: Train batch 2762: loss: 1.98 gradNorm: 14.95 \n",
      "2025-03-13 22:01:25,695: Train batch 2763: loss: 2.68 gradNorm: 15.63 \n",
      "2025-03-13 22:01:25,777: Train batch 2764: loss: 4.44 gradNorm: 21.61 \n",
      "2025-03-13 22:01:26,062: Train batch 2765: loss: 4.76 gradNorm: 26.49 \n",
      "2025-03-13 22:01:26,133: Train batch 2766: loss: 2.79 gradNorm: 18.49 \n",
      "2025-03-13 22:01:26,218: Train batch 2767: loss: 2.40 gradNorm: 15.73 \n",
      "2025-03-13 22:01:26,423: Train batch 2768: loss: 2.49 gradNorm: 19.69 \n",
      "2025-03-13 22:01:26,624: Train batch 2769: loss: 1.61 gradNorm: 14.56 \n",
      "2025-03-13 22:01:26,818: Train batch 2770: loss: 2.16 gradNorm: 18.24 \n",
      "2025-03-13 22:01:27,095: Train batch 2771: loss: 2.68 gradNorm: 19.61 \n",
      "2025-03-13 22:01:27,181: Train batch 2772: loss: 3.20 gradNorm: 20.77 \n",
      "2025-03-13 22:01:27,452: Train batch 2773: loss: 2.09 gradNorm: 18.91 \n",
      "2025-03-13 22:01:27,615: Train batch 2774: loss: 1.92 gradNorm: 23.68 \n",
      "2025-03-13 22:01:27,701: Train batch 2775: loss: 2.40 gradNorm: 17.71 \n",
      "2025-03-13 22:01:27,796: Train batch 2776: loss: 3.36 gradNorm: 19.99 \n",
      "2025-03-13 22:01:27,893: Train batch 2777: loss: 2.11 gradNorm: 13.98 \n",
      "2025-03-13 22:01:28,023: Train batch 2778: loss: 1.73 gradNorm: 14.89 \n",
      "2025-03-13 22:01:28,180: Train batch 2779: loss: 1.38 gradNorm: 15.48 \n",
      "2025-03-13 22:01:28,525: Train batch 2780: loss: 4.84 gradNorm: 32.32 \n",
      "2025-03-13 22:01:28,594: Train batch 2781: loss: 2.79 gradNorm: 18.36 \n",
      "2025-03-13 22:01:28,755: Train batch 2782: loss: 1.38 gradNorm: 18.61 \n",
      "2025-03-13 22:01:29,028: Train batch 2783: loss: 4.88 gradNorm: 33.11 \n",
      "2025-03-13 22:01:29,120: Train batch 2784: loss: 2.32 gradNorm: 20.66 \n",
      "2025-03-13 22:01:29,208: Train batch 2785: loss: 2.88 gradNorm: 16.94 \n",
      "2025-03-13 22:01:29,362: Train batch 2786: loss: 1.25 gradNorm: 14.94 \n",
      "2025-03-13 22:01:29,462: Train batch 2787: loss: 3.01 gradNorm: 21.09 \n",
      "2025-03-13 22:01:29,613: Train batch 2788: loss: 1.17 gradNorm: 12.05 \n",
      "2025-03-13 22:01:29,681: Train batch 2789: loss: 1.92 gradNorm: 15.09 \n",
      "2025-03-13 22:01:29,749: Train batch 2790: loss: 2.49 gradNorm: 16.79 \n",
      "2025-03-13 22:01:29,963: Train batch 2791: loss: 1.72 gradNorm: 14.74 \n",
      "2025-03-13 22:01:30,076: Train batch 2792: loss: 2.44 gradNorm: 14.52 \n",
      "2025-03-13 22:01:30,206: Train batch 2793: loss: 2.78 gradNorm: 19.09 \n",
      "2025-03-13 22:01:30,292: Train batch 2794: loss: 2.09 gradNorm: 15.37 \n",
      "2025-03-13 22:01:30,446: Train batch 2795: loss: 1.59 gradNorm: 15.08 \n",
      "2025-03-13 22:01:30,719: Train batch 2796: loss: 2.63 gradNorm: 23.79 \n",
      "2025-03-13 22:01:30,860: Train batch 2797: loss: 1.73 gradNorm: 15.40 \n",
      "2025-03-13 22:01:30,960: Train batch 2798: loss: 2.48 gradNorm: 17.83 \n",
      "2025-03-13 22:01:31,044: Train batch 2799: loss: 2.23 gradNorm: 17.34 \n",
      "2025-03-13 22:01:31,132: Train batch 2800: loss: 2.35 gradNorm: 15.63 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:01:31,341: Val batch: CER (t18.2025.01.15): 0.081\n",
      "2025-03-13 22:01:31,343: Val batch 2800: CER (avg): 0.081 \n",
      "2025-03-13 22:01:31,343: Batches since validation CER improved: 200\n",
      "2025-03-13 22:01:31,445: Train batch 2801: loss: 2.31 gradNorm: 14.55 \n",
      "2025-03-13 22:01:31,534: Train batch 2802: loss: 2.78 gradNorm: 17.44 \n",
      "2025-03-13 22:01:31,626: Train batch 2803: loss: 2.07 gradNorm: 13.64 \n",
      "2025-03-13 22:01:31,753: Train batch 2804: loss: 1.63 gradNorm: 14.27 \n",
      "2025-03-13 22:01:32,032: Train batch 2805: loss: 3.98 gradNorm: 25.92 \n",
      "2025-03-13 22:01:32,114: Train batch 2806: loss: 2.37 gradNorm: 16.23 \n",
      "2025-03-13 22:01:32,379: Train batch 2807: loss: 1.90 gradNorm: 19.53 \n",
      "2025-03-13 22:01:32,575: Train batch 2808: loss: 2.05 gradNorm: 15.72 \n",
      "2025-03-13 22:01:32,644: Train batch 2809: loss: 2.71 gradNorm: 16.49 \n",
      "2025-03-13 22:01:32,795: Train batch 2810: loss: 3.43 gradNorm: 21.32 \n",
      "2025-03-13 22:01:32,873: Train batch 2811: loss: 2.46 gradNorm: 13.94 \n",
      "2025-03-13 22:01:32,958: Train batch 2812: loss: 2.39 gradNorm: 16.27 \n",
      "2025-03-13 22:01:33,057: Train batch 2813: loss: 4.08 gradNorm: 18.73 \n",
      "2025-03-13 22:01:33,209: Train batch 2814: loss: 1.54 gradNorm: 14.49 \n",
      "2025-03-13 22:01:33,595: Train batch 2815: loss: 3.61 gradNorm: 25.07 \n",
      "2025-03-13 22:01:33,752: Train batch 2816: loss: 2.66 gradNorm: 20.74 \n",
      "2025-03-13 22:01:34,036: Train batch 2817: loss: 2.63 gradNorm: 19.66 \n",
      "2025-03-13 22:01:34,142: Train batch 2818: loss: 1.70 gradNorm: 12.19 \n",
      "2025-03-13 22:01:34,321: Train batch 2819: loss: 1.22 gradNorm: 19.55 \n",
      "2025-03-13 22:01:34,447: Train batch 2820: loss: 2.79 gradNorm: 17.46 \n",
      "2025-03-13 22:01:34,720: Train batch 2821: loss: 3.28 gradNorm: 21.72 \n",
      "2025-03-13 22:01:34,796: Train batch 2822: loss: 3.11 gradNorm: 18.94 \n",
      "2025-03-13 22:01:34,873: Train batch 2823: loss: 2.16 gradNorm: 15.03 \n",
      "2025-03-13 22:01:35,033: Train batch 2824: loss: 1.94 gradNorm: 19.30 \n",
      "2025-03-13 22:01:35,130: Train batch 2825: loss: 3.23 gradNorm: 19.94 \n",
      "2025-03-13 22:01:35,199: Train batch 2826: loss: 2.58 gradNorm: 17.18 \n",
      "2025-03-13 22:01:35,466: Train batch 2827: loss: 2.93 gradNorm: 23.36 \n",
      "2025-03-13 22:01:35,838: Train batch 2828: loss: 3.93 gradNorm: 19.97 \n",
      "2025-03-13 22:01:35,923: Train batch 2829: loss: 3.39 gradNorm: 19.80 \n",
      "2025-03-13 22:01:36,010: Train batch 2830: loss: 2.83 gradNorm: 21.49 \n",
      "2025-03-13 22:01:36,130: Train batch 2831: loss: 2.32 gradNorm: 14.71 \n",
      "2025-03-13 22:01:36,204: Train batch 2832: loss: 2.45 gradNorm: 16.03 \n",
      "2025-03-13 22:01:36,577: Train batch 2833: loss: 2.44 gradNorm: 19.25 \n",
      "2025-03-13 22:01:36,732: Train batch 2834: loss: 1.32 gradNorm: 11.21 \n",
      "2025-03-13 22:01:36,884: Train batch 2835: loss: 1.43 gradNorm: 14.78 \n",
      "2025-03-13 22:01:36,967: Train batch 2836: loss: 1.91 gradNorm: 15.55 \n",
      "2025-03-13 22:01:37,070: Train batch 2837: loss: 4.00 gradNorm: 21.56 \n",
      "2025-03-13 22:01:37,146: Train batch 2838: loss: 2.70 gradNorm: 20.47 \n",
      "2025-03-13 22:01:37,343: Train batch 2839: loss: 2.33 gradNorm: 16.03 \n",
      "2025-03-13 22:01:37,425: Train batch 2840: loss: 2.16 gradNorm: 14.85 \n",
      "2025-03-13 22:01:37,505: Train batch 2841: loss: 2.71 gradNorm: 20.11 \n",
      "2025-03-13 22:01:37,647: Train batch 2842: loss: 2.14 gradNorm: 15.58 \n",
      "2025-03-13 22:01:37,742: Train batch 2843: loss: 2.41 gradNorm: 18.77 \n",
      "2025-03-13 22:01:37,905: Train batch 2844: loss: 1.34 gradNorm: 16.48 \n",
      "2025-03-13 22:01:38,077: Train batch 2845: loss: 1.95 gradNorm: 17.85 \n",
      "2025-03-13 22:01:38,321: Train batch 2846: loss: 2.37 gradNorm: 20.34 \n",
      "2025-03-13 22:01:38,399: Train batch 2847: loss: 2.50 gradNorm: 18.27 \n",
      "2025-03-13 22:01:38,502: Train batch 2848: loss: 3.44 gradNorm: 19.30 \n",
      "2025-03-13 22:01:38,582: Train batch 2849: loss: 2.66 gradNorm: 21.14 \n",
      "2025-03-13 22:01:38,772: Train batch 2850: loss: 2.06 gradNorm: 16.10 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:01:38,984: Val batch: CER (t18.2025.01.15): 0.088\n",
      "2025-03-13 22:01:38,986: Val batch 2850: CER (avg): 0.088 \n",
      "2025-03-13 22:01:38,986: Batches since validation CER improved: 250\n",
      "2025-03-13 22:01:39,249: Train batch 2851: loss: 2.77 gradNorm: 19.70 \n",
      "2025-03-13 22:01:39,360: Train batch 2852: loss: 1.94 gradNorm: 16.75 \n",
      "2025-03-13 22:01:39,436: Train batch 2853: loss: 2.05 gradNorm: 17.34 \n",
      "2025-03-13 22:01:39,810: Train batch 2854: loss: 2.12 gradNorm: 17.00 \n",
      "2025-03-13 22:01:39,989: Train batch 2855: loss: 1.63 gradNorm: 16.46 \n",
      "2025-03-13 22:01:40,072: Train batch 2856: loss: 2.22 gradNorm: 17.03 \n",
      "2025-03-13 22:01:40,442: Train batch 2857: loss: 2.29 gradNorm: 18.73 \n",
      "2025-03-13 22:01:40,716: Train batch 2858: loss: 2.95 gradNorm: 20.68 \n",
      "2025-03-13 22:01:40,987: Train batch 2859: loss: 2.81 gradNorm: 18.56 \n",
      "2025-03-13 22:01:41,086: Train batch 2860: loss: 3.60 gradNorm: 22.06 \n",
      "2025-03-13 22:01:41,268: Train batch 2861: loss: 1.95 gradNorm: 16.17 \n",
      "2025-03-13 22:01:41,364: Train batch 2862: loss: 3.26 gradNorm: 30.66 \n",
      "2025-03-13 22:01:41,491: Train batch 2863: loss: 2.24 gradNorm: 18.99 \n",
      "2025-03-13 22:01:41,737: Train batch 2864: loss: 2.31 gradNorm: 21.90 \n",
      "2025-03-13 22:01:41,821: Train batch 2865: loss: 2.90 gradNorm: 17.41 \n",
      "2025-03-13 22:01:41,995: Train batch 2866: loss: 1.29 gradNorm: 13.05 \n",
      "2025-03-13 22:01:42,097: Train batch 2867: loss: 2.58 gradNorm: 17.20 \n",
      "2025-03-13 22:01:42,400: Train batch 2868: loss: 2.63 gradNorm: 22.19 \n",
      "2025-03-13 22:01:42,530: Train batch 2869: loss: 1.82 gradNorm: 14.71 \n",
      "2025-03-13 22:01:42,611: Train batch 2870: loss: 3.24 gradNorm: 21.81 \n",
      "2025-03-13 22:01:42,969: Train batch 2871: loss: 1.87 gradNorm: 16.27 \n",
      "2025-03-13 22:01:43,050: Train batch 2872: loss: 3.08 gradNorm: 15.54 \n",
      "2025-03-13 22:01:43,321: Train batch 2873: loss: 2.63 gradNorm: 19.48 \n",
      "2025-03-13 22:01:43,407: Train batch 2874: loss: 2.54 gradNorm: 14.97 \n",
      "2025-03-13 22:01:43,696: Train batch 2875: loss: 2.82 gradNorm: 37.90 \n",
      "2025-03-13 22:01:43,874: Train batch 2876: loss: 2.04 gradNorm: 16.80 \n",
      "2025-03-13 22:01:43,949: Train batch 2877: loss: 2.02 gradNorm: 12.74 \n",
      "2025-03-13 22:01:44,129: Train batch 2878: loss: 1.24 gradNorm: 12.58 \n",
      "2025-03-13 22:01:44,259: Train batch 2879: loss: 2.43 gradNorm: 14.67 \n",
      "2025-03-13 22:01:44,340: Train batch 2880: loss: 2.49 gradNorm: 15.99 \n",
      "2025-03-13 22:01:44,504: Train batch 2881: loss: 1.72 gradNorm: 14.83 \n",
      "2025-03-13 22:01:44,663: Train batch 2882: loss: 1.85 gradNorm: 14.81 \n",
      "2025-03-13 22:01:44,737: Train batch 2883: loss: 2.40 gradNorm: 16.95 \n",
      "2025-03-13 22:01:44,911: Train batch 2884: loss: 1.39 gradNorm: 11.93 \n",
      "2025-03-13 22:01:45,158: Train batch 2885: loss: 2.34 gradNorm: 15.66 \n",
      "2025-03-13 22:01:45,306: Train batch 2886: loss: 1.47 gradNorm: 15.42 \n",
      "2025-03-13 22:01:45,467: Train batch 2887: loss: 1.24 gradNorm: 24.90 \n",
      "2025-03-13 22:01:45,582: Train batch 2888: loss: 1.14 gradNorm: 15.24 \n",
      "2025-03-13 22:01:45,679: Train batch 2889: loss: 2.18 gradNorm: 15.02 \n",
      "2025-03-13 22:01:45,832: Train batch 2890: loss: 0.64 gradNorm: 9.08 \n",
      "2025-03-13 22:01:46,117: Train batch 2891: loss: 2.28 gradNorm: 19.83 \n",
      "2025-03-13 22:01:46,230: Train batch 2892: loss: 1.86 gradNorm: 17.12 \n",
      "2025-03-13 22:01:46,333: Train batch 2893: loss: 1.92 gradNorm: 23.00 \n",
      "2025-03-13 22:01:46,489: Train batch 2894: loss: 1.23 gradNorm: 15.92 \n",
      "2025-03-13 22:01:46,773: Train batch 2895: loss: 2.87 gradNorm: 23.54 \n",
      "2025-03-13 22:01:46,927: Train batch 2896: loss: 1.90 gradNorm: 14.14 \n",
      "2025-03-13 22:01:47,013: Train batch 2897: loss: 3.85 gradNorm: 24.28 \n",
      "2025-03-13 22:01:47,109: Train batch 2898: loss: 2.71 gradNorm: 16.63 \n",
      "2025-03-13 22:01:47,254: Train batch 2899: loss: 1.59 gradNorm: 12.99 \n",
      "2025-03-13 22:01:47,566: Train batch 2900: loss: 2.14 gradNorm: 20.39 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:01:47,766: Val batch: CER (t18.2025.01.15): 0.072\n",
      "2025-03-13 22:01:47,768: Val batch 2900: CER (avg): 0.072 \n",
      "2025-03-13 22:01:47,768: Batches since validation CER improved: 300\n",
      "2025-03-13 22:01:47,880: Train batch 2901: loss: 2.96 gradNorm: 15.97 \n",
      "2025-03-13 22:01:48,127: Train batch 2902: loss: 2.39 gradNorm: 23.11 \n",
      "2025-03-13 22:01:48,290: Train batch 2903: loss: 1.70 gradNorm: 16.17 \n",
      "2025-03-13 22:01:48,372: Train batch 2904: loss: 2.85 gradNorm: 16.92 \n",
      "2025-03-13 22:01:48,471: Train batch 2905: loss: 3.06 gradNorm: 16.85 \n",
      "2025-03-13 22:01:48,667: Train batch 2906: loss: 1.60 gradNorm: 12.86 \n",
      "2025-03-13 22:01:48,766: Train batch 2907: loss: 2.74 gradNorm: 17.08 \n",
      "2025-03-13 22:01:48,846: Train batch 2908: loss: 2.10 gradNorm: 14.92 \n",
      "2025-03-13 22:01:48,936: Train batch 2909: loss: 2.18 gradNorm: 16.58 \n",
      "2025-03-13 22:01:49,019: Train batch 2910: loss: 2.19 gradNorm: 19.45 \n",
      "2025-03-13 22:01:49,321: Train batch 2911: loss: 2.61 gradNorm: 19.57 \n",
      "2025-03-13 22:01:49,592: Train batch 2912: loss: 1.85 gradNorm: 14.19 \n",
      "2025-03-13 22:01:49,863: Train batch 2913: loss: 2.44 gradNorm: 24.43 \n",
      "2025-03-13 22:01:49,938: Train batch 2914: loss: 2.68 gradNorm: 17.43 \n",
      "2025-03-13 22:01:50,290: Train batch 2915: loss: 2.87 gradNorm: 21.27 \n",
      "2025-03-13 22:01:50,371: Train batch 2916: loss: 3.04 gradNorm: 17.79 \n",
      "2025-03-13 22:01:50,528: Train batch 2917: loss: 1.60 gradNorm: 13.03 \n",
      "2025-03-13 22:01:50,857: Train batch 2918: loss: 2.18 gradNorm: 17.58 \n",
      "2025-03-13 22:01:51,035: Train batch 2919: loss: 2.03 gradNorm: 14.55 \n",
      "2025-03-13 22:01:51,231: Train batch 2920: loss: 1.46 gradNorm: 18.11 \n",
      "2025-03-13 22:01:51,489: Train batch 2921: loss: 2.33 gradNorm: 16.96 \n",
      "2025-03-13 22:01:51,660: Train batch 2922: loss: 1.36 gradNorm: 17.12 \n",
      "2025-03-13 22:01:51,816: Train batch 2923: loss: 3.63 gradNorm: 20.33 \n",
      "2025-03-13 22:01:51,908: Train batch 2924: loss: 2.54 gradNorm: 17.16 \n",
      "2025-03-13 22:01:52,020: Train batch 2925: loss: 2.28 gradNorm: 16.63 \n",
      "2025-03-13 22:01:52,166: Train batch 2926: loss: 1.18 gradNorm: 13.42 \n",
      "2025-03-13 22:01:52,386: Train batch 2927: loss: 2.91 gradNorm: 22.73 \n",
      "2025-03-13 22:01:52,547: Train batch 2928: loss: 2.28 gradNorm: 16.87 \n",
      "2025-03-13 22:01:52,626: Train batch 2929: loss: 2.42 gradNorm: 14.86 \n",
      "2025-03-13 22:01:52,725: Train batch 2930: loss: 3.53 gradNorm: 21.53 \n",
      "2025-03-13 22:01:53,075: Train batch 2931: loss: 2.92 gradNorm: 22.51 \n",
      "2025-03-13 22:01:53,152: Train batch 2932: loss: 2.36 gradNorm: 16.30 \n",
      "2025-03-13 22:01:53,429: Train batch 2933: loss: 2.48 gradNorm: 18.16 \n",
      "2025-03-13 22:01:53,701: Train batch 2934: loss: 2.20 gradNorm: 20.04 \n",
      "2025-03-13 22:01:53,943: Train batch 2935: loss: 2.22 gradNorm: 16.81 \n",
      "2025-03-13 22:01:54,027: Train batch 2936: loss: 2.58 gradNorm: 15.56 \n",
      "2025-03-13 22:01:54,416: Train batch 2937: loss: 2.48 gradNorm: 18.71 \n",
      "2025-03-13 22:01:54,480: Train batch 2938: loss: 2.41 gradNorm: 19.12 \n",
      "2025-03-13 22:01:54,600: Train batch 2939: loss: 3.05 gradNorm: 16.29 \n",
      "2025-03-13 22:01:54,897: Train batch 2940: loss: 1.80 gradNorm: 15.04 \n",
      "2025-03-13 22:01:55,054: Train batch 2941: loss: 1.03 gradNorm: 11.45 \n",
      "2025-03-13 22:01:55,180: Train batch 2942: loss: 1.85 gradNorm: 16.17 \n",
      "2025-03-13 22:01:55,357: Train batch 2943: loss: 2.34 gradNorm: 18.04 \n",
      "2025-03-13 22:01:55,604: Train batch 2944: loss: 2.19 gradNorm: 26.62 \n",
      "2025-03-13 22:01:55,874: Train batch 2945: loss: 3.03 gradNorm: 19.88 \n",
      "2025-03-13 22:01:55,998: Train batch 2946: loss: 4.38 gradNorm: 23.61 \n",
      "2025-03-13 22:01:56,194: Train batch 2947: loss: 1.60 gradNorm: 15.37 \n",
      "2025-03-13 22:01:56,275: Train batch 2948: loss: 3.37 gradNorm: 19.67 \n",
      "2025-03-13 22:01:56,458: Train batch 2949: loss: 1.29 gradNorm: 12.65 \n",
      "2025-03-13 22:01:56,601: Train batch 2950: loss: 1.49 gradNorm: 14.02 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:01:56,832: Val batch: CER (t18.2025.01.15): 0.081\n",
      "2025-03-13 22:01:56,834: Val batch 2950: CER (avg): 0.081 \n",
      "2025-03-13 22:01:56,834: Batches since validation CER improved: 350\n",
      "2025-03-13 22:01:56,899: Train batch 2951: loss: 2.44 gradNorm: 16.13 \n",
      "2025-03-13 22:01:56,990: Train batch 2952: loss: 3.47 gradNorm: 21.67 \n",
      "2025-03-13 22:01:57,118: Train batch 2953: loss: 1.45 gradNorm: 13.71 \n",
      "2025-03-13 22:01:57,487: Train batch 2954: loss: 3.07 gradNorm: 24.61 \n",
      "2025-03-13 22:01:57,575: Train batch 2955: loss: 2.62 gradNorm: 21.45 \n",
      "2025-03-13 22:01:57,932: Train batch 2956: loss: 1.74 gradNorm: 14.78 \n",
      "2025-03-13 22:01:58,084: Train batch 2957: loss: 1.56 gradNorm: 16.56 \n",
      "2025-03-13 22:01:58,280: Train batch 2958: loss: 1.73 gradNorm: 20.42 \n",
      "2025-03-13 22:01:58,351: Train batch 2959: loss: 2.91 gradNorm: 20.62 \n",
      "2025-03-13 22:01:58,448: Train batch 2960: loss: 3.13 gradNorm: 18.69 \n",
      "2025-03-13 22:01:58,604: Train batch 2961: loss: 1.33 gradNorm: 12.68 \n",
      "2025-03-13 22:01:58,896: Train batch 2962: loss: 3.29 gradNorm: 28.17 \n",
      "2025-03-13 22:01:58,999: Train batch 2963: loss: 2.79 gradNorm: 18.06 \n",
      "2025-03-13 22:01:59,073: Train batch 2964: loss: 2.63 gradNorm: 17.13 \n",
      "2025-03-13 22:01:59,163: Train batch 2965: loss: 3.49 gradNorm: 22.92 \n",
      "2025-03-13 22:01:59,359: Train batch 2966: loss: 1.75 gradNorm: 14.68 \n",
      "2025-03-13 22:01:59,513: Train batch 2967: loss: 1.74 gradNorm: 16.08 \n",
      "2025-03-13 22:01:59,719: Train batch 2968: loss: 2.26 gradNorm: 21.48 \n",
      "2025-03-13 22:01:59,837: Train batch 2969: loss: 2.21 gradNorm: 17.13 \n",
      "2025-03-13 22:02:00,003: Train batch 2970: loss: 1.91 gradNorm: 19.04 \n",
      "2025-03-13 22:02:00,091: Train batch 2971: loss: 2.18 gradNorm: 16.25 \n",
      "2025-03-13 22:02:00,321: Train batch 2972: loss: 2.05 gradNorm: 17.66 \n",
      "2025-03-13 22:02:00,448: Train batch 2973: loss: 1.26 gradNorm: 14.60 \n",
      "2025-03-13 22:02:00,535: Train batch 2974: loss: 2.61 gradNorm: 17.46 \n",
      "2025-03-13 22:02:00,681: Train batch 2975: loss: 1.21 gradNorm: 11.25 \n",
      "2025-03-13 22:02:00,816: Train batch 2976: loss: 1.17 gradNorm: 15.98 \n",
      "2025-03-13 22:02:00,904: Train batch 2977: loss: 3.33 gradNorm: 25.20 \n",
      "2025-03-13 22:02:01,055: Train batch 2978: loss: 0.91 gradNorm: 9.14 \n",
      "2025-03-13 22:02:01,209: Train batch 2979: loss: 1.48 gradNorm: 11.87 \n",
      "2025-03-13 22:02:01,412: Train batch 2980: loss: 1.87 gradNorm: 18.28 \n",
      "2025-03-13 22:02:01,768: Train batch 2981: loss: 3.43 gradNorm: 20.46 \n",
      "2025-03-13 22:02:01,848: Train batch 2982: loss: 1.67 gradNorm: 13.66 \n",
      "2025-03-13 22:02:02,146: Train batch 2983: loss: 2.12 gradNorm: 18.10 \n",
      "2025-03-13 22:02:02,303: Train batch 2984: loss: 1.44 gradNorm: 15.42 \n",
      "2025-03-13 22:02:02,383: Train batch 2985: loss: 2.50 gradNorm: 24.68 \n",
      "2025-03-13 22:02:02,459: Train batch 2986: loss: 2.15 gradNorm: 16.00 \n",
      "2025-03-13 22:02:02,530: Train batch 2987: loss: 2.36 gradNorm: 15.33 \n",
      "2025-03-13 22:02:02,643: Train batch 2988: loss: 2.44 gradNorm: 19.84 \n",
      "2025-03-13 22:02:02,794: Train batch 2989: loss: 1.69 gradNorm: 13.40 \n",
      "2025-03-13 22:02:02,909: Train batch 2990: loss: 1.88 gradNorm: 15.14 \n",
      "2025-03-13 22:02:02,992: Train batch 2991: loss: 3.65 gradNorm: 20.98 \n",
      "2025-03-13 22:02:03,071: Train batch 2992: loss: 2.11 gradNorm: 15.54 \n",
      "2025-03-13 22:02:03,141: Train batch 2993: loss: 2.19 gradNorm: 15.77 \n",
      "2025-03-13 22:02:03,274: Train batch 2994: loss: 1.73 gradNorm: 14.13 \n",
      "2025-03-13 22:02:03,372: Train batch 2995: loss: 3.41 gradNorm: 20.30 \n",
      "2025-03-13 22:02:03,542: Train batch 2996: loss: 1.35 gradNorm: 9.94 \n",
      "2025-03-13 22:02:03,633: Train batch 2997: loss: 2.17 gradNorm: 16.29 \n",
      "2025-03-13 22:02:03,753: Train batch 2998: loss: 2.52 gradNorm: 19.18 \n",
      "2025-03-13 22:02:03,913: Train batch 2999: loss: 1.20 gradNorm: 13.75 \n",
      "2025-03-13 22:02:04,072: Train batch 3000: loss: 1.18 gradNorm: 13.56 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:02:04,263: Val batch: CER (t18.2025.01.15): 0.077\n",
      "2025-03-13 22:02:04,265: Val batch 3000: CER (avg): 0.077 \n",
      "2025-03-13 22:02:04,265: Batches since validation CER improved: 400\n",
      "2025-03-13 22:02:04,355: Train batch 3001: loss: 1.84 gradNorm: 15.72 \n",
      "2025-03-13 22:02:04,530: Train batch 3002: loss: 2.58 gradNorm: 19.25 \n",
      "2025-03-13 22:02:04,700: Train batch 3003: loss: 1.43 gradNorm: 18.62 \n",
      "2025-03-13 22:02:04,857: Train batch 3004: loss: 1.80 gradNorm: 13.74 \n",
      "2025-03-13 22:02:04,938: Train batch 3005: loss: 3.88 gradNorm: 24.38 \n",
      "2025-03-13 22:02:05,325: Train batch 3006: loss: 2.32 gradNorm: 21.09 \n",
      "2025-03-13 22:02:05,412: Train batch 3007: loss: 1.89 gradNorm: 11.19 \n",
      "2025-03-13 22:02:05,490: Train batch 3008: loss: 2.15 gradNorm: 13.81 \n",
      "2025-03-13 22:02:05,657: Train batch 3009: loss: 0.93 gradNorm: 11.45 \n",
      "2025-03-13 22:02:05,747: Train batch 3010: loss: 2.63 gradNorm: 17.09 \n",
      "2025-03-13 22:02:05,950: Train batch 3011: loss: 2.52 gradNorm: 17.78 \n",
      "2025-03-13 22:02:06,118: Train batch 3012: loss: 1.88 gradNorm: 16.48 \n",
      "2025-03-13 22:02:06,298: Train batch 3013: loss: 1.30 gradNorm: 13.21 \n",
      "2025-03-13 22:02:06,378: Train batch 3014: loss: 2.03 gradNorm: 16.30 \n",
      "2025-03-13 22:02:06,447: Train batch 3015: loss: 2.79 gradNorm: 19.45 \n",
      "2025-03-13 22:02:06,550: Train batch 3016: loss: 2.91 gradNorm: 19.19 \n",
      "2025-03-13 22:02:06,647: Train batch 3017: loss: 2.83 gradNorm: 17.67 \n",
      "2025-03-13 22:02:06,760: Train batch 3018: loss: 2.11 gradNorm: 13.49 \n",
      "2025-03-13 22:02:06,946: Train batch 3019: loss: 1.85 gradNorm: 21.18 \n",
      "2025-03-13 22:02:07,087: Train batch 3020: loss: 2.24 gradNorm: 20.31 \n",
      "2025-03-13 22:02:07,197: Train batch 3021: loss: 1.31 gradNorm: 11.52 \n",
      "2025-03-13 22:02:07,327: Train batch 3022: loss: 1.46 gradNorm: 13.97 \n",
      "2025-03-13 22:02:07,412: Train batch 3023: loss: 3.39 gradNorm: 21.72 \n",
      "2025-03-13 22:02:07,492: Train batch 3024: loss: 1.97 gradNorm: 16.81 \n",
      "2025-03-13 22:02:07,747: Train batch 3025: loss: 2.07 gradNorm: 21.87 \n",
      "2025-03-13 22:02:07,828: Train batch 3026: loss: 2.05 gradNorm: 13.85 \n",
      "2025-03-13 22:02:07,922: Train batch 3027: loss: 3.36 gradNorm: 19.15 \n",
      "2025-03-13 22:02:08,003: Train batch 3028: loss: 3.11 gradNorm: 19.30 \n",
      "2025-03-13 22:02:08,210: Train batch 3029: loss: 1.55 gradNorm: 13.92 \n",
      "2025-03-13 22:02:08,291: Train batch 3030: loss: 2.05 gradNorm: 16.97 \n",
      "2025-03-13 22:02:08,561: Train batch 3031: loss: 1.93 gradNorm: 16.22 \n",
      "2025-03-13 22:02:08,734: Train batch 3032: loss: 1.21 gradNorm: 11.78 \n",
      "2025-03-13 22:02:08,834: Train batch 3033: loss: 2.62 gradNorm: 17.61 \n",
      "2025-03-13 22:02:08,921: Train batch 3034: loss: 1.83 gradNorm: 13.61 \n",
      "2025-03-13 22:02:08,991: Train batch 3035: loss: 2.13 gradNorm: 15.41 \n",
      "2025-03-13 22:02:09,072: Train batch 3036: loss: 2.40 gradNorm: 15.66 \n",
      "2025-03-13 22:02:09,459: Train batch 3037: loss: 1.89 gradNorm: 18.08 \n",
      "2025-03-13 22:02:09,661: Train batch 3038: loss: 1.88 gradNorm: 14.21 \n",
      "2025-03-13 22:02:10,001: Train batch 3039: loss: 3.14 gradNorm: 21.52 \n",
      "2025-03-13 22:02:10,075: Train batch 3040: loss: 2.00 gradNorm: 14.79 \n",
      "2025-03-13 22:02:10,172: Train batch 3041: loss: 2.95 gradNorm: 16.91 \n",
      "2025-03-13 22:02:10,261: Train batch 3042: loss: 2.74 gradNorm: 15.91 \n",
      "2025-03-13 22:02:10,379: Train batch 3043: loss: 2.08 gradNorm: 15.14 \n",
      "2025-03-13 22:02:10,459: Train batch 3044: loss: 2.08 gradNorm: 19.01 \n",
      "2025-03-13 22:02:10,809: Train batch 3045: loss: 3.18 gradNorm: 23.13 \n",
      "2025-03-13 22:02:11,107: Train batch 3046: loss: 2.39 gradNorm: 19.18 \n",
      "2025-03-13 22:02:11,187: Train batch 3047: loss: 2.09 gradNorm: 13.17 \n",
      "2025-03-13 22:02:11,283: Train batch 3048: loss: 2.11 gradNorm: 15.25 \n",
      "2025-03-13 22:02:11,357: Train batch 3049: loss: 1.71 gradNorm: 12.90 \n",
      "2025-03-13 22:02:11,527: Train batch 3050: loss: 1.65 gradNorm: 15.19 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:02:11,730: Val batch: CER (t18.2025.01.15): 0.080\n",
      "2025-03-13 22:02:11,731: Val batch 3050: CER (avg): 0.080 \n",
      "2025-03-13 22:02:11,732: Batches since validation CER improved: 450\n",
      "2025-03-13 22:02:11,832: Train batch 3051: loss: 2.23 gradNorm: 15.22 \n",
      "2025-03-13 22:02:11,911: Train batch 3052: loss: 1.49 gradNorm: 13.83 \n",
      "2025-03-13 22:02:12,038: Train batch 3053: loss: 1.94 gradNorm: 14.44 \n",
      "2025-03-13 22:02:12,125: Train batch 3054: loss: 2.47 gradNorm: 17.58 \n",
      "2025-03-13 22:02:12,202: Train batch 3055: loss: 2.16 gradNorm: 16.36 \n",
      "2025-03-13 22:02:12,278: Train batch 3056: loss: 2.07 gradNorm: 16.42 \n",
      "2025-03-13 22:02:12,347: Train batch 3057: loss: 1.79 gradNorm: 26.17 \n",
      "2025-03-13 22:02:12,454: Train batch 3058: loss: 2.74 gradNorm: 19.30 \n",
      "2025-03-13 22:02:12,542: Train batch 3059: loss: 2.52 gradNorm: 19.15 \n",
      "2025-03-13 22:02:12,627: Train batch 3060: loss: 1.75 gradNorm: 15.38 \n",
      "2025-03-13 22:02:12,705: Train batch 3061: loss: 2.39 gradNorm: 17.98 \n",
      "2025-03-13 22:02:13,061: Train batch 3062: loss: 4.14 gradNorm: 33.99 \n",
      "2025-03-13 22:02:13,327: Train batch 3063: loss: 2.83 gradNorm: 19.43 \n",
      "2025-03-13 22:02:13,483: Train batch 3064: loss: 1.38 gradNorm: 26.72 \n",
      "2025-03-13 22:02:13,758: Train batch 3065: loss: 1.77 gradNorm: 18.40 \n",
      "2025-03-13 22:02:13,857: Train batch 3066: loss: 2.93 gradNorm: 17.35 \n",
      "2025-03-13 22:02:14,150: Train batch 3067: loss: 2.36 gradNorm: 17.84 \n",
      "2025-03-13 22:02:14,245: Train batch 3068: loss: 2.67 gradNorm: 20.24 \n",
      "2025-03-13 22:02:14,399: Train batch 3069: loss: 1.01 gradNorm: 13.89 \n",
      "2025-03-13 22:02:14,760: Train batch 3070: loss: 2.51 gradNorm: 23.59 \n",
      "2025-03-13 22:02:14,923: Train batch 3071: loss: 2.41 gradNorm: 18.43 \n",
      "2025-03-13 22:02:15,228: Train batch 3072: loss: 2.05 gradNorm: 20.10 \n",
      "2025-03-13 22:02:15,495: Train batch 3073: loss: 2.31 gradNorm: 22.52 \n",
      "2025-03-13 22:02:15,654: Train batch 3074: loss: 0.95 gradNorm: 16.18 \n",
      "2025-03-13 22:02:15,732: Train batch 3075: loss: 2.71 gradNorm: 21.94 \n",
      "2025-03-13 22:02:16,018: Train batch 3076: loss: 2.10 gradNorm: 18.50 \n",
      "2025-03-13 22:02:16,088: Train batch 3077: loss: 2.60 gradNorm: 20.65 \n",
      "2025-03-13 22:02:16,210: Train batch 3078: loss: 2.37 gradNorm: 19.82 \n",
      "2025-03-13 22:02:16,290: Train batch 3079: loss: 3.51 gradNorm: 21.60 \n",
      "2025-03-13 22:02:16,479: Train batch 3080: loss: 2.22 gradNorm: 18.53 \n",
      "2025-03-13 22:02:16,753: Train batch 3081: loss: 3.02 gradNorm: 24.37 \n",
      "2025-03-13 22:02:16,828: Train batch 3082: loss: 1.97 gradNorm: 15.09 \n",
      "2025-03-13 22:02:16,999: Train batch 3083: loss: 1.98 gradNorm: 16.82 \n",
      "2025-03-13 22:02:17,135: Train batch 3084: loss: 2.43 gradNorm: 19.92 \n",
      "2025-03-13 22:02:17,288: Train batch 3085: loss: 1.49 gradNorm: 13.30 \n",
      "2025-03-13 22:02:17,368: Train batch 3086: loss: 1.85 gradNorm: 16.67 \n",
      "2025-03-13 22:02:17,520: Train batch 3087: loss: 0.81 gradNorm: 8.43 \n",
      "2025-03-13 22:02:17,790: Train batch 3088: loss: 1.84 gradNorm: 21.63 \n",
      "2025-03-13 22:02:18,033: Train batch 3089: loss: 3.24 gradNorm: 23.08 \n",
      "2025-03-13 22:02:18,122: Train batch 3090: loss: 2.95 gradNorm: 19.09 \n",
      "2025-03-13 22:02:18,224: Train batch 3091: loss: 1.51 gradNorm: 12.81 \n",
      "2025-03-13 22:02:18,350: Train batch 3092: loss: 1.09 gradNorm: 12.59 \n",
      "2025-03-13 22:02:18,519: Train batch 3093: loss: 1.33 gradNorm: 12.16 \n",
      "2025-03-13 22:02:18,821: Train batch 3094: loss: 1.60 gradNorm: 15.12 \n",
      "2025-03-13 22:02:18,932: Train batch 3095: loss: 2.08 gradNorm: 16.11 \n",
      "2025-03-13 22:02:19,313: Train batch 3096: loss: 2.42 gradNorm: 19.63 \n",
      "2025-03-13 22:02:19,424: Train batch 3097: loss: 1.83 gradNorm: 20.01 \n",
      "2025-03-13 22:02:19,531: Train batch 3098: loss: 1.52 gradNorm: 11.09 \n",
      "2025-03-13 22:02:19,614: Train batch 3099: loss: 2.15 gradNorm: 18.32 \n",
      "2025-03-13 22:02:19,696: Train batch 3100: loss: 1.70 gradNorm: 12.03 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:02:19,905: Val batch: CER (t18.2025.01.15): 0.085\n",
      "2025-03-13 22:02:19,907: Val batch 3100: CER (avg): 0.085 \n",
      "2025-03-13 22:02:19,907: Batches since validation CER improved: 500\n",
      "2025-03-13 22:02:20,142: Train batch 3101: loss: 2.08 gradNorm: 20.34 \n",
      "2025-03-13 22:02:20,500: Train batch 3102: loss: 2.08 gradNorm: 18.31 \n",
      "2025-03-13 22:02:20,660: Train batch 3103: loss: 1.62 gradNorm: 13.50 \n",
      "2025-03-13 22:02:20,742: Train batch 3104: loss: 2.80 gradNorm: 16.56 \n",
      "2025-03-13 22:02:20,831: Train batch 3105: loss: 1.76 gradNorm: 13.84 \n",
      "2025-03-13 22:02:20,933: Train batch 3106: loss: 2.88 gradNorm: 19.31 \n",
      "2025-03-13 22:02:21,003: Train batch 3107: loss: 1.90 gradNorm: 18.57 \n",
      "2025-03-13 22:02:21,153: Train batch 3108: loss: 1.51 gradNorm: 12.16 \n",
      "2025-03-13 22:02:21,426: Train batch 3109: loss: 3.18 gradNorm: 22.34 \n",
      "2025-03-13 22:02:21,506: Train batch 3110: loss: 1.62 gradNorm: 14.02 \n",
      "2025-03-13 22:02:21,798: Train batch 3111: loss: 2.17 gradNorm: 17.03 \n",
      "2025-03-13 22:02:22,148: Train batch 3112: loss: 1.49 gradNorm: 16.45 \n",
      "2025-03-13 22:02:22,236: Train batch 3113: loss: 2.44 gradNorm: 17.03 \n",
      "2025-03-13 22:02:22,393: Train batch 3114: loss: 1.11 gradNorm: 10.12 \n",
      "2025-03-13 22:02:22,461: Train batch 3115: loss: 1.82 gradNorm: 14.33 \n",
      "2025-03-13 22:02:22,543: Train batch 3116: loss: 1.63 gradNorm: 12.98 \n",
      "2025-03-13 22:02:22,703: Train batch 3117: loss: 0.87 gradNorm: 10.09 \n",
      "2025-03-13 22:02:22,948: Train batch 3118: loss: 2.77 gradNorm: 24.65 \n",
      "2025-03-13 22:02:23,056: Train batch 3119: loss: 3.02 gradNorm: 19.97 \n",
      "2025-03-13 22:02:23,282: Train batch 3120: loss: 2.67 gradNorm: 19.94 \n",
      "2025-03-13 22:02:23,369: Train batch 3121: loss: 2.62 gradNorm: 18.67 \n",
      "2025-03-13 22:02:23,535: Train batch 3122: loss: 1.42 gradNorm: 15.66 \n",
      "2025-03-13 22:02:23,839: Train batch 3123: loss: 2.24 gradNorm: 17.58 \n",
      "2025-03-13 22:02:23,999: Train batch 3124: loss: 0.70 gradNorm: 9.18 \n",
      "2025-03-13 22:02:24,167: Train batch 3125: loss: 1.26 gradNorm: 12.91 \n",
      "2025-03-13 22:02:24,264: Train batch 3126: loss: 2.40 gradNorm: 15.78 \n",
      "2025-03-13 22:02:24,335: Train batch 3127: loss: 1.71 gradNorm: 13.49 \n",
      "2025-03-13 22:02:24,519: Train batch 3128: loss: 1.89 gradNorm: 16.36 \n",
      "2025-03-13 22:02:24,724: Train batch 3129: loss: 3.49 gradNorm: 25.27 \n",
      "2025-03-13 22:02:24,835: Train batch 3130: loss: 2.56 gradNorm: 17.14 \n",
      "2025-03-13 22:02:25,196: Train batch 3131: loss: 2.40 gradNorm: 21.68 \n",
      "2025-03-13 22:02:25,309: Train batch 3132: loss: 2.68 gradNorm: 20.04 \n",
      "2025-03-13 22:02:25,466: Train batch 3133: loss: 1.60 gradNorm: 16.30 \n",
      "2025-03-13 22:02:25,607: Train batch 3134: loss: 0.97 gradNorm: 15.97 \n",
      "2025-03-13 22:02:25,982: Train batch 3135: loss: 1.67 gradNorm: 15.36 \n",
      "2025-03-13 22:02:26,062: Train batch 3136: loss: 2.53 gradNorm: 18.15 \n",
      "2025-03-13 22:02:26,134: Train batch 3137: loss: 3.44 gradNorm: 21.16 \n",
      "2025-03-13 22:02:26,335: Train batch 3138: loss: 2.27 gradNorm: 21.68 \n",
      "2025-03-13 22:02:26,502: Train batch 3139: loss: 1.99 gradNorm: 20.65 \n",
      "2025-03-13 22:02:26,593: Train batch 3140: loss: 2.76 gradNorm: 19.17 \n",
      "2025-03-13 22:02:26,748: Train batch 3141: loss: 0.75 gradNorm: 9.31 \n",
      "2025-03-13 22:02:26,949: Train batch 3142: loss: 2.31 gradNorm: 19.94 \n",
      "2025-03-13 22:02:27,094: Train batch 3143: loss: 1.60 gradNorm: 12.47 \n",
      "2025-03-13 22:02:27,374: Train batch 3144: loss: 2.17 gradNorm: 19.82 \n",
      "2025-03-13 22:02:27,444: Train batch 3145: loss: 1.33 gradNorm: 13.63 \n",
      "2025-03-13 22:02:27,536: Train batch 3146: loss: 2.23 gradNorm: 17.12 \n",
      "2025-03-13 22:02:27,708: Train batch 3147: loss: 1.73 gradNorm: 21.04 \n",
      "2025-03-13 22:02:27,837: Train batch 3148: loss: 1.07 gradNorm: 11.47 \n",
      "2025-03-13 22:02:28,086: Train batch 3149: loss: 2.33 gradNorm: 16.71 \n",
      "2025-03-13 22:02:28,224: Train batch 3150: loss: 0.74 gradNorm: 10.18 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:02:28,417: Val batch: CER (t18.2025.01.15): 0.073\n",
      "2025-03-13 22:02:28,420: Val batch 3150: CER (avg): 0.073 \n",
      "2025-03-13 22:02:28,420: Batches since validation CER improved: 550\n",
      "2025-03-13 22:02:28,573: Train batch 3151: loss: 0.86 gradNorm: 10.24 \n",
      "2025-03-13 22:02:28,703: Train batch 3152: loss: 0.78 gradNorm: 9.74 \n",
      "2025-03-13 22:02:28,782: Train batch 3153: loss: 2.10 gradNorm: 13.62 \n",
      "2025-03-13 22:02:28,853: Train batch 3154: loss: 1.95 gradNorm: 12.84 \n",
      "2025-03-13 22:02:29,160: Train batch 3155: loss: 2.52 gradNorm: 19.73 \n",
      "2025-03-13 22:02:29,241: Train batch 3156: loss: 2.30 gradNorm: 19.10 \n",
      "2025-03-13 22:02:29,339: Train batch 3157: loss: 1.81 gradNorm: 14.16 \n",
      "2025-03-13 22:02:29,687: Train batch 3158: loss: 1.28 gradNorm: 12.15 \n",
      "2025-03-13 22:02:29,860: Train batch 3159: loss: 1.23 gradNorm: 11.50 \n",
      "2025-03-13 22:02:30,134: Train batch 3160: loss: 2.66 gradNorm: 19.20 \n",
      "2025-03-13 22:02:30,236: Train batch 3161: loss: 2.80 gradNorm: 17.42 \n",
      "2025-03-13 22:02:30,322: Train batch 3162: loss: 2.10 gradNorm: 16.53 \n",
      "2025-03-13 22:02:30,604: Train batch 3163: loss: 2.18 gradNorm: 17.77 \n",
      "2025-03-13 22:02:30,679: Train batch 3164: loss: 1.71 gradNorm: 13.28 \n",
      "2025-03-13 22:02:30,785: Train batch 3165: loss: 2.88 gradNorm: 18.48 \n",
      "2025-03-13 22:02:30,954: Train batch 3166: loss: 1.55 gradNorm: 14.33 \n",
      "2025-03-13 22:02:31,086: Train batch 3167: loss: 2.01 gradNorm: 14.29 \n",
      "2025-03-13 22:02:31,359: Train batch 3168: loss: 1.78 gradNorm: 21.17 \n",
      "2025-03-13 22:02:31,482: Train batch 3169: loss: 0.85 gradNorm: 9.74 \n",
      "2025-03-13 22:02:31,565: Train batch 3170: loss: 1.98 gradNorm: 13.03 \n",
      "2025-03-13 22:02:31,773: Train batch 3171: loss: 1.76 gradNorm: 19.32 \n",
      "2025-03-13 22:02:31,956: Train batch 3172: loss: 0.76 gradNorm: 8.57 \n",
      "2025-03-13 22:02:32,114: Train batch 3173: loss: 0.88 gradNorm: 14.58 \n",
      "2025-03-13 22:02:32,310: Train batch 3174: loss: 1.54 gradNorm: 16.43 \n",
      "2025-03-13 22:02:32,389: Train batch 3175: loss: 1.65 gradNorm: 20.56 \n",
      "2025-03-13 22:02:32,691: Train batch 3176: loss: 1.97 gradNorm: 23.05 \n",
      "2025-03-13 22:02:32,762: Train batch 3177: loss: 2.37 gradNorm: 20.92 \n",
      "2025-03-13 22:02:32,854: Train batch 3178: loss: 2.18 gradNorm: 16.22 \n",
      "2025-03-13 22:02:32,932: Train batch 3179: loss: 2.17 gradNorm: 15.72 \n",
      "2025-03-13 22:02:33,033: Train batch 3180: loss: 3.21 gradNorm: 23.31 \n",
      "2025-03-13 22:02:33,313: Train batch 3181: loss: 2.43 gradNorm: 18.60 \n",
      "2025-03-13 22:02:33,420: Train batch 3182: loss: 3.30 gradNorm: 23.69 \n",
      "2025-03-13 22:02:33,490: Train batch 3183: loss: 1.63 gradNorm: 13.64 \n",
      "2025-03-13 22:02:33,640: Train batch 3184: loss: 0.89 gradNorm: 9.53 \n",
      "2025-03-13 22:02:33,721: Train batch 3185: loss: 2.13 gradNorm: 15.48 \n",
      "2025-03-13 22:02:33,830: Train batch 3186: loss: 1.94 gradNorm: 14.19 \n",
      "2025-03-13 22:02:34,119: Train batch 3187: loss: 3.57 gradNorm: 26.51 \n",
      "2025-03-13 22:02:34,280: Train batch 3188: loss: 1.30 gradNorm: 13.04 \n",
      "2025-03-13 22:02:34,434: Train batch 3189: loss: 1.21 gradNorm: 12.22 \n",
      "2025-03-13 22:02:34,708: Train batch 3190: loss: 2.20 gradNorm: 18.68 \n",
      "2025-03-13 22:02:34,905: Train batch 3191: loss: 1.54 gradNorm: 27.91 \n",
      "2025-03-13 22:02:35,076: Train batch 3192: loss: 0.70 gradNorm: 8.62 \n",
      "2025-03-13 22:02:35,155: Train batch 3193: loss: 2.49 gradNorm: 17.10 \n",
      "2025-03-13 22:02:35,276: Train batch 3194: loss: 0.63 gradNorm: 9.81 \n",
      "2025-03-13 22:02:35,377: Train batch 3195: loss: 2.92 gradNorm: 18.38 \n",
      "2025-03-13 22:02:35,532: Train batch 3196: loss: 1.18 gradNorm: 11.08 \n",
      "2025-03-13 22:02:35,604: Train batch 3197: loss: 1.80 gradNorm: 13.42 \n",
      "2025-03-13 22:02:35,915: Train batch 3198: loss: 2.68 gradNorm: 19.00 \n",
      "2025-03-13 22:02:36,130: Train batch 3199: loss: 2.50 gradNorm: 21.50 \n",
      "2025-03-13 22:02:36,268: Train batch 3200: loss: 2.34 gradNorm: 19.08 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:02:36,488: Val batch: CER (t18.2025.01.15): 0.080\n",
      "2025-03-13 22:02:36,491: Val batch 3200: CER (avg): 0.080 \n",
      "2025-03-13 22:02:36,491: Batches since validation CER improved: 600\n",
      "2025-03-13 22:02:36,584: Train batch 3201: loss: 2.99 gradNorm: 19.61 \n",
      "2025-03-13 22:02:36,887: Train batch 3202: loss: 2.27 gradNorm: 20.63 \n",
      "2025-03-13 22:02:37,083: Train batch 3203: loss: 1.45 gradNorm: 16.16 \n",
      "2025-03-13 22:02:37,243: Train batch 3204: loss: 2.40 gradNorm: 17.74 \n",
      "2025-03-13 22:02:37,347: Train batch 3205: loss: 1.44 gradNorm: 13.13 \n",
      "2025-03-13 22:02:37,695: Train batch 3206: loss: 1.41 gradNorm: 12.59 \n",
      "2025-03-13 22:02:37,783: Train batch 3207: loss: 1.80 gradNorm: 14.79 \n",
      "2025-03-13 22:02:38,048: Train batch 3208: loss: 3.14 gradNorm: 22.28 \n",
      "2025-03-13 22:02:38,120: Train batch 3209: loss: 2.34 gradNorm: 19.13 \n",
      "2025-03-13 22:02:38,312: Train batch 3210: loss: 1.86 gradNorm: 15.64 \n",
      "2025-03-13 22:02:38,479: Train batch 3211: loss: 1.75 gradNorm: 15.56 \n",
      "2025-03-13 22:02:38,859: Train batch 3212: loss: 2.15 gradNorm: 18.98 \n",
      "2025-03-13 22:02:39,134: Train batch 3213: loss: 1.75 gradNorm: 15.04 \n",
      "2025-03-13 22:02:39,382: Train batch 3214: loss: 2.39 gradNorm: 20.88 \n",
      "2025-03-13 22:02:39,576: Train batch 3215: loss: 1.58 gradNorm: 14.18 \n",
      "2025-03-13 22:02:39,646: Train batch 3216: loss: 2.51 gradNorm: 18.95 \n",
      "2025-03-13 22:02:39,728: Train batch 3217: loss: 2.69 gradNorm: 22.41 \n",
      "2025-03-13 22:02:39,809: Train batch 3218: loss: 2.42 gradNorm: 16.98 \n",
      "2025-03-13 22:02:40,161: Train batch 3219: loss: 2.05 gradNorm: 16.81 \n",
      "2025-03-13 22:02:40,242: Train batch 3220: loss: 2.36 gradNorm: 18.54 \n",
      "2025-03-13 22:02:40,331: Train batch 3221: loss: 2.92 gradNorm: 17.60 \n",
      "2025-03-13 22:02:40,645: Train batch 3222: loss: 1.62 gradNorm: 15.60 \n",
      "2025-03-13 22:02:40,724: Train batch 3223: loss: 2.06 gradNorm: 16.47 \n",
      "2025-03-13 22:02:40,871: Train batch 3224: loss: 1.52 gradNorm: 19.74 \n",
      "2025-03-13 22:02:41,149: Train batch 3225: loss: 2.96 gradNorm: 26.66 \n",
      "2025-03-13 22:02:41,221: Train batch 3226: loss: 2.08 gradNorm: 17.18 \n",
      "2025-03-13 22:02:41,332: Train batch 3227: loss: 2.73 gradNorm: 16.43 \n",
      "2025-03-13 22:02:41,681: Train batch 3228: loss: 2.25 gradNorm: 23.78 \n",
      "2025-03-13 22:02:41,780: Train batch 3229: loss: 2.89 gradNorm: 19.27 \n",
      "2025-03-13 22:02:41,862: Train batch 3230: loss: 2.88 gradNorm: 21.29 \n",
      "2025-03-13 22:02:41,948: Train batch 3231: loss: 1.68 gradNorm: 13.00 \n",
      "2025-03-13 22:02:42,109: Train batch 3232: loss: 1.13 gradNorm: 17.79 \n",
      "2025-03-13 22:02:42,183: Train batch 3233: loss: 2.52 gradNorm: 17.11 \n",
      "2025-03-13 22:02:42,265: Train batch 3234: loss: 1.51 gradNorm: 22.82 \n",
      "2025-03-13 22:02:42,346: Train batch 3235: loss: 1.60 gradNorm: 13.20 \n",
      "2025-03-13 22:02:42,413: Train batch 3236: loss: 1.47 gradNorm: 13.12 \n",
      "2025-03-13 22:02:42,597: Train batch 3237: loss: 2.66 gradNorm: 23.63 \n",
      "2025-03-13 22:02:42,686: Train batch 3238: loss: 1.83 gradNorm: 11.95 \n",
      "2025-03-13 22:02:42,979: Train batch 3239: loss: 2.11 gradNorm: 17.92 \n",
      "2025-03-13 22:02:43,175: Train batch 3240: loss: 1.62 gradNorm: 17.10 \n",
      "2025-03-13 22:02:43,294: Train batch 3241: loss: 1.10 gradNorm: 10.26 \n",
      "2025-03-13 22:02:43,374: Train batch 3242: loss: 2.33 gradNorm: 21.36 \n",
      "2025-03-13 22:02:43,444: Train batch 3243: loss: 1.94 gradNorm: 16.18 \n",
      "2025-03-13 22:02:43,556: Train batch 3244: loss: 2.31 gradNorm: 16.37 \n",
      "2025-03-13 22:02:43,801: Train batch 3245: loss: 2.24 gradNorm: 20.53 \n",
      "2025-03-13 22:02:43,881: Train batch 3246: loss: 1.65 gradNorm: 17.11 \n",
      "2025-03-13 22:02:43,985: Train batch 3247: loss: 3.17 gradNorm: 24.42 \n",
      "2025-03-13 22:02:44,066: Train batch 3248: loss: 3.16 gradNorm: 21.70 \n",
      "2025-03-13 22:02:44,244: Train batch 3249: loss: 0.73 gradNorm: 9.08 \n",
      "2025-03-13 22:02:44,545: Train batch 3250: loss: 2.12 gradNorm: 19.55 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:02:44,737: Val batch: CER (t18.2025.01.15): 0.071\n",
      "2025-03-13 22:02:44,739: Val batch 3250: CER (avg): 0.071 \n",
      "2025-03-13 22:02:44,739: Batches since validation CER improved: 650\n",
      "2025-03-13 22:02:44,836: Train batch 3251: loss: 2.06 gradNorm: 14.53 \n",
      "2025-03-13 22:02:45,005: Train batch 3252: loss: 2.39 gradNorm: 18.81 \n",
      "2025-03-13 22:02:45,261: Train batch 3253: loss: 2.46 gradNorm: 18.66 \n",
      "2025-03-13 22:02:45,537: Train batch 3254: loss: 2.12 gradNorm: 20.35 \n",
      "2025-03-13 22:02:45,610: Train batch 3255: loss: 2.41 gradNorm: 19.83 \n",
      "2025-03-13 22:02:45,821: Train batch 3256: loss: 2.61 gradNorm: 32.35 \n",
      "2025-03-13 22:02:45,895: Train batch 3257: loss: 2.95 gradNorm: 22.81 \n",
      "2025-03-13 22:02:46,193: Train batch 3258: loss: 1.67 gradNorm: 15.71 \n",
      "2025-03-13 22:02:46,341: Train batch 3259: loss: 1.43 gradNorm: 16.99 \n",
      "2025-03-13 22:02:46,642: Train batch 3260: loss: 1.31 gradNorm: 13.43 \n",
      "2025-03-13 22:02:46,722: Train batch 3261: loss: 2.33 gradNorm: 16.68 \n",
      "2025-03-13 22:02:47,072: Train batch 3262: loss: 1.33 gradNorm: 13.18 \n",
      "2025-03-13 22:02:47,155: Train batch 3263: loss: 1.89 gradNorm: 15.86 \n",
      "2025-03-13 22:02:47,221: Train batch 3264: loss: 1.21 gradNorm: 14.72 \n",
      "2025-03-13 22:02:47,630: Train batch 3265: loss: 2.24 gradNorm: 19.93 \n",
      "2025-03-13 22:02:47,710: Train batch 3266: loss: 1.92 gradNorm: 15.32 \n",
      "2025-03-13 22:02:48,014: Train batch 3267: loss: 1.58 gradNorm: 14.77 \n",
      "2025-03-13 22:02:48,168: Train batch 3268: loss: 1.32 gradNorm: 10.31 \n",
      "2025-03-13 22:02:48,239: Train batch 3269: loss: 2.17 gradNorm: 17.97 \n",
      "2025-03-13 22:02:48,381: Train batch 3270: loss: 1.22 gradNorm: 16.73 \n",
      "2025-03-13 22:02:48,632: Train batch 3271: loss: 1.52 gradNorm: 15.71 \n",
      "2025-03-13 22:02:48,781: Train batch 3272: loss: 0.80 gradNorm: 10.48 \n",
      "2025-03-13 22:02:49,050: Train batch 3273: loss: 1.57 gradNorm: 12.73 \n",
      "2025-03-13 22:02:49,117: Train batch 3274: loss: 1.67 gradNorm: 13.99 \n",
      "2025-03-13 22:02:49,398: Train batch 3275: loss: 2.57 gradNorm: 20.21 \n",
      "2025-03-13 22:02:49,784: Train batch 3276: loss: 2.39 gradNorm: 19.16 \n",
      "2025-03-13 22:02:49,954: Train batch 3277: loss: 0.90 gradNorm: 12.11 \n",
      "2025-03-13 22:02:50,080: Train batch 3278: loss: 1.67 gradNorm: 12.36 \n",
      "2025-03-13 22:02:50,352: Train batch 3279: loss: 1.65 gradNorm: 16.28 \n",
      "2025-03-13 22:02:50,452: Train batch 3280: loss: 3.27 gradNorm: 21.71 \n",
      "2025-03-13 22:02:50,741: Train batch 3281: loss: 1.51 gradNorm: 15.49 \n",
      "2025-03-13 22:02:50,872: Train batch 3282: loss: 0.63 gradNorm: 8.04 \n",
      "2025-03-13 22:02:50,953: Train batch 3283: loss: 2.22 gradNorm: 18.78 \n",
      "2025-03-13 22:02:51,124: Train batch 3284: loss: 1.51 gradNorm: 15.48 \n",
      "2025-03-13 22:02:51,194: Train batch 3285: loss: 2.46 gradNorm: 14.91 \n",
      "2025-03-13 22:02:51,327: Train batch 3286: loss: 2.26 gradNorm: 15.60 \n",
      "2025-03-13 22:02:51,678: Train batch 3287: loss: 2.25 gradNorm: 17.23 \n",
      "2025-03-13 22:02:51,767: Train batch 3288: loss: 1.81 gradNorm: 18.17 \n",
      "2025-03-13 22:02:51,853: Train batch 3289: loss: 1.61 gradNorm: 14.38 \n",
      "2025-03-13 22:02:51,938: Train batch 3290: loss: 2.17 gradNorm: 16.67 \n",
      "2025-03-13 22:02:52,019: Train batch 3291: loss: 1.49 gradNorm: 13.49 \n",
      "2025-03-13 22:02:52,112: Train batch 3292: loss: 2.61 gradNorm: 17.52 \n",
      "2025-03-13 22:02:52,269: Train batch 3293: loss: 0.94 gradNorm: 12.17 \n",
      "2025-03-13 22:02:52,541: Train batch 3294: loss: 1.68 gradNorm: 19.26 \n",
      "2025-03-13 22:02:52,812: Train batch 3295: loss: 1.56 gradNorm: 20.31 \n",
      "2025-03-13 22:02:52,976: Train batch 3296: loss: 1.36 gradNorm: 15.37 \n",
      "2025-03-13 22:02:53,064: Train batch 3297: loss: 3.22 gradNorm: 22.10 \n",
      "2025-03-13 22:02:53,366: Train batch 3298: loss: 1.26 gradNorm: 11.87 \n",
      "2025-03-13 22:02:53,741: Train batch 3299: loss: 1.86 gradNorm: 21.27 \n",
      "2025-03-13 22:02:53,840: Train batch 3300: loss: 2.24 gradNorm: 18.54 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:02:54,079: Val batch: CER (t18.2025.01.15): 0.084\n",
      "2025-03-13 22:02:54,081: Val batch 3300: CER (avg): 0.084 \n",
      "2025-03-13 22:02:54,081: Batches since validation CER improved: 700\n",
      "2025-03-13 22:02:54,161: Train batch 3301: loss: 2.14 gradNorm: 15.46 \n",
      "2025-03-13 22:02:54,291: Train batch 3302: loss: 2.72 gradNorm: 16.82 \n",
      "2025-03-13 22:02:54,386: Train batch 3303: loss: 2.39 gradNorm: 17.14 \n",
      "2025-03-13 22:02:54,538: Train batch 3304: loss: 1.42 gradNorm: 16.95 \n",
      "2025-03-13 22:02:54,604: Train batch 3305: loss: 1.54 gradNorm: 14.59 \n",
      "2025-03-13 22:02:54,775: Train batch 3306: loss: 2.67 gradNorm: 20.80 \n",
      "2025-03-13 22:02:54,855: Train batch 3307: loss: 1.10 gradNorm: 10.58 \n",
      "2025-03-13 22:02:54,925: Train batch 3308: loss: 1.34 gradNorm: 13.78 \n",
      "2025-03-13 22:02:55,000: Train batch 3309: loss: 1.44 gradNorm: 12.93 \n",
      "2025-03-13 22:02:55,088: Train batch 3310: loss: 3.06 gradNorm: 20.03 \n",
      "2025-03-13 22:02:55,311: Train batch 3311: loss: 3.28 gradNorm: 30.22 \n",
      "2025-03-13 22:02:55,447: Train batch 3312: loss: 3.10 gradNorm: 21.08 \n",
      "2025-03-13 22:02:55,549: Train batch 3313: loss: 2.45 gradNorm: 16.94 \n",
      "2025-03-13 22:02:55,620: Train batch 3314: loss: 1.80 gradNorm: 19.34 \n",
      "2025-03-13 22:02:55,795: Train batch 3315: loss: 1.23 gradNorm: 10.57 \n",
      "2025-03-13 22:02:56,111: Train batch 3316: loss: 1.52 gradNorm: 13.57 \n",
      "2025-03-13 22:02:56,186: Train batch 3317: loss: 1.75 gradNorm: 16.04 \n",
      "2025-03-13 22:02:56,271: Train batch 3318: loss: 1.98 gradNorm: 15.23 \n",
      "2025-03-13 22:02:56,521: Train batch 3319: loss: 1.33 gradNorm: 13.70 \n",
      "2025-03-13 22:02:56,718: Train batch 3320: loss: 1.48 gradNorm: 15.10 \n",
      "2025-03-13 22:02:56,821: Train batch 3321: loss: 3.41 gradNorm: 25.13 \n",
      "2025-03-13 22:02:56,909: Train batch 3322: loss: 2.12 gradNorm: 16.07 \n",
      "2025-03-13 22:02:56,978: Train batch 3323: loss: 2.24 gradNorm: 21.51 \n",
      "2025-03-13 22:02:57,137: Train batch 3324: loss: 1.55 gradNorm: 22.21 \n",
      "2025-03-13 22:02:57,315: Train batch 3325: loss: 0.88 gradNorm: 10.16 \n",
      "2025-03-13 22:02:57,436: Train batch 3326: loss: 2.04 gradNorm: 17.00 \n",
      "2025-03-13 22:02:57,516: Train batch 3327: loss: 1.34 gradNorm: 13.41 \n",
      "2025-03-13 22:02:57,592: Train batch 3328: loss: 1.86 gradNorm: 13.55 \n",
      "2025-03-13 22:02:57,670: Train batch 3329: loss: 1.69 gradNorm: 15.32 \n",
      "2025-03-13 22:02:57,739: Train batch 3330: loss: 1.60 gradNorm: 15.59 \n",
      "2025-03-13 22:02:58,116: Train batch 3331: loss: 1.80 gradNorm: 21.60 \n",
      "2025-03-13 22:02:58,384: Train batch 3332: loss: 2.97 gradNorm: 19.57 \n",
      "2025-03-13 22:02:58,480: Train batch 3333: loss: 2.61 gradNorm: 25.80 \n",
      "2025-03-13 22:02:58,866: Train batch 3334: loss: 1.70 gradNorm: 14.76 \n",
      "2025-03-13 22:02:58,946: Train batch 3335: loss: 1.65 gradNorm: 13.82 \n",
      "2025-03-13 22:02:59,151: Train batch 3336: loss: 1.52 gradNorm: 20.73 \n",
      "2025-03-13 22:02:59,514: Train batch 3337: loss: 2.08 gradNorm: 17.63 \n",
      "2025-03-13 22:02:59,618: Train batch 3338: loss: 2.24 gradNorm: 14.83 \n",
      "2025-03-13 22:02:59,723: Train batch 3339: loss: 2.34 gradNorm: 16.29 \n",
      "2025-03-13 22:02:59,991: Train batch 3340: loss: 1.47 gradNorm: 14.37 \n",
      "2025-03-13 22:03:00,077: Train batch 3341: loss: 1.85 gradNorm: 16.01 \n",
      "2025-03-13 22:03:00,178: Train batch 3342: loss: 1.97 gradNorm: 15.10 \n",
      "2025-03-13 22:03:00,263: Train batch 3343: loss: 1.87 gradNorm: 15.66 \n",
      "2025-03-13 22:03:00,649: Train batch 3344: loss: 2.20 gradNorm: 20.48 \n",
      "2025-03-13 22:03:00,746: Train batch 3345: loss: 2.30 gradNorm: 16.90 \n",
      "2025-03-13 22:03:00,850: Train batch 3346: loss: 3.35 gradNorm: 22.50 \n",
      "2025-03-13 22:03:00,926: Train batch 3347: loss: 1.90 gradNorm: 16.08 \n",
      "2025-03-13 22:03:00,998: Train batch 3348: loss: 2.10 gradNorm: 19.54 \n",
      "2025-03-13 22:03:01,194: Train batch 3349: loss: 1.52 gradNorm: 11.73 \n",
      "2025-03-13 22:03:01,301: Train batch 3350: loss: 1.60 gradNorm: 12.56 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:03:01,499: Val batch: CER (t18.2025.01.15): 0.078\n",
      "2025-03-13 22:03:01,501: Val batch 3350: CER (avg): 0.078 \n",
      "2025-03-13 22:03:01,502: Batches since validation CER improved: 750\n",
      "2025-03-13 22:03:01,588: Train batch 3351: loss: 2.10 gradNorm: 17.19 \n",
      "2025-03-13 22:03:01,748: Train batch 3352: loss: 1.87 gradNorm: 18.01 \n",
      "2025-03-13 22:03:01,908: Train batch 3353: loss: 0.56 gradNorm: 11.75 \n",
      "2025-03-13 22:03:02,039: Train batch 3354: loss: 1.90 gradNorm: 15.14 \n",
      "2025-03-13 22:03:02,221: Train batch 3355: loss: 1.31 gradNorm: 14.32 \n",
      "2025-03-13 22:03:02,501: Train batch 3356: loss: 2.44 gradNorm: 17.53 \n",
      "2025-03-13 22:03:02,655: Train batch 3357: loss: 0.74 gradNorm: 8.63 \n",
      "2025-03-13 22:03:02,930: Train batch 3358: loss: 1.64 gradNorm: 16.48 \n",
      "2025-03-13 22:03:03,002: Train batch 3359: loss: 2.52 gradNorm: 20.04 \n",
      "2025-03-13 22:03:03,399: Train batch 3360: loss: 1.36 gradNorm: 17.60 \n",
      "2025-03-13 22:03:03,595: Train batch 3361: loss: 1.54 gradNorm: 14.63 \n",
      "2025-03-13 22:03:03,878: Train batch 3362: loss: 2.01 gradNorm: 27.79 \n",
      "2025-03-13 22:03:03,957: Train batch 3363: loss: 1.53 gradNorm: 13.03 \n",
      "2025-03-13 22:03:04,057: Train batch 3364: loss: 2.77 gradNorm: 17.23 \n",
      "2025-03-13 22:03:04,141: Train batch 3365: loss: 1.94 gradNorm: 16.21 \n",
      "2025-03-13 22:03:04,237: Train batch 3366: loss: 2.48 gradNorm: 19.63 \n",
      "2025-03-13 22:03:04,387: Train batch 3367: loss: 1.73 gradNorm: 17.49 \n",
      "2025-03-13 22:03:04,471: Train batch 3368: loss: 2.02 gradNorm: 19.73 \n",
      "2025-03-13 22:03:04,667: Train batch 3369: loss: 1.82 gradNorm: 19.18 \n",
      "2025-03-13 22:03:04,755: Train batch 3370: loss: 2.42 gradNorm: 18.54 \n",
      "2025-03-13 22:03:04,922: Train batch 3371: loss: 0.78 gradNorm: 12.01 \n",
      "2025-03-13 22:03:05,069: Train batch 3372: loss: 0.75 gradNorm: 8.24 \n",
      "2025-03-13 22:03:05,151: Train batch 3373: loss: 1.98 gradNorm: 19.02 \n",
      "2025-03-13 22:03:05,404: Train batch 3374: loss: 1.47 gradNorm: 13.88 \n",
      "2025-03-13 22:03:05,625: Train batch 3375: loss: 1.79 gradNorm: 19.21 \n",
      "2025-03-13 22:03:05,795: Train batch 3376: loss: 1.45 gradNorm: 15.10 \n",
      "2025-03-13 22:03:05,951: Train batch 3377: loss: 2.16 gradNorm: 16.01 \n",
      "2025-03-13 22:03:06,057: Train batch 3378: loss: 1.76 gradNorm: 12.74 \n",
      "2025-03-13 22:03:06,210: Train batch 3379: loss: 0.96 gradNorm: 11.26 \n",
      "2025-03-13 22:03:06,337: Train batch 3380: loss: 1.24 gradNorm: 14.64 \n",
      "2025-03-13 22:03:06,433: Train batch 3381: loss: 2.30 gradNorm: 19.44 \n",
      "2025-03-13 22:03:06,713: Train batch 3382: loss: 1.49 gradNorm: 17.40 \n",
      "2025-03-13 22:03:06,892: Train batch 3383: loss: 1.20 gradNorm: 11.24 \n",
      "2025-03-13 22:03:06,963: Train batch 3384: loss: 1.04 gradNorm: 11.78 \n",
      "2025-03-13 22:03:07,053: Train batch 3385: loss: 1.47 gradNorm: 12.58 \n",
      "2025-03-13 22:03:07,123: Train batch 3386: loss: 2.15 gradNorm: 17.13 \n",
      "2025-03-13 22:03:07,415: Train batch 3387: loss: 1.54 gradNorm: 15.40 \n",
      "2025-03-13 22:03:07,765: Train batch 3388: loss: 2.07 gradNorm: 14.98 \n",
      "2025-03-13 22:03:08,140: Train batch 3389: loss: 1.31 gradNorm: 12.89 \n",
      "2025-03-13 22:03:08,397: Train batch 3390: loss: 1.17 gradNorm: 11.53 \n",
      "2025-03-13 22:03:08,626: Train batch 3391: loss: 0.85 gradNorm: 8.90 \n",
      "2025-03-13 22:03:08,820: Train batch 3392: loss: 1.18 gradNorm: 14.62 \n",
      "2025-03-13 22:03:08,946: Train batch 3393: loss: 1.99 gradNorm: 24.59 \n",
      "2025-03-13 22:03:09,294: Train batch 3394: loss: 2.21 gradNorm: 20.98 \n",
      "2025-03-13 22:03:09,561: Train batch 3395: loss: 2.19 gradNorm: 24.34 \n",
      "2025-03-13 22:03:09,720: Train batch 3396: loss: 1.99 gradNorm: 20.24 \n",
      "2025-03-13 22:03:10,122: Train batch 3397: loss: 2.48 gradNorm: 22.44 \n",
      "2025-03-13 22:03:10,466: Train batch 3398: loss: 1.80 gradNorm: 21.47 \n",
      "2025-03-13 22:03:10,731: Train batch 3399: loss: 2.12 gradNorm: 20.40 \n",
      "2025-03-13 22:03:10,833: Train batch 3400: loss: 2.52 gradNorm: 20.40 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:03:11,025: Val batch: CER (t18.2025.01.15): 0.088\n",
      "2025-03-13 22:03:11,027: Val batch 3400: CER (avg): 0.088 \n",
      "2025-03-13 22:03:11,027: Batches since validation CER improved: 800\n",
      "2025-03-13 22:03:11,123: Train batch 3401: loss: 2.89 gradNorm: 20.67 \n",
      "2025-03-13 22:03:11,214: Train batch 3402: loss: 2.94 gradNorm: 22.32 \n",
      "2025-03-13 22:03:11,379: Train batch 3403: loss: 0.81 gradNorm: 14.38 \n",
      "2025-03-13 22:03:11,525: Train batch 3404: loss: 0.93 gradNorm: 10.70 \n",
      "2025-03-13 22:03:11,611: Train batch 3405: loss: 1.67 gradNorm: 13.62 \n",
      "2025-03-13 22:03:11,687: Train batch 3406: loss: 1.55 gradNorm: 12.91 \n",
      "2025-03-13 22:03:11,885: Train batch 3407: loss: 1.68 gradNorm: 17.46 \n",
      "2025-03-13 22:03:11,989: Train batch 3408: loss: 1.93 gradNorm: 17.31 \n",
      "2025-03-13 22:03:12,075: Train batch 3409: loss: 2.08 gradNorm: 18.67 \n",
      "2025-03-13 22:03:12,172: Train batch 3410: loss: 1.88 gradNorm: 14.91 \n",
      "2025-03-13 22:03:12,340: Train batch 3411: loss: 1.79 gradNorm: 16.39 \n",
      "2025-03-13 22:03:12,541: Train batch 3412: loss: 1.91 gradNorm: 19.04 \n",
      "2025-03-13 22:03:12,667: Train batch 3413: loss: 1.63 gradNorm: 15.32 \n",
      "2025-03-13 22:03:12,824: Train batch 3414: loss: 1.46 gradNorm: 17.32 \n",
      "2025-03-13 22:03:12,911: Train batch 3415: loss: 1.79 gradNorm: 17.41 \n",
      "2025-03-13 22:03:13,108: Train batch 3416: loss: 1.53 gradNorm: 15.96 \n",
      "2025-03-13 22:03:13,256: Train batch 3417: loss: 0.79 gradNorm: 8.73 \n",
      "2025-03-13 22:03:13,498: Train batch 3418: loss: 2.29 gradNorm: 27.06 \n",
      "2025-03-13 22:03:13,566: Train batch 3419: loss: 1.34 gradNorm: 10.24 \n",
      "2025-03-13 22:03:13,862: Train batch 3420: loss: 1.55 gradNorm: 17.57 \n",
      "2025-03-13 22:03:14,150: Train batch 3421: loss: 1.68 gradNorm: 17.77 \n",
      "2025-03-13 22:03:14,419: Train batch 3422: loss: 1.32 gradNorm: 13.03 \n",
      "2025-03-13 22:03:14,721: Train batch 3423: loss: 1.87 gradNorm: 15.47 \n",
      "2025-03-13 22:03:14,802: Train batch 3424: loss: 1.66 gradNorm: 12.36 \n",
      "2025-03-13 22:03:14,870: Train batch 3425: loss: 3.39 gradNorm: 23.97 \n",
      "2025-03-13 22:03:15,040: Train batch 3426: loss: 1.60 gradNorm: 16.56 \n",
      "2025-03-13 22:03:15,165: Train batch 3427: loss: 3.15 gradNorm: 21.69 \n",
      "2025-03-13 22:03:15,518: Train batch 3428: loss: 2.56 gradNorm: 22.32 \n",
      "2025-03-13 22:03:15,816: Train batch 3429: loss: 1.64 gradNorm: 16.70 \n",
      "2025-03-13 22:03:16,028: Train batch 3430: loss: 2.23 gradNorm: 22.17 \n",
      "2025-03-13 22:03:16,160: Train batch 3431: loss: 2.48 gradNorm: 15.42 \n",
      "2025-03-13 22:03:16,268: Train batch 3432: loss: 1.03 gradNorm: 15.80 \n",
      "2025-03-13 22:03:16,567: Train batch 3433: loss: 1.66 gradNorm: 15.53 \n",
      "2025-03-13 22:03:16,663: Train batch 3434: loss: 3.87 gradNorm: 22.97 \n",
      "2025-03-13 22:03:16,754: Train batch 3435: loss: 2.67 gradNorm: 18.82 \n",
      "2025-03-13 22:03:16,878: Train batch 3436: loss: 3.24 gradNorm: 18.75 \n",
      "2025-03-13 22:03:16,972: Train batch 3437: loss: 1.68 gradNorm: 15.11 \n",
      "2025-03-13 22:03:17,130: Train batch 3438: loss: 0.85 gradNorm: 12.16 \n",
      "2025-03-13 22:03:17,199: Train batch 3439: loss: 2.14 gradNorm: 14.58 \n",
      "2025-03-13 22:03:17,392: Train batch 3440: loss: 1.67 gradNorm: 17.21 \n",
      "2025-03-13 22:03:17,640: Train batch 3441: loss: 1.99 gradNorm: 16.35 \n",
      "2025-03-13 22:03:17,714: Train batch 3442: loss: 2.85 gradNorm: 22.18 \n",
      "2025-03-13 22:03:18,097: Train batch 3443: loss: 1.80 gradNorm: 15.36 \n",
      "2025-03-13 22:03:18,184: Train batch 3444: loss: 2.66 gradNorm: 18.79 \n",
      "2025-03-13 22:03:18,369: Train batch 3445: loss: 1.72 gradNorm: 17.11 \n",
      "2025-03-13 22:03:18,467: Train batch 3446: loss: 3.91 gradNorm: 24.73 \n",
      "2025-03-13 22:03:18,737: Train batch 3447: loss: 1.38 gradNorm: 13.19 \n",
      "2025-03-13 22:03:19,116: Train batch 3448: loss: 1.87 gradNorm: 17.17 \n",
      "2025-03-13 22:03:19,390: Train batch 3449: loss: 1.99 gradNorm: 16.78 \n",
      "2025-03-13 22:03:19,560: Train batch 3450: loss: 1.87 gradNorm: 14.84 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:03:19,771: Val batch: CER (t18.2025.01.15): 0.080\n",
      "2025-03-13 22:03:19,773: Val batch 3450: CER (avg): 0.080 \n",
      "2025-03-13 22:03:19,773: Batches since validation CER improved: 850\n",
      "2025-03-13 22:03:19,875: Train batch 3451: loss: 2.28 gradNorm: 18.61 \n",
      "2025-03-13 22:03:20,039: Train batch 3452: loss: 1.08 gradNorm: 12.40 \n",
      "2025-03-13 22:03:20,247: Train batch 3453: loss: 1.78 gradNorm: 18.18 \n",
      "2025-03-13 22:03:20,383: Train batch 3454: loss: 3.00 gradNorm: 21.68 \n",
      "2025-03-13 22:03:20,464: Train batch 3455: loss: 2.10 gradNorm: 16.12 \n",
      "2025-03-13 22:03:20,658: Train batch 3456: loss: 1.89 gradNorm: 19.41 \n",
      "2025-03-13 22:03:20,799: Train batch 3457: loss: 1.53 gradNorm: 18.71 \n",
      "2025-03-13 22:03:21,007: Train batch 3458: loss: 1.12 gradNorm: 12.45 \n",
      "2025-03-13 22:03:21,083: Train batch 3459: loss: 1.55 gradNorm: 13.18 \n",
      "2025-03-13 22:03:21,384: Train batch 3460: loss: 1.44 gradNorm: 15.35 \n",
      "2025-03-13 22:03:21,517: Train batch 3461: loss: 2.00 gradNorm: 15.28 \n",
      "2025-03-13 22:03:21,604: Train batch 3462: loss: 2.61 gradNorm: 21.50 \n",
      "2025-03-13 22:03:21,713: Train batch 3463: loss: 1.31 gradNorm: 12.24 \n",
      "2025-03-13 22:03:21,865: Train batch 3464: loss: 1.40 gradNorm: 14.21 \n",
      "2025-03-13 22:03:21,969: Train batch 3465: loss: 2.03 gradNorm: 15.97 \n",
      "2025-03-13 22:03:22,139: Train batch 3466: loss: 1.99 gradNorm: 19.39 \n",
      "2025-03-13 22:03:22,224: Train batch 3467: loss: 2.80 gradNorm: 20.33 \n",
      "2025-03-13 22:03:22,544: Train batch 3468: loss: 1.92 gradNorm: 16.16 \n",
      "2025-03-13 22:03:22,633: Train batch 3469: loss: 2.33 gradNorm: 18.12 \n",
      "2025-03-13 22:03:22,911: Train batch 3470: loss: 1.80 gradNorm: 20.07 \n",
      "2025-03-13 22:03:23,023: Train batch 3471: loss: 0.91 gradNorm: 9.02 \n",
      "2025-03-13 22:03:23,119: Train batch 3472: loss: 2.45 gradNorm: 23.43 \n",
      "2025-03-13 22:03:23,421: Train batch 3473: loss: 1.32 gradNorm: 10.92 \n",
      "2025-03-13 22:03:23,501: Train batch 3474: loss: 1.80 gradNorm: 14.75 \n",
      "2025-03-13 22:03:23,625: Train batch 3475: loss: 0.62 gradNorm: 12.93 \n",
      "2025-03-13 22:03:23,694: Train batch 3476: loss: 1.17 gradNorm: 12.89 \n",
      "2025-03-13 22:03:24,070: Train batch 3477: loss: 1.83 gradNorm: 19.87 \n",
      "2025-03-13 22:03:24,151: Train batch 3478: loss: 1.83 gradNorm: 13.74 \n",
      "2025-03-13 22:03:24,219: Train batch 3479: loss: 1.51 gradNorm: 13.31 \n",
      "2025-03-13 22:03:24,338: Train batch 3480: loss: 2.95 gradNorm: 19.09 \n",
      "2025-03-13 22:03:24,727: Train batch 3481: loss: 1.57 gradNorm: 15.16 \n",
      "2025-03-13 22:03:24,920: Train batch 3482: loss: 1.80 gradNorm: 17.70 \n",
      "2025-03-13 22:03:25,001: Train batch 3483: loss: 1.63 gradNorm: 14.01 \n",
      "2025-03-13 22:03:25,273: Train batch 3484: loss: 2.37 gradNorm: 20.30 \n",
      "2025-03-13 22:03:25,425: Train batch 3485: loss: 0.85 gradNorm: 15.21 \n",
      "2025-03-13 22:03:25,564: Train batch 3486: loss: 1.23 gradNorm: 13.16 \n",
      "2025-03-13 22:03:25,666: Train batch 3487: loss: 2.98 gradNorm: 18.88 \n",
      "2025-03-13 22:03:25,854: Train batch 3488: loss: 1.49 gradNorm: 15.54 \n",
      "2025-03-13 22:03:25,923: Train batch 3489: loss: 2.20 gradNorm: 18.37 \n",
      "2025-03-13 22:03:26,144: Train batch 3490: loss: 1.47 gradNorm: 17.08 \n",
      "2025-03-13 22:03:26,536: Train batch 3491: loss: 2.84 gradNorm: 17.92 \n",
      "2025-03-13 22:03:26,695: Train batch 3492: loss: 1.57 gradNorm: 12.29 \n",
      "2025-03-13 22:03:26,793: Train batch 3493: loss: 2.05 gradNorm: 18.03 \n",
      "2025-03-13 22:03:27,170: Train batch 3494: loss: 1.58 gradNorm: 12.88 \n",
      "2025-03-13 22:03:27,525: Train batch 3495: loss: 1.49 gradNorm: 13.40 \n",
      "2025-03-13 22:03:27,708: Train batch 3496: loss: 2.08 gradNorm: 16.03 \n",
      "2025-03-13 22:03:27,883: Train batch 3497: loss: 1.06 gradNorm: 10.91 \n",
      "2025-03-13 22:03:27,992: Train batch 3498: loss: 1.68 gradNorm: 16.23 \n",
      "2025-03-13 22:03:28,120: Train batch 3499: loss: 2.80 gradNorm: 15.72 \n",
      "2025-03-13 22:03:28,233: Train batch 3500: loss: 2.02 gradNorm: 15.83 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:03:28,447: Val batch: CER (t18.2025.01.15): 0.081\n",
      "2025-03-13 22:03:28,449: Val batch 3500: CER (avg): 0.081 \n",
      "2025-03-13 22:03:28,449: Batches since validation CER improved: 900\n",
      "2025-03-13 22:03:28,559: Train batch 3501: loss: 1.82 gradNorm: 13.30 \n",
      "2025-03-13 22:03:28,670: Train batch 3502: loss: 2.24 gradNorm: 15.71 \n",
      "2025-03-13 22:03:28,773: Train batch 3503: loss: 2.47 gradNorm: 17.23 \n",
      "2025-03-13 22:03:28,866: Train batch 3504: loss: 1.73 gradNorm: 15.02 \n",
      "2025-03-13 22:03:28,943: Train batch 3505: loss: 1.47 gradNorm: 16.64 \n",
      "2025-03-13 22:03:29,050: Train batch 3506: loss: 1.37 gradNorm: 13.06 \n",
      "2025-03-13 22:03:29,221: Train batch 3507: loss: 1.06 gradNorm: 11.42 \n",
      "2025-03-13 22:03:29,291: Train batch 3508: loss: 1.17 gradNorm: 10.56 \n",
      "2025-03-13 22:03:29,414: Train batch 3509: loss: 1.27 gradNorm: 12.67 \n",
      "2025-03-13 22:03:29,711: Train batch 3510: loss: 2.19 gradNorm: 21.43 \n",
      "2025-03-13 22:03:29,779: Train batch 3511: loss: 1.69 gradNorm: 14.94 \n",
      "2025-03-13 22:03:29,944: Train batch 3512: loss: 0.94 gradNorm: 12.01 \n",
      "2025-03-13 22:03:30,228: Train batch 3513: loss: 1.72 gradNorm: 17.35 \n",
      "2025-03-13 22:03:30,383: Train batch 3514: loss: 0.81 gradNorm: 10.58 \n",
      "2025-03-13 22:03:30,586: Train batch 3515: loss: 1.15 gradNorm: 10.40 \n",
      "2025-03-13 22:03:30,938: Train batch 3516: loss: 2.11 gradNorm: 18.28 \n",
      "2025-03-13 22:03:31,094: Train batch 3517: loss: 1.53 gradNorm: 17.37 \n",
      "2025-03-13 22:03:31,180: Train batch 3518: loss: 1.55 gradNorm: 13.14 \n",
      "2025-03-13 22:03:31,453: Train batch 3519: loss: 1.61 gradNorm: 14.47 \n",
      "2025-03-13 22:03:31,558: Train batch 3520: loss: 4.59 gradNorm: 30.02 \n",
      "2025-03-13 22:03:31,706: Train batch 3521: loss: 1.17 gradNorm: 11.82 \n",
      "2025-03-13 22:03:31,802: Train batch 3522: loss: 2.63 gradNorm: 18.25 \n",
      "2025-03-13 22:03:31,892: Train batch 3523: loss: 2.51 gradNorm: 20.13 \n",
      "2025-03-13 22:03:32,014: Train batch 3524: loss: 2.30 gradNorm: 16.33 \n",
      "2025-03-13 22:03:32,097: Train batch 3525: loss: 2.34 gradNorm: 20.56 \n",
      "2025-03-13 22:03:32,183: Train batch 3526: loss: 1.56 gradNorm: 14.58 \n",
      "2025-03-13 22:03:32,350: Train batch 3527: loss: 1.88 gradNorm: 18.84 \n",
      "2025-03-13 22:03:32,540: Train batch 3528: loss: 2.31 gradNorm: 23.80 \n",
      "2025-03-13 22:03:32,656: Train batch 3529: loss: 1.76 gradNorm: 15.17 \n",
      "2025-03-13 22:03:32,813: Train batch 3530: loss: 1.63 gradNorm: 14.68 \n",
      "2025-03-13 22:03:32,964: Train batch 3531: loss: 1.09 gradNorm: 13.50 \n",
      "2025-03-13 22:03:33,107: Train batch 3532: loss: 0.97 gradNorm: 11.80 \n",
      "2025-03-13 22:03:33,493: Train batch 3533: loss: 1.20 gradNorm: 15.31 \n",
      "2025-03-13 22:03:33,578: Train batch 3534: loss: 1.69 gradNorm: 15.89 \n",
      "2025-03-13 22:03:33,747: Train batch 3535: loss: 0.97 gradNorm: 9.82 \n",
      "2025-03-13 22:03:33,834: Train batch 3536: loss: 1.30 gradNorm: 10.18 \n",
      "2025-03-13 22:03:33,986: Train batch 3537: loss: 1.11 gradNorm: 12.45 \n",
      "2025-03-13 22:03:34,176: Train batch 3538: loss: 1.82 gradNorm: 16.55 \n",
      "2025-03-13 22:03:34,348: Train batch 3539: loss: 1.80 gradNorm: 16.30 \n",
      "2025-03-13 22:03:34,415: Train batch 3540: loss: 1.93 gradNorm: 16.70 \n",
      "2025-03-13 22:03:34,529: Train batch 3541: loss: 3.03 gradNorm: 19.34 \n",
      "2025-03-13 22:03:34,808: Train batch 3542: loss: 2.19 gradNorm: 19.48 \n",
      "2025-03-13 22:03:35,016: Train batch 3543: loss: 2.36 gradNorm: 20.46 \n",
      "2025-03-13 22:03:35,121: Train batch 3544: loss: 1.22 gradNorm: 12.07 \n",
      "2025-03-13 22:03:35,191: Train batch 3545: loss: 1.50 gradNorm: 13.75 \n",
      "2025-03-13 22:03:35,368: Train batch 3546: loss: 1.10 gradNorm: 15.03 \n",
      "2025-03-13 22:03:35,465: Train batch 3547: loss: 2.01 gradNorm: 14.83 \n",
      "2025-03-13 22:03:35,535: Train batch 3548: loss: 1.41 gradNorm: 12.39 \n",
      "2025-03-13 22:03:35,635: Train batch 3549: loss: 1.47 gradNorm: 15.55 \n",
      "2025-03-13 22:03:35,792: Train batch 3550: loss: 1.73 gradNorm: 19.30 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:03:35,993: Val batch: CER (t18.2025.01.15): 0.091\n",
      "2025-03-13 22:03:35,995: Val batch 3550: CER (avg): 0.091 \n",
      "2025-03-13 22:03:35,995: Batches since validation CER improved: 950\n",
      "2025-03-13 22:03:36,080: Train batch 3551: loss: 3.09 gradNorm: 19.67 \n",
      "2025-03-13 22:03:36,459: Train batch 3552: loss: 1.56 gradNorm: 20.37 \n",
      "2025-03-13 22:03:36,758: Train batch 3553: loss: 1.94 gradNorm: 18.37 \n",
      "2025-03-13 22:03:36,857: Train batch 3554: loss: 1.62 gradNorm: 17.36 \n",
      "2025-03-13 22:03:37,096: Train batch 3555: loss: 0.99 gradNorm: 10.84 \n",
      "2025-03-13 22:03:37,182: Train batch 3556: loss: 1.25 gradNorm: 13.62 \n",
      "2025-03-13 22:03:37,255: Train batch 3557: loss: 1.56 gradNorm: 13.30 \n",
      "2025-03-13 22:03:37,521: Train batch 3558: loss: 1.03 gradNorm: 14.89 \n",
      "2025-03-13 22:03:37,611: Train batch 3559: loss: 1.69 gradNorm: 16.81 \n",
      "2025-03-13 22:03:37,721: Train batch 3560: loss: 1.78 gradNorm: 15.22 \n",
      "2025-03-13 22:03:38,035: Train batch 3561: loss: 1.48 gradNorm: 18.83 \n",
      "2025-03-13 22:03:38,105: Train batch 3562: loss: 1.66 gradNorm: 14.08 \n",
      "2025-03-13 22:03:38,426: Train batch 3563: loss: 1.28 gradNorm: 16.55 \n",
      "2025-03-13 22:03:38,773: Train batch 3564: loss: 1.26 gradNorm: 12.49 \n",
      "2025-03-13 22:03:39,001: Train batch 3565: loss: 0.73 gradNorm: 19.49 \n",
      "2025-03-13 22:03:39,153: Train batch 3566: loss: 1.10 gradNorm: 11.00 \n",
      "2025-03-13 22:03:39,232: Train batch 3567: loss: 1.80 gradNorm: 14.87 \n",
      "2025-03-13 22:03:39,334: Train batch 3568: loss: 1.47 gradNorm: 17.58 \n",
      "2025-03-13 22:03:39,683: Train batch 3569: loss: 0.88 gradNorm: 12.65 \n",
      "2025-03-13 22:03:39,751: Train batch 3570: loss: 1.99 gradNorm: 16.36 \n",
      "2025-03-13 22:03:39,862: Train batch 3571: loss: 1.82 gradNorm: 16.16 \n",
      "2025-03-13 22:03:39,944: Train batch 3572: loss: 1.95 gradNorm: 21.89 \n",
      "2025-03-13 22:03:40,024: Train batch 3573: loss: 1.55 gradNorm: 14.08 \n",
      "2025-03-13 22:03:40,191: Train batch 3574: loss: 1.75 gradNorm: 15.27 \n",
      "2025-03-13 22:03:40,512: Train batch 3575: loss: 1.43 gradNorm: 15.31 \n",
      "2025-03-13 22:03:40,593: Train batch 3576: loss: 1.29 gradNorm: 10.40 \n",
      "2025-03-13 22:03:40,785: Train batch 3577: loss: 0.97 gradNorm: 10.68 \n",
      "2025-03-13 22:03:41,064: Train batch 3578: loss: 1.72 gradNorm: 15.83 \n",
      "2025-03-13 22:03:41,235: Train batch 3579: loss: 1.06 gradNorm: 10.04 \n",
      "2025-03-13 22:03:41,356: Train batch 3580: loss: 2.09 gradNorm: 18.60 \n",
      "2025-03-13 22:03:41,649: Train batch 3581: loss: 0.91 gradNorm: 9.87 \n",
      "2025-03-13 22:03:41,803: Train batch 3582: loss: 0.73 gradNorm: 13.11 \n",
      "2025-03-13 22:03:41,868: Train batch 3583: loss: 1.36 gradNorm: 11.60 \n",
      "2025-03-13 22:03:41,981: Train batch 3584: loss: 2.39 gradNorm: 15.91 \n",
      "2025-03-13 22:03:42,258: Train batch 3585: loss: 1.57 gradNorm: 13.29 \n",
      "2025-03-13 22:03:42,609: Train batch 3586: loss: 1.40 gradNorm: 17.70 \n",
      "2025-03-13 22:03:42,713: Train batch 3587: loss: 1.82 gradNorm: 17.41 \n",
      "2025-03-13 22:03:42,840: Train batch 3588: loss: 0.89 gradNorm: 12.18 \n",
      "2025-03-13 22:03:42,930: Train batch 3589: loss: 1.94 gradNorm: 16.95 \n",
      "2025-03-13 22:03:43,044: Train batch 3590: loss: 1.10 gradNorm: 11.83 \n",
      "2025-03-13 22:03:43,123: Train batch 3591: loss: 0.96 gradNorm: 9.23 \n",
      "2025-03-13 22:03:43,195: Train batch 3592: loss: 1.59 gradNorm: 16.88 \n",
      "2025-03-13 22:03:43,318: Train batch 3593: loss: 2.34 gradNorm: 14.53 \n",
      "2025-03-13 22:03:43,584: Train batch 3594: loss: 1.94 gradNorm: 19.48 \n",
      "2025-03-13 22:03:43,754: Train batch 3595: loss: 0.58 gradNorm: 8.29 \n",
      "2025-03-13 22:03:43,864: Train batch 3596: loss: 1.56 gradNorm: 12.78 \n",
      "2025-03-13 22:03:43,961: Train batch 3597: loss: 1.48 gradNorm: 15.14 \n",
      "2025-03-13 22:03:44,033: Train batch 3598: loss: 1.68 gradNorm: 15.06 \n",
      "2025-03-13 22:03:44,137: Train batch 3599: loss: 2.85 gradNorm: 17.21 \n",
      "2025-03-13 22:03:44,302: Train batch 3600: loss: 1.44 gradNorm: 16.39 \n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:03:44,494: Val batch: CER (t18.2025.01.15): 0.077\n",
      "2025-03-13 22:03:44,496: Val batch 3600: CER (avg): 0.077 \n",
      "2025-03-13 22:03:44,496: Batches since validation CER improved: 1000\n",
      "2025-03-13 22:03:44,496: Overall validation CER has not improved in 1000 batches. Stopping training early.\n",
      "2025-03-13 22:03:44,496: Best avg CER achieved: 0.06834\n",
      "2025-03-13 22:03:44,497: Total training time: 10.23 minutes.\n",
      "2025-03-13 22:03:44,497: Using GPU #: 0\n",
      "Model: \"gru_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_7 (GRU)                 multiple                  6687744   \n",
      "                                                                 \n",
      " gru_8 (GRU)                 multiple                  1575936   \n",
      "                                                                 \n",
      " gru_9 (GRU)                 multiple                  1575936   \n",
      "                                                                 \n",
      " gru_10 (GRU)                multiple                  1575936   \n",
      "                                                                 \n",
      " gru_11 (GRU)                multiple                  1575936   \n",
      "                                                                 \n",
      " dense_9 (Dense)             multiple                  15903     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13007903 (49.62 MB)\n",
      "Trainable params: 13007903 (49.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "2025-03-13 22:03:45,022: None\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_17 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65792 (257.00 KB)\n",
      "Trainable params: 65792 (257.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "2025-03-13 22:03:45,109: Initialized decoding model and input networks.\n",
      "2025-03-13 22:03:45,113: Initialized optimizer.\n",
      "2025-03-13 22:03:45,114: Set trainable variables.\n",
      "2025-03-13 22:03:45,118: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2024.12.04/Typing/tfdata_20ms/train\n",
      "2025-03-13 22:03:45,138: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2024.12.04/Typing/tfdata_20ms/test\n",
      "2025-03-13 22:03:45,146: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2024.12.05/Typing/tfdata_20ms/train\n",
      "2025-03-13 22:03:45,163: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2024.12.05/Typing/tfdata_20ms/test\n",
      "2025-03-13 22:03:45,174: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.14/Typing/tfdata_20ms/train\n",
      "2025-03-13 22:03:45,192: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.14/Typing/tfdata_20ms/test\n",
      "2025-03-13 22:03:45,203: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.15/Typing/tfdata_20ms/train\n",
      "2025-03-13 22:03:45,222: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.15/Typing/tfdata_20ms/test\n",
      "2025-03-13 22:03:45,235: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.21/Typing/tfdata_20ms/train\n",
      "2025-03-13 22:03:45,252: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.21/Typing/tfdata_20ms/test\n",
      "2025-03-13 22:03:45,266: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.22/Typing/tfdata_20ms/train\n",
      "2025-03-13 22:03:45,283: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.01.22/Typing/tfdata_20ms/test\n",
      "2025-03-13 22:03:45,298: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.02.04/Typing/tfdata_20ms/train\n",
      "2025-03-13 22:03:45,315: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.02.04/Typing/tfdata_20ms/test\n",
      "2025-03-13 22:03:45,332: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.02.05/Typing/tfdata_20ms/train\n",
      "2025-03-13 22:03:45,349: Loaded data from: /home/justin/Projects/NGEC/Data/t18-Arrays/t18.2025.02.05/Typing/tfdata_20ms/test\n",
      "2025-03-13 22:03:45,363: Loaded all data and created datasets and iterators.\n",
      "2025-03-13 22:03:45,363: Loading pre-trained RNN from : /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/-1\n",
      "2025-03-13 22:03:45,395: Saved dataset info to: /home/justin/Projects/NGEC/brand-modules/typing-east/T18ArraybyArray/256-384/brainToText_decoder_tfdata_params.json\n",
      "(40, 2506, 256)\n",
      "(10, 2041, 256)\n",
      "2025-03-13 22:03:45,660: Val batch: CER (t18.2025.01.15): 0.068\n",
      "[0.07692308 0.08108108 0.06060606 0.18421053 0.03225806 0.\n",
      " 0.03846154 0.03571429 0.05405405 0.         0.03333333 0.03448276\n",
      " 0.11764706 0.02702703 0.07142857 0.07142857 0.         0.02777778\n",
      " 0.08823529 0.03448276 0.11538462 0.07894737 0.         0.02941176\n",
      " 0.13333333 0.11538462 0.07142857 0.11111111 0.03333333 0.05555556\n",
      " 0.11764706 0.05263158 0.10810811 0.05714286 0.         0.125\n",
      " 0.07407407 0.03703704 0.         0.16216216 0.14814815 0.06666667\n",
      " 0.10714286 0.10714286 0.05555556 0.03703704 0.13333333 0.03571429\n",
      " 0.09677419 0.02857143]\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('../')\n",
    "import utils.brainToText_trainDecoder\n",
    "import gc\n",
    "all_channel_ranges = [(0,384), # All channels\n",
    "                      (0, 64), (64, 128), \n",
    "                      (128, 192), (192, 256), (256, 320), (320, 384), # Each array\n",
    "                      (256, 384)] # Each hemisphere\n",
    "\n",
    "for channel_range in all_channel_ranges:\n",
    "    importlib.reload(brainToText_trainDecoder)\n",
    "    args = OmegaConf.load('T18_trainArgsArraybyArray.yaml')\n",
    "    num_features = (channel_range[1] - channel_range[0]) * 2\n",
    "    print(num_features)\n",
    "    args['dataset']['nInputFeatures'] = num_features\n",
    "    args['model']['inputLayerSize'] = num_features\n",
    "    args['model']['inputNetwork']['inputLayerSizes'] = [num_features]\n",
    "    out_dir = 'T18ArraybyArrayModels/' + str(channel_range[0]) + '-' + str(channel_range[1])\n",
    "    args['outputDir'] = out_dir\n",
    "    args['dataset']['use_channels'] = list(range(channel_range[0], channel_range[1])) + list(range(channel_range[0] + 384, channel_range[1] + 384))\n",
    "    args['loadDir'] = 'null'\n",
    "    decoder = utils.brainToText_trainDecoder.brainToText_decoder(args)\n",
    "    infOut, stats = decoder.train()\n",
    "    decoder = None\n",
    "    args['loadDir'] = out_dir\n",
    "    \n",
    "    decoder = utils.brainToText_trainDecoder.brainToText_decoder(args)\n",
    "    out, out_by_day = decoder.inference()\n",
    "    print(out['seqErrorRateIndividual'])\n",
    "    np.save('T18Train-' + str(channel_range[0]) + '-' + str(channel_range[1]) +'-RawCERs.npy',out['seqErrorRateIndividual'])\n",
    "    decoder = None\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
