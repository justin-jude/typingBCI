dataset:
  name: speech
  sessions: # each session is all the training blocks from a given subject/day
    - t5.2019.12.09
    - t5.2019.12.11
    - t5.2019.12.18
    - t5.2019.12.20
    - t5.2020.01.06
    - t5.2020.01.08
    - t5.2020.01.13
    - t5.2020.01.15
  datasetProbabilityVal: [0,0,0,0,0,1.0,0,0]
  dataDir:  # path to all sessions
  - ../Data/t5-Plot-Subs/t5.2019.12.09/tfdata_20ms
  - ../Data/t5-Plot-Subs/t5.2019.12.11/tfdata_20ms
  - ../Data/t5-Plot-Subs/t5.2019.12.18/tfdata_20ms
  - ../Data/t5-Plot-Subs/t5.2019.12.20/tfdata_20ms
  - ../Data/t5-Plot-Subs/t5.2020.01.06/tfdata_20ms
  - ../Data/t5-Plot-Subs/t5.2020.01.08/tfdata_20ms
  - ../Data/t5-Plot-Subs/t5.2020.01.13/tfdata_20ms
  - ../Data/t5-Plot-Subs/t5.2020.01.15/tfdata_20ms
  whiteNoiseSD: 1.5     # SD of whtie noise that is added to neural data for training 
  constantOffsetSD: 0.10   # SD of constant offset noise for each channel  
  randomWalkSD: 0       # SD of random walk noise for each channel
  nInputFeatures: 192     # number of neural features. 
  staticGainSD: 0       # SD of static gain noise
  randomCut: 0          
  nClasses: 30            # number of characters.
  maxSeqElements: 120     # maximum number of phonemes per sentence.
  bufferSize: 1024
model:
  nUnits: 512             # number of units per GRU layer
  inputLayerSize: 192     # input layer size is typically same as number of input features 
  subsampleFactor: 1    
  weightReg: 1.0e-05      # weight regularization
  actReg: 0.0             # activity regularization
  dropout: 0.5          # dropout for GRU layers 
  trainable: true         # is the model trainable?
  nLayers: 5              # number of GRU layers  
  patch_size: 24       # how many patches 
  patch_stride: 18       # take every nth bin 
  inputNetwork:
    nInputLayers: 1       # one input layer per network
    inputLayerSizes:      # input layer size should be the same as the number of input features
    - 192
    trainable: true       # trainable input network?
    activation: softsign  
    dropout: 0.20         # dropout for input networks  
gpuNumber: '0'            # which GPU to use for training. 0 uses the first one.
mode: train
outputDir: T5subplot 
loadDir: T5Handwriting_LM_comparison_models-normalised/0.3CER
loadCheckpointIdx: null
smoothInputs: 1           # temporal gaussian smooth of input features
smoothKernelSD: 3
learnRateStart: 0.02      # learning rate starts here and decays over X batches
learnRateEnd: 0.0
learnRateDecaySteps: 10000
learnRatePower: 1.0       # power for learning rate degredation. 1 is linear.
trainableInput: true
trainableBackend: true
seed: -1
batchesPerSave: 0
batchesPerVal: 2         # how often to do validation (lower # = slower training)
nBatchesToTrain: 10000    # how many batches to train for
batchSize: 64             # how many trials per batch
inputScale: 1.0
inferenceOutputFileName: null
inputLayerForInference: null
gradClipValue: 10         # clip gradients to prevent explosion
lossType: ctc
normLayer: false
randomWalkAxis: -1
warmUpSteps: 0
early_stopping_num_batches: 1000
EndCER: 0.3