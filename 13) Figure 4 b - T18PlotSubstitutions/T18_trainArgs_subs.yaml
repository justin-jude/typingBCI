dataset:
  name: speech
  sessions: # each session is all the training blocks from a given subject/day
  - t18.2025.01.15
  - t18.2025.01.21
  - t18.2025.01.22
  - t18.2025.02.04
  - t18.2025.02.05
  datasetProbabilityVal: [0,0,1.0,0,0]
  dataDir:  # path to all sessions
  - ../Data/t18-Plot-Subs/t18.2025.01.15/Typing/tfdata_20ms
  - ../Data/t18-Plot-Subs/t18.2025.01.21/Typing/tfdata_20ms
  - ../Data/t18-Plot-Subs/t18.2025.01.22/Typing/tfdata_20ms
  - ../Data/t18-Plot-Subs/t18.2025.02.04/Typing/tfdata_20ms
  - ../Data/t18-Plot-Subs/t18.2025.02.05/Typing/tfdata_20ms
  whiteNoiseSD: 1.5     # SD of whtie noise that is added to neural data for training 
  constantOffsetSD: 0.22   # SD of constant offset noise for each channel
  randomWalkSD: 0       # SD of random walk noise for each channel
  nInputFeatures: 768     # number of neural features. Usually twice the number of channels (spikes + bandpower for each)
  staticGainSD: 0       # SD of static gain noise
  randomCut: 0          
  nClasses: 30            # number of characters. 
  maxSeqElements: 60     # maximum number of phonemes per sentence.
  bufferSize: 1024
model:
  nUnits: 512             # number of units per GRU layer
  inputLayerSize: 768     # input layer size is typically same as number of input features #512 if ignoring hand-knob arrays
  subsampleFactor: 1    
  weightReg: 1.0e-05      # weight regularization
  actReg: 0.0             # activity regularization
  dropout: 0.6          # dropout for GRU layers 
  trainable: true         # is the model trainable?
  nLayers: 5              # number of GRU layers 
  patch_size: 15       # how many patches 
  patch_stride: 6      # take every nth bin
  inputNetwork:
    nInputLayers: 1       # one input layer per network
    inputLayerSizes:      # input layer size should be the same as the number of input features
    - 768
    trainable: true       # trainable input network?
    activation: softsign  
    dropout: 0.2         # dropout for input networks 
gpuNumber: '0'            # which GPU to use for training. 0 uses the first one.
mode: train
outputDir: T18subplot 
loadDir:  T18Handwriting_LM_comparison_models-normalised/0.3CER 
loadCheckpointIdx: null
smoothInputs: 1           # temporal gaussian smooth of input features
smoothKernelSD: 2
learnRateStart: 0.02      # learning rate starts here and decays over X batches
learnRateEnd: 0.0
learnRateDecaySteps: 10000
learnRatePower: 1.0       # power for learning rate degredation. 1 is linear.
trainableInput: true
trainableBackend: true
seed: -1
batchesPerSave: 0
batchesPerVal: 5         # how often to do validation (lower # = slower training)
nBatchesToTrain: 10000    # how many batches to train for
batchSize: 32             # how many trials per batch
inputScale: 1.0
inferenceOutputFileName: null
inputLayerForInference: null
gradClipValue: 10         # clip gradients to prevent explosion
lossType: ctc
normLayer: false
randomWalkAxis: -1
warmUpSteps: 0
early_stopping_num_batches: 500
EndCER: 0.35
